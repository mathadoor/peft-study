{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e45fd35-cdca-44b8-a00f-607078ee1a67",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Investigating Ranks\n",
    "In the first section, I am going to investigate what an appropriate rank to consider should be. It will make sense to look at the \n",
    "ranks of a trained matrix and a randomly initialized matrix. We will start by looking at a randomly initialized matrix and then\n",
    "look at a trained matrix. Specifically, I will look at the rank of a randomly initialized matrix. I will need to consider the sizes \n",
    "that are seen in RoBERTa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1397017-4851-477f-a432-2a934510e545",
   "metadata": {},
   "source": [
    "## Imports and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f2172b3-4630-4fed-8bfd-4cf8b3ea7ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Appropriate Libraries\n",
    "%load_ext autoreload\n",
    "    \n",
    "import torch\n",
    "import matplotlib.pyplot as plt \n",
    "from tqdm.notebook import tqdm \n",
    "import bisect\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "# Set Logging Level\n",
    "import logging\n",
    "level = logging.DEBUG\n",
    "logging.getLogger(\"requests\").setLevel(level)\n",
    "logging.getLogger(\"urllib3\").setLevel(level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "25dba68b-c775-4672-8fbf-f2bb3da6f1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_principle_direction(A, exp_var):\n",
    "    U, S, V = torch.linalg.svd(A)\n",
    "    X = (torch.cumsum(S, 0) / S.sum().item()).tolist()\n",
    "    num = bisect.bisect(X, exp_var)\n",
    "    return num\n",
    "\n",
    "def get_principle_directions(A, exp_vars):\n",
    "    U, S, V = torch.linalg.svd(A)\n",
    "    X = (torch.cumsum(S, 0) / S.sum().item()).tolist()\n",
    "    ret = []\n",
    "    i, j = 0, 0\n",
    "    while i < len(exp_vars) and j < len(X):\n",
    "        val = X[j]\n",
    "        exp_var = exp_vars[i]\n",
    "        if val < exp_var:\n",
    "            j += 1\n",
    "        else:\n",
    "            ret.append(j)\n",
    "            i += 1\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4aaf774e-319c-4da5-a422-d7267ea08966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight tensor([0.0898, 0.2773, 0.4727, 0.6758, 0.8880, 0.9427, 0.9883])\n",
      "weight tensor([0.0525, 0.1751, 0.3210, 0.5019, 0.7588, 0.8521, 0.9572])\n",
      "weight tensor([0., 0., 0., 0., 0., 0., 0.])\n",
      "0.attention.self.query.weight tensor([0.0443, 0.1497, 0.2760, 0.4349, 0.6719, 0.7682, 0.8958])\n",
      "0.attention.self.key.weight tensor([0.0443, 0.1497, 0.2760, 0.4349, 0.6719, 0.7669, 0.8945])\n",
      "0.attention.self.value.weight tensor([0.0443, 0.1497, 0.2747, 0.4349, 0.6706, 0.7669, 0.8945])\n"
     ]
    }
   ],
   "source": [
    "k = 0\n",
    "for key, param in roberta_base_model.named_parameters():\n",
    "    with torch.no_grad():\n",
    "        key = \".\".join(key.split(\".\")[3:])\n",
    "        p_size = min(param.size())\n",
    "        if len(param.size()) < 2:\n",
    "            continue\n",
    "        print(key, torch.tensor(get_principle_directions(param, [0.1, 0.3, 0.5, 0.7, 0.9, 0.95, 0.99])) / p_size)\n",
    "    k += 1\n",
    "    if k > 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35846b95-4743-425e-a2bd-ac1db674217a",
   "metadata": {},
   "source": [
    "## Normally Initialized Matrix Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a4005e5-d964-4def-94f1-41d18030aeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(n=100, size=[768, 768], exp_var=0.99):\n",
    "    results = []\n",
    "    for _ in range(n):\n",
    "        A = torch.randn(size)\n",
    "        num = get_principle_direction(A, exp_var)\n",
    "        results.append(num)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86b27c30-3205-4635-b424-9e5e80078d10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([39.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 61.]),\n",
       " array([515. , 515.1, 515.2, 515.3, 515.4, 515.5, 515.6, 515.7, 515.8,\n",
       "        515.9, 516. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(10, 3))\n",
    "ax[0].hist(plot_results())\n",
    "ax[1].hist(plot_results(exp_var=0.95))\n",
    "ax[2].hist(plot_results(exp_var=0.9))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601559f6-0a40-40bb-830e-a836a35c5a5b",
   "metadata": {},
   "source": [
    "The above results indicate the rank is very tightly distributed based on the explained variance. The reader is encouraged to play with the explained_variance and support the claim themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41538748-3d56-4a69-a5f6-aa4f2e98f146",
   "metadata": {},
   "source": [
    "## Analysis of RoBERTa pre-trained Weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "373c7b6d-5ccd-447e-b545-7349e19e7e90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Now let's look at how tightly the weights are distributed in RoBERTa \n",
    "from transformers import AutoModelForQuestionAnswering, AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a18bc9a7-0c31-41be-970e-e37992303f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract RoBERTa Configuration\n",
    "roberta_base_config = AutoConfig.from_pretrained('roberta-base')\n",
    "roberta_large_config = AutoConfig.from_pretrained('roberta-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acf40d4d-e557-4b8c-a6ec-fe8725910263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract RoBERTa Model\n",
    "roberta_base_model = AutoModelForQuestionAnswering.from_config(roberta_base_config)\n",
    "roberta_large_model = AutoModelForQuestionAnswering.from_config(roberta_large_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9dd7956d-48ca-4f5f-96e1-72ca43118c2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def print_principle_direction(model, exp_var):\n",
    "    for key, param in model.named_parameters():\n",
    "        with torch.no_grad():\n",
    "            key = \".\".join(key.split(\".\")[3:])\n",
    "            p_size = min(param.size())\n",
    "            if len(param.size()) < 2:\n",
    "                continue\n",
    "            print(key, get_principle_direction(param, exp_var), p_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6379350c-f84f-41e2-bd2e-da56058431bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Roberta Base Principle Directions \n",
      "\n",
      "weight 759 768\n",
      "weight 492 514\n",
      "weight 0 1\n",
      "0.attention.self.query.weight 688 768\n",
      "0.attention.self.key.weight 687 768\n",
      "0.attention.self.value.weight 687 768\n",
      "0.attention.output.dense.weight 688 768\n",
      "0.intermediate.dense.weight 753 768\n",
      "0.output.dense.weight 753 768\n",
      "1.attention.self.query.weight 687 768\n",
      "1.attention.self.key.weight 688 768\n",
      "1.attention.self.value.weight 687 768\n",
      "1.attention.output.dense.weight 687 768\n",
      "1.intermediate.dense.weight 753 768\n",
      "1.output.dense.weight 753 768\n",
      "2.attention.self.query.weight 687 768\n",
      "2.attention.self.key.weight 687 768\n",
      "2.attention.self.value.weight 688 768\n",
      "2.attention.output.dense.weight 688 768\n",
      "2.intermediate.dense.weight 753 768\n",
      "2.output.dense.weight 753 768\n",
      "3.attention.self.query.weight 687 768\n",
      "3.attention.self.key.weight 688 768\n",
      "3.attention.self.value.weight 687 768\n",
      "3.attention.output.dense.weight 688 768\n",
      "3.intermediate.dense.weight 753 768\n",
      "3.output.dense.weight 753 768\n",
      "4.attention.self.query.weight 687 768\n",
      "4.attention.self.key.weight 688 768\n",
      "4.attention.self.value.weight 688 768\n",
      "4.attention.output.dense.weight 688 768\n",
      "4.intermediate.dense.weight 753 768\n",
      "4.output.dense.weight 753 768\n",
      "5.attention.self.query.weight 688 768\n",
      "5.attention.self.key.weight 687 768\n",
      "5.attention.self.value.weight 687 768\n",
      "5.attention.output.dense.weight 688 768\n",
      "5.intermediate.dense.weight 753 768\n",
      "5.output.dense.weight 753 768\n",
      "6.attention.self.query.weight 687 768\n",
      "6.attention.self.key.weight 688 768\n",
      "6.attention.self.value.weight 687 768\n",
      "6.attention.output.dense.weight 687 768\n",
      "6.intermediate.dense.weight 753 768\n",
      "6.output.dense.weight 753 768\n",
      "7.attention.self.query.weight 688 768\n",
      "7.attention.self.key.weight 688 768\n",
      "7.attention.self.value.weight 688 768\n",
      "7.attention.output.dense.weight 687 768\n",
      "7.intermediate.dense.weight 753 768\n",
      "7.output.dense.weight 753 768\n",
      "8.attention.self.query.weight 688 768\n",
      "8.attention.self.key.weight 687 768\n",
      "8.attention.self.value.weight 687 768\n",
      "8.attention.output.dense.weight 688 768\n",
      "8.intermediate.dense.weight 753 768\n",
      "8.output.dense.weight 753 768\n",
      "9.attention.self.query.weight 687 768\n",
      "9.attention.self.key.weight 687 768\n",
      "9.attention.self.value.weight 688 768\n",
      "9.attention.output.dense.weight 688 768\n",
      "9.intermediate.dense.weight 753 768\n",
      "9.output.dense.weight 753 768\n",
      "10.attention.self.query.weight 687 768\n",
      "10.attention.self.key.weight 687 768\n",
      "10.attention.self.value.weight 688 768\n",
      "10.attention.output.dense.weight 687 768\n",
      "10.intermediate.dense.weight 753 768\n",
      "10.output.dense.weight 753 768\n",
      "11.attention.self.query.weight 688 768\n",
      "11.attention.self.key.weight 688 768\n",
      "11.attention.self.value.weight 688 768\n",
      "11.attention.output.dense.weight 687 768\n",
      "11.intermediate.dense.weight 753 768\n",
      "11.output.dense.weight 753 768\n",
      " 1 2\n",
      "\n",
      "# Roberta Large Principle Directions \n",
      "\n",
      "weight 1012 1024\n",
      "weight 498 514\n",
      "weight 0 1\n",
      "0.attention.self.query.weight 916 1024\n",
      "0.attention.self.key.weight 916 1024\n",
      "0.attention.self.value.weight 917 1024\n",
      "0.attention.output.dense.weight 917 1024\n",
      "0.intermediate.dense.weight 1005 1024\n",
      "0.output.dense.weight 1005 1024\n",
      "1.attention.self.query.weight 918 1024\n",
      "1.attention.self.key.weight 917 1024\n",
      "1.attention.self.value.weight 917 1024\n",
      "1.attention.output.dense.weight 917 1024\n",
      "1.intermediate.dense.weight 1005 1024\n",
      "1.output.dense.weight 1005 1024\n",
      "2.attention.self.query.weight 917 1024\n",
      "2.attention.self.key.weight 917 1024\n",
      "2.attention.self.value.weight 917 1024\n",
      "2.attention.output.dense.weight 916 1024\n",
      "2.intermediate.dense.weight 1005 1024\n",
      "2.output.dense.weight 1005 1024\n",
      "3.attention.self.query.weight 917 1024\n",
      "3.attention.self.key.weight 917 1024\n",
      "3.attention.self.value.weight 917 1024\n",
      "3.attention.output.dense.weight 917 1024\n",
      "3.intermediate.dense.weight 1005 1024\n",
      "3.output.dense.weight 1005 1024\n",
      "4.attention.self.query.weight 917 1024\n",
      "4.attention.self.key.weight 917 1024\n",
      "4.attention.self.value.weight 916 1024\n",
      "4.attention.output.dense.weight 917 1024\n",
      "4.intermediate.dense.weight 1005 1024\n",
      "4.output.dense.weight 1005 1024\n",
      "5.attention.self.query.weight 917 1024\n",
      "5.attention.self.key.weight 917 1024\n",
      "5.attention.self.value.weight 916 1024\n",
      "5.attention.output.dense.weight 917 1024\n",
      "5.intermediate.dense.weight 1005 1024\n",
      "5.output.dense.weight 1005 1024\n",
      "6.attention.self.query.weight 917 1024\n",
      "6.attention.self.key.weight 918 1024\n",
      "6.attention.self.value.weight 917 1024\n",
      "6.attention.output.dense.weight 917 1024\n",
      "6.intermediate.dense.weight 1005 1024\n",
      "6.output.dense.weight 1005 1024\n",
      "7.attention.self.query.weight 917 1024\n",
      "7.attention.self.key.weight 917 1024\n",
      "7.attention.self.value.weight 917 1024\n",
      "7.attention.output.dense.weight 916 1024\n",
      "7.intermediate.dense.weight 1005 1024\n",
      "7.output.dense.weight 1005 1024\n",
      "8.attention.self.query.weight 917 1024\n",
      "8.attention.self.key.weight 918 1024\n",
      "8.attention.self.value.weight 917 1024\n",
      "8.attention.output.dense.weight 917 1024\n",
      "8.intermediate.dense.weight 1005 1024\n",
      "8.output.dense.weight 1005 1024\n",
      "9.attention.self.query.weight 917 1024\n",
      "9.attention.self.key.weight 917 1024\n",
      "9.attention.self.value.weight 917 1024\n",
      "9.attention.output.dense.weight 918 1024\n",
      "9.intermediate.dense.weight 1005 1024\n",
      "9.output.dense.weight 1005 1024\n",
      "10.attention.self.query.weight 917 1024\n",
      "10.attention.self.key.weight 917 1024\n",
      "10.attention.self.value.weight 916 1024\n",
      "10.attention.output.dense.weight 917 1024\n",
      "10.intermediate.dense.weight 1005 1024\n",
      "10.output.dense.weight 1005 1024\n",
      "11.attention.self.query.weight 917 1024\n",
      "11.attention.self.key.weight 917 1024\n",
      "11.attention.self.value.weight 916 1024\n",
      "11.attention.output.dense.weight 916 1024\n",
      "11.intermediate.dense.weight 1005 1024\n",
      "11.output.dense.weight 1005 1024\n",
      "12.attention.self.query.weight 917 1024\n",
      "12.attention.self.key.weight 916 1024\n",
      "12.attention.self.value.weight 918 1024\n",
      "12.attention.output.dense.weight 916 1024\n",
      "12.intermediate.dense.weight 1005 1024\n",
      "12.output.dense.weight 1005 1024\n",
      "13.attention.self.query.weight 917 1024\n",
      "13.attention.self.key.weight 917 1024\n",
      "13.attention.self.value.weight 917 1024\n",
      "13.attention.output.dense.weight 916 1024\n",
      "13.intermediate.dense.weight 1005 1024\n",
      "13.output.dense.weight 1005 1024\n",
      "14.attention.self.query.weight 917 1024\n",
      "14.attention.self.key.weight 917 1024\n",
      "14.attention.self.value.weight 916 1024\n",
      "14.attention.output.dense.weight 917 1024\n",
      "14.intermediate.dense.weight 1005 1024\n",
      "14.output.dense.weight 1005 1024\n",
      "15.attention.self.query.weight 916 1024\n",
      "15.attention.self.key.weight 917 1024\n",
      "15.attention.self.value.weight 917 1024\n",
      "15.attention.output.dense.weight 917 1024\n",
      "15.intermediate.dense.weight 1005 1024\n",
      "15.output.dense.weight 1005 1024\n",
      "16.attention.self.query.weight 917 1024\n",
      "16.attention.self.key.weight 917 1024\n",
      "16.attention.self.value.weight 917 1024\n",
      "16.attention.output.dense.weight 917 1024\n",
      "16.intermediate.dense.weight 1005 1024\n",
      "16.output.dense.weight 1005 1024\n",
      "17.attention.self.query.weight 917 1024\n",
      "17.attention.self.key.weight 917 1024\n",
      "17.attention.self.value.weight 917 1024\n",
      "17.attention.output.dense.weight 917 1024\n",
      "17.intermediate.dense.weight 1005 1024\n",
      "17.output.dense.weight 1005 1024\n",
      "18.attention.self.query.weight 916 1024\n",
      "18.attention.self.key.weight 917 1024\n",
      "18.attention.self.value.weight 917 1024\n",
      "18.attention.output.dense.weight 917 1024\n",
      "18.intermediate.dense.weight 1005 1024\n",
      "18.output.dense.weight 1005 1024\n",
      "19.attention.self.query.weight 917 1024\n",
      "19.attention.self.key.weight 917 1024\n",
      "19.attention.self.value.weight 917 1024\n",
      "19.attention.output.dense.weight 917 1024\n",
      "19.intermediate.dense.weight 1005 1024\n",
      "19.output.dense.weight 1005 1024\n",
      "20.attention.self.query.weight 917 1024\n",
      "20.attention.self.key.weight 917 1024\n",
      "20.attention.self.value.weight 917 1024\n",
      "20.attention.output.dense.weight 917 1024\n",
      "20.intermediate.dense.weight 1005 1024\n",
      "20.output.dense.weight 1005 1024\n",
      "21.attention.self.query.weight 917 1024\n",
      "21.attention.self.key.weight 917 1024\n",
      "21.attention.self.value.weight 917 1024\n",
      "21.attention.output.dense.weight 916 1024\n",
      "21.intermediate.dense.weight 1005 1024\n",
      "21.output.dense.weight 1005 1024\n",
      "22.attention.self.query.weight 917 1024\n",
      "22.attention.self.key.weight 918 1024\n",
      "22.attention.self.value.weight 917 1024\n",
      "22.attention.output.dense.weight 917 1024\n",
      "22.intermediate.dense.weight 1005 1024\n",
      "22.output.dense.weight 1005 1024\n",
      "23.attention.self.query.weight 917 1024\n",
      "23.attention.self.key.weight 917 1024\n",
      "23.attention.self.value.weight 917 1024\n",
      "23.attention.output.dense.weight 917 1024\n",
      "23.intermediate.dense.weight 1005 1024\n",
      "23.output.dense.weight 1005 1024\n",
      " 1 2\n"
     ]
    }
   ],
   "source": [
    "explained_variance = 0.99\n",
    "print(\"# Roberta Base Principle Directions \\n\")\n",
    "print_principle_direction(roberta_base_model, explained_variance)\n",
    "\n",
    "print(\"\\n# Roberta Large Principle Directions \\n\")\n",
    "print_principle_direction(roberta_large_model, explained_variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a39e3e-2c84-458e-a47f-5e5a86443278",
   "metadata": {},
   "source": [
    "On the surface, the above results suggest you need most of the directions to account for the explained variance in the parameters.\n",
    "As such, we finetune the models for SQuAD and review the difference in the weights of the base and the tuned model. If the difference has \n",
    "fewer principle directions with high explanation, it would mean it is possible that task-based fine-tuning can be done in a smaller subspace.\n",
    "Let's add a final touch to the above results by looking at the ratio of number of significant principle directions and all directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a60ab56-63f9-4ccd-a60b-66d9faf8dc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def principle_direction_distribution(tensor_dict, exp_var):\n",
    "    explained_dirs = []\n",
    "    all_dirs = []\n",
    "    for key, param in tensor_dict.items():\n",
    "        with torch.no_grad():\n",
    "            if len(param.size()) < 2:\n",
    "                continue\n",
    "            p_size = min(param.size())\n",
    "            exp_size = get_principle_direction(param, exp_var)\n",
    "\n",
    "            all_dirs.append(p_size)\n",
    "            explained_dirs.append(exp_size)\n",
    "    return torch.tensor(explained_dirs), torch.tensor(all_dirs)\n",
    "\n",
    "def print_mean_var(x_t):\n",
    "    if not isinstance(x_t, torch.Tensor):\n",
    "        x_t = torch.tensor(x_t)\n",
    "    return x_t.mean().item(), x_t.std().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eec50554-ea33-4a22-8c04-b98c2d7384f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean, Standard Dev\n",
      "# Roberta Base Principle Directions \n",
      "\n",
      "(0.9071772694587708, 0.1229465901851654)\n",
      "# Roberta Large Principle Directions \n",
      "\n",
      "(0.9157129526138306, 0.09281840175390244)\n"
     ]
    }
   ],
   "source": [
    "explained_variance = 0.99\n",
    "print(\"Mean, Standard Dev\")\n",
    "print(\"# Roberta Base Principle Directions \\n\")\n",
    "exp_dirs, all_dirs = principle_direction_distribution(roberta_base_model.state_dict(), 0.99)\n",
    "print(print_mean_var(exp_dirs / all_dirs))\n",
    "\n",
    "print(\"# Roberta Large Principle Directions \\n\")\n",
    "exp_dirs, all_dirs = principle_direction_distribution(roberta_large_model.state_dict(), 0.99)\n",
    "print(print_mean_var(exp_dirs / all_dirs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a1ff52-126c-4665-92fd-c3e5d8c83fb2",
   "metadata": {},
   "source": [
    "## Rank Variation During Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d59c3413-b156-4dfd-a1f5-1df488ff818b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Dataset\n",
    "import hydra\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from train.train import train_epoch\n",
    "from utils.metrics import AverageMeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "719ff899-edf9-4f7e-b665-ea440ef6bc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Training Configuration\n",
    "with hydra.initialize(version_base=None, config_path=\"../config\"):\n",
    "    cfg = hydra.compose(config_name=\"app_config\")\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9efb6c9-3495-4760-9c2b-094fa47b98bd",
   "metadata": {},
   "source": [
    "### Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4b82e45-bb22-4aea-b6ac-1c2ef67ab9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Data\n",
    "train_dataset = load_dataset(\"squad\", split=\"train\")\n",
    "val_dataset = load_dataset(\"squad\", split=\"validation\")\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.model.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0bab19-fe5f-41d3-a4c8-282bbda8647d",
   "metadata": {},
   "source": [
    "#### Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76dfbf26-bbad-4d73-933a-652dded44c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Max Length is 512\n",
      "Dataset Features are {'id': Value(dtype='string', id=None), 'title': Value(dtype='string', id=None), 'context': Value(dtype='string', id=None), 'question': Value(dtype='string', id=None), 'answers': Sequence(feature={'text': Value(dtype='string', id=None), 'answer_start': Value(dtype='int32', id=None)}, length=-1, id=None)}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model Max Length is {tokenizer.model_max_length}\")\n",
    "print(f\"Dataset Features are {train_dataset.features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c89dd9c-1957-45c8-8461-e69b41717cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to Remove\n",
    "remove_columns=['question', 'context', 'id', 'title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19b45799-1fa8-4ed4-a7c2-b3004997e9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 143 training instances with length longer than 512\n"
     ]
    }
   ],
   "source": [
    "cut_off_length = 512\n",
    "long_dataset = train_dataset.filter(lambda x: len(tokenizer(x['question'], x['context']).input_ids) > cut_off_length)\n",
    "print(f\"There are {long_dataset.num_rows} training instances with length longer than {cut_off_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932bb38a-5b28-4dbd-9faa-0aa9571a02e7",
   "metadata": {},
   "source": [
    "Since there are only 143 instances with the length longer than 512. I will remove those instances for the purpose of simplicity. These changes are unlikely to affect the outcome. The learned concept will not account for\n",
    "the cases in which the context may not contain the answer. Also, if there is a regularity in the placement of the answer in the question, the model may over fit to it. But given large number of samples, I assume it is not\n",
    "the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "119e8460-86c9-40d3-8fdb-2ac52d692766",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.filter(lambda x: len(tokenizer(x['question'], x['context']).input_ids) <= cut_off_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5dbd6078-c403-46ae-9aee-abd56889548b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188\n",
      "['a copper statue of Christ']\n",
      "a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n"
     ]
    }
   ],
   "source": [
    "## Let's See how the answer looks like\n",
    "idx = 1\n",
    "instance = train_dataset[idx]\n",
    "answer = instance['answers']\n",
    "print(answer['answer_start'][0])\n",
    "print(answer['text'])\n",
    "print(instance['context'][answer['answer_start'][0]:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae666051-6c8c-4e42-995e-adc84cef27fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer is not character based\n"
     ]
    }
   ],
   "source": [
    "# The above indicates the answer_start is the beginning character. Let's see if this is consistent with the rest of the results\n",
    "char_tokenizer = True\n",
    "for idx, item in enumerate(train_dataset):\n",
    "    start = item['answers']['answer_start'][0]\n",
    "    end = start + len(item['answers']['text'][0])\n",
    "    extracted_answer = item['context'][start:end]\n",
    "    answer = item['answers']['text'][0]\n",
    "    if extracted_answer != answer:\n",
    "        print(idx, extracted_answer, answer)\n",
    "    if char_tokenizer:\n",
    "        tokenized_answer = tokenizer(answer).input_ids\n",
    "        char_tokenizer = len(tokenized_answer) - 2 == len(answer)\n",
    "\n",
    "if not char_tokenizer:\n",
    "    print(\"Tokenizer is not character based\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c37f437-6dbf-4a61-a180-42285188b5c2",
   "metadata": {},
   "source": [
    "The above result confirms that the answers are made available in terms of character offset instead of tokenizer offset. This is likely the case to maintain generality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d1e3db4e-f142-4acd-ae85-0ff707c3384e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bisect_custom(l, r, tokenizer, tokenized_context, target, l_off=0, r_off=0, mid_off=0):\n",
    "    # Standard Binary Search with a Few Bells and Whistles\n",
    "    while l < r:\n",
    "        mid = l + (r - l + mid_off) // 2\n",
    "        partial_context = tokenizer.decode(tokenized_context[:mid])\n",
    "        if len(partial_context) <= target:\n",
    "            l = mid + l_off\n",
    "        else:\n",
    "            r = mid + r_off\n",
    "        # print(r, l, mid)\n",
    "    return max(l, r)\n",
    "\n",
    "def get_start_end_index(tokenized_context, answer, answer_start):\n",
    "    # Set Answer End Index\n",
    "    answer_end = answer_start + len(answer)\n",
    "\n",
    "    # Search for the starting index of the Token That contains the answer\n",
    "    ll, rl = 0, len(tokenized_context)\n",
    "    # print(\"Starting Left Search\")\n",
    "    start_index = bisect_custom(ll, rl, tokenizer, tokenized_context, answer_start, r_off=-1, mid_off=1)\n",
    "\n",
    "    # Search for the ending index of the tokens in which the answer terminates\n",
    "    rl, rr = start_index, len(tokenized_context)\n",
    "    # print(\"Starting Right Search\")\n",
    "    end_index = bisect_custom(start_index, len(tokenized_context), tokenizer, tokenized_context, answer_end, l_off=1)\n",
    "    \n",
    "    return start_index, end_index\n",
    "\n",
    "def validate(tokenized_context, start_index, end_index, idx, answer, context, question, ret):\n",
    "    # Test if the correct Solution is found by extracting the partial context and checking if the answer is contained.\n",
    "    # While this is not a 100% foolproof solution. Informal tests before confirmed this to be the case.\n",
    "    # You can set the extracted_answers char offset to answer_start\n",
    "    extracted_context = tokenized_context[start_index:end_index]\n",
    "    extracted_answer = tokenizer.decode(extracted_context).strip()\n",
    "    \n",
    "    if extracted_answer.find(answer) == -1:\n",
    "        failed_instance = {\"idx\" : idx, \n",
    "                           \"start_index\" : start_index,\n",
    "                           \"end_index\" : end_index,\n",
    "                           \"extracted_answer\" : extracted_answer,\n",
    "                           \"answer\" : answer,\n",
    "                           \"context\" : context,\n",
    "                           \"question\" : question} \n",
    "        ret.append(failed_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "be31a5ad-b61e-475d-874e-db47e377ea5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6d5abbcd72344e5ae24dfdd11a4b95e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's tokenize the start and end character\n",
    "failed_indices = []\n",
    "for idx, item in tqdm(enumerate(train_dataset)):\n",
    "    question = item['question']\n",
    "    context = item['context']\n",
    "    \n",
    "    answer = item['answers']['text'][0]\n",
    "    answer_start = item['answers']['answer_start'][0]\n",
    "\n",
    "    # Extract Start and End Index of the Tokenized Question/Context Duo\n",
    "    tokenized_question = tokenizer(question).input_ids\n",
    "    tokenized_context = tokenizer(question, context).input_ids\n",
    "    start_index, end_index = get_start_end_index(tokenized_context, answer, answer_start + len(question) + 10)\n",
    "\n",
    "    # Test if the correct Solution is found\n",
    "    validate(tokenized_context, start_index, end_index, idx, answer, context, question, failed_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d26eb5e8-1043-40ff-8920-b8693db68547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 71 failed instances by the above method\n"
     ]
    }
   ],
   "source": [
    "print(fr\"There are {len(failed_indices)} failed instances by the above method\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "726329a5-b44d-4615-9c2c-35ac09a49789",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_failed_instances = []\n",
    "\n",
    "def process_batched_training_examples(instances):\n",
    "    questions, contexts = instances['question'], instances['context']\n",
    "    answers = instances['answers']\n",
    "    \n",
    "    inputs = tokenizer(questions, contexts) # Batched Tokenization\n",
    "    start_index, end_index = [], [] # Store the start and end_index in the beginning\n",
    "    \n",
    "    for idx, instance in enumerate(instances['id']):\n",
    "        answer_text, answer_start = answers[idx]['text'][0], answers[idx]['answer_start'][0]\n",
    "\n",
    "        start, end = get_start_end_index(inputs.input_ids[idx], answer_text, answer_start + len(questions[idx]) + 10)\n",
    "        \n",
    "        start_index.append(start)\n",
    "        end_index.append(end)\n",
    "\n",
    "        # Result Validation and storing the ones that failed. - map failed instances must be cleared every time to avoid accumulation\n",
    "        validate(inputs.input_ids[idx], start, end, 0, answer_text, contexts[idx], questions[idx], map_failed_instances)\n",
    "\n",
    "    inputs['start_positions'] = start_index\n",
    "    inputs['end_positions'] = end_index\n",
    "\n",
    "    return inputs\n",
    "\n",
    "def process_training_examples(instance):\n",
    "    \n",
    "    question, context = instance['question'], instance['context']\n",
    "    answer = instance['answers']\n",
    "    \n",
    "    inputs = tokenizer(question, context)\n",
    "    \n",
    "    answer_text, answer_start = answer['text'][0], answer['answer_start'][0]\n",
    "\n",
    "    start_index, end_index = get_start_end_index(inputs.input_ids, answer_text, answer_start + len(question) + 10)\n",
    "    \n",
    "    inputs['start_positions'] = start_index\n",
    "    inputs['end_positions'] = end_index\n",
    "\n",
    "    # Result Validation and storing the ones that failed. - map failed instances must be cleared every time to avoid accumulation\n",
    "    validate(inputs.input_ids, start_index, end_index, 0, answer_text, context, question, map_failed_instances)\n",
    "\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5ca9b2a1-63df-4897-b8bc-a91e46ef61ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(\n",
    "    process_batched_training_examples,\n",
    "    batched=True,\n",
    "    remove_columns=remove_columns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b02930ef-a55b-44fb-9795-1bb44475e351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eef2bf65da2d422b8f76a70725e31478",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/87456 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset.save_to_disk(\"../data/squad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6d4e98-7a06-4fdc-92d4-8177b5e558b0",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#### Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4cadb715-65dc-4ddd-a0e3-11368be102d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "    num_rows: 10570\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset.filter(lambda x: len(x['answers']) > 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "72715268-e5d5-47f1-b96a-638573644aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = val_dataset.filter(lambda x: len(tokenizer(x['question'], x['context']).input_ids) <= cut_off_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2ddd2ee3-809f-462f-9963-2f47d26673a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_validation_examples(instances):\n",
    "    questions, contexts = instances['question'], instances['context']\n",
    "    inputs = tokenizer(questions, contexts) # Batched Tokenization\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4e4e51ad-b3b3-41c0-b0c6-637170f18a2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "897d2e61fea14416885ecd0353cfcec5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10521 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_dataset = val_dataset.map(\n",
    "    process_validation_examples,\n",
    "    batched=True,\n",
    "    remove_columns=remove_columns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e24b2996-8eea-4b7e-8241-530ad19a4b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['answers', 'input_ids', 'attention_mask'])\n"
     ]
    }
   ],
   "source": [
    "print(val_dataset[0].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a31482-ff23-4244-8f81-6f9656ac8a15",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ab63ac41-7dca-4d80-a70a-cbcec6a5e818",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from collections import defaultdict\n",
    "pad_keys = ['input_ids', 'attention_mask']\n",
    "stack_keys = [ 'start_positions', 'end_positions']\n",
    "pad_id = tokenizer.pad_token_id\n",
    "\n",
    "def collate_fn(batch):\n",
    "    tensor_batch = defaultdict(list)\n",
    "        \n",
    "    # Generate list of samples in batch\n",
    "    for sample in batch:\n",
    "        for key in pad_keys + stack_keys:\n",
    "            key_tensor = torch.tensor(sample[key])\n",
    "            tensor_batch[key].append(key_tensor)\n",
    "\n",
    "    # padding value of attention_mask is 0 since it is multiplied\n",
    "    tensor_batch['input_ids'] = pad_sequence(tensor_batch['input_ids'], padding_value=pad_id, batch_first=True)\n",
    "    tensor_batch['attention_mask'] = pad_sequence(tensor_batch['attention_mask'], padding_value=0, batch_first=True)\n",
    "    \n",
    "    for key in stack_keys:\n",
    "        tensor_batch[key] = torch.stack(tensor_batch[key])\n",
    "\n",
    "    for k, v in tensor_batch.items():\n",
    "        tensor_batch[k] = v.to(device)\n",
    "\n",
    "    return tensor_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "b2f70cea-a6ee-46f2-92b8-4b944cd6d881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensors for Gradient Accumulation\n",
    "class GradAccumulator:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, model):\n",
    "        \n",
    "        self.model = model\n",
    "        self.accumulator = self.init_grad()\n",
    "    \n",
    "    def init_grad(self):\n",
    "        accumulator = dict()\n",
    "        for name, param in self.model.named_parameters():\n",
    "            accumulator[name] = torch.zeros_like(param, requires_grad=False)\n",
    "    \n",
    "        return accumulator\n",
    "\n",
    "    def accumulate_grad(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            self.accumulator[name] += param.grad\n",
    "\n",
    "    def reset(self):\n",
    "        for key in self.accumulator.keys():\n",
    "            self.accumulator[key] = 0\n",
    "\n",
    "    def analyze_grad(self, exp_var):\n",
    "        # Move to CPU\n",
    "        for key in self.accumulator.keys():\n",
    "            self.accumulator[key] = self.accumulator[key].cpu() \n",
    "        \n",
    "        exp_dirs, all_dirs = principle_direction_distribution(self.accumulator, exp_var)\n",
    "        mean_dir, var_dir = print_mean_var(exp_dirs / all_dirs)\n",
    "        return mean_dir, var_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "0a7bcbd0-1ab7-48b6-b890-79dddf7abbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Training Artifacts\n",
    "training_cfg = hydra.utils.instantiate(cfg.model.model.train)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=training_cfg.batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=training_cfg.batch_size)\n",
    "\n",
    "model = roberta_base_model if cfg.model.name == 'roberta-base' else roberta_large_model\n",
    "model.to(device)\n",
    "optimizer = AdamW(params=model.parameters(), lr=training_cfg.lr, weight_decay=training_cfg.weight_decay)\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "loss_meter = AverageMeter()\n",
    "acc_meter = AverageMeter()\n",
    "accumulator = GradAccumulator(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "1efba1f3-4678-4dc2-b887-1191ea32cfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ML Flow\n",
    "import mlflow\n",
    "\n",
    "# Set Tracking URI\n",
    "uri = mlflow.get_tracking_uri()\n",
    "if uri is None or uri != \"http://127.0.0.1:8080\":\n",
    "    mlflow.set_tracking_uri(\"http://127.0.0.1:8080\")\n",
    "\n",
    "# Set Experiment\n",
    "experiment_name = f\"{cfg.model.name}-{cfg.dataset.name}\"\n",
    "exps = mlflow.search_experiments(filter_string=f\"name='{experiment_name}'\")\n",
    "\n",
    "if len(exps) == 0:\n",
    "    experiment_id = mlflow.create_experiment(experiment_name)\n",
    "else:\n",
    "    experiment_id = exps[0].experiment_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "13ccdfe5-a449-4da3-8e9c-0b1517fe61c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72b05a6250e446b396290991b20fd963",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32796 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training Loop\n",
    "progress_bar = tqdm(total=training_cfg.epochs * len(train_loader))\n",
    "with mlflow.start_run(experiment_id=experiment_id, run_name=experiment_name) as run:\n",
    "    for epoch in range(training_cfg.epochs):\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            # Reset Optimizer to 0\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            # Forward Pass\n",
    "            output = model(**batch)\n",
    "        \n",
    "            # Backward Pass\n",
    "            output.loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate Gradient\n",
    "            accumulator.accumulate_grad()\n",
    "            \n",
    "            # Evaluate Performance\n",
    "            start_position_pred = torch.argmax(output.start_logits, dim=1)\n",
    "            end_position_pred = torch.argmax(output.end_logits, dim=1)\n",
    "            correct_exact_matches = ((batch[\"start_positions\"] == start_position_pred) \n",
    "                                     & (batch[\"end_positions\"] == end_position_pred)).sum()\n",
    "            \n",
    "            # Update Meters\n",
    "            acc_meter.update(correct_exact_matches.item(), len(batch))\n",
    "            loss_meter.update(output.loss.item(), 1)\n",
    "\n",
    "            # Update Progress\n",
    "            progress_bar.update(1)\n",
    "        \n",
    "        # Compute Metrics\n",
    "        match_accuracy = acc_meter.average()\n",
    "        training_loss = loss_meter.average()\n",
    "        mean_dirs, std_dirs = accumulator.analyze_grad(exp_var=0.99)\n",
    "\n",
    "        # Reset Accumulator\n",
    "        accumulator.reset()\n",
    "        \n",
    "        \n",
    "        # Track Metrics\n",
    "        mlflow.log_metric(\"training match accuracy\", match_accuracy, step=epoch)\n",
    "        mlflow.log_metric(\"training loss\", training_loss, step=epoch)\n",
    "        mlflow.log_metric(\"Mean Principle Dir Ratio\", mean_dirs, step=epoch)\n",
    "        mlflow.log_metric(\"Std Dev Principle Dir Ratio\", std_dirs, step=epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "165b2f4c-7afe-4826-9302-02d4eaf27b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "param = None\n",
    "for params in model.parameters():\n",
    "    param = params\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "50378659-1c48-4fea-9b3a-dff612d13b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "618e1ac9-77d9-4289-a7af-3b3ed5a2a073",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_position = torch.argmax(output.start_logits, dim=1)\n",
    "end_position = torch.argmax(output.end_logits, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "84c4645c-4f29-44ff-884c-0b4a306c0dc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((batch[\"start_positions\"] == start_position) \n",
    " & (batch[\"end_positions\"] == end_position)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5935d06e-6829-4170-8382-57d203cd8467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [0, 3972, 2661, 222, 5, 9880, 2708, 2346, 2082, 11, 504, 4432, 11, 226, 2126, 10067, 1470, 116, 2, 2, 37848, 37471, 28108, 6, 5, 334, 34, 10, 4019, 2048, 4, 497, 1517, 5, 4326, 6919, 18, 1637, 31346, 16, 10, 9030, 9577, 9, 5, 9880, 2708, 4, 29261, 11, 760, 9, 5, 4326, 6919, 8, 2114, 24, 6, 16, 10, 7621, 9577, 9, 4845, 19, 3701, 62, 33161, 19, 5, 7875, 22, 39043, 1459, 1614, 1464, 13292, 4977, 845, 4130, 7, 5, 4326, 6919, 16, 5, 26429, 2426, 9, 5, 25095, 6924, 4, 29261, 639, 5, 32394, 2426, 16, 5, 7461, 26187, 6, 10, 19035, 317, 9, 9621, 8, 12456, 4, 85, 16, 10, 24633, 9, 5, 11491, 26187, 23, 226, 2126, 10067, 6, 1470, 147, 5, 9880, 2708, 2851, 13735, 352, 1382, 7, 6130, 6552, 625, 3398, 208, 22895, 853, 1827, 11, 504, 4432, 4, 497, 5, 253, 9, 5, 1049, 1305, 36, 463, 11, 10, 2228, 516, 14, 15230, 149, 155, 19638, 8, 5, 2610, 25336, 238, 16, 10, 2007, 6, 2297, 7326, 9577, 9, 2708, 4, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "for item in train_dataset:\n",
    "    x = f\"[CLS]{item['question']}[SEP]{item['context']}\"\n",
    "    tokenized_x = tokenizer(item['question'], item['context'])\n",
    "    print(tokenized_x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd23942f-6ea1-4a79-98ae-ddaa0d6a765e",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model.forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1d0c2e-1193-416a-8d95-52dc9e5c3832",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c1c168-4382-4963-bab5-e723a863a4f7",
   "metadata": {},
   "source": [
    "# Introducing LoRA Weights\n",
    "This section introduces LoRA weights in our models for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec9a0bca-9500-4e4f-9650-23bfc0a02439",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# On the surface the above results suggest you need most of the directions to account for most of the directions. \n",
    "# It will be useful to see which directions are affected and how later.\n",
    "from models.LoRA import LoRALinearLayer\n",
    "def add_linear_lora(module, rank, init_type=0):\n",
    "    for key, child in module.named_children():\n",
    "        if isinstance(child, torch.nn.Linear):\n",
    "            lora_layer =  LoRALinearLayer(child, rank=rank, init_type=init_type)\n",
    "            setattr(module, key, lora_layer)\n",
    "        else:\n",
    "            add_linear_lora(child, rank, init_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa02e6c0-2f89-40ab-b387-9c77de31b5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_linear_lora(roberta_base, rank=10, init_type=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe6c9164-4b29-4ad1-b98f-a50c4f368e7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaHubInterface(\n",
       "  (model): RobertaModel(\n",
       "    (encoder): RobertaEncoder(\n",
       "      (sentence_encoder): TransformerEncoder(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
       "        (embed_positions): LearnedPositionalEmbedding(514, 768, padding_idx=1)\n",
       "        (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (layers): ModuleList(\n",
       "          (0-11): 12 x TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): LoRALinear(in_features=768, in_features=768, rank=$10, init_type=$1)\n",
       "              (v_proj): LoRALinear(in_features=768, in_features=768, rank=$10, init_type=$1)\n",
       "              (q_proj): LoRALinear(in_features=768, in_features=768, rank=$10, init_type=$1)\n",
       "              (out_proj): LoRALinear(in_features=768, in_features=768, rank=$10, init_type=$1)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): LoRALinear(in_features=768, in_features=3072, rank=$10, init_type=$1)\n",
       "            (fc2): LoRALinear(in_features=3072, in_features=768, rank=$10, init_type=$1)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (lm_head): RobertaLMHead(\n",
       "        (dense): LoRALinear(in_features=768, in_features=768, rank=$10, init_type=$1)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (classification_heads): ModuleDict()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c66542b-d1f8-417c-bae4-172a31970431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaHubInterface(\n",
       "  (model): RobertaModel(\n",
       "    (encoder): RobertaEncoder(\n",
       "      (sentence_encoder): TransformerEncoder(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
       "        (embed_positions): LearnedPositionalEmbedding(514, 1024, padding_idx=1)\n",
       "        (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (layers): ModuleList(\n",
       "          (0-23): 24 x TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (lm_head): RobertaLMHead(\n",
       "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (classification_heads): ModuleDict()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0ff436bb-df8a-4baa-af98-0be01d85c430",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "40d88517-de50-4af3-ac05-2096882f94d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /roberta-base/resolve/main/config.json HTTP/1.1\" 200 0\n"
     ]
    }
   ],
   "source": [
    "model_config = AutoConfig.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "489ec480-9ada-4d51-8c54-a21b050de0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel.from_config(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88bb4c7-0028-4504-840b-081200450898",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
