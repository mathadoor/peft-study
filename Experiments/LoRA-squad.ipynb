{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e45fd35-cdca-44b8-a00f-607078ee1a67",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Investigating Ranks\n",
    "In the first section, I am going to investigate what an appropriate rank to consider should be. It will make sense to look at the \n",
    "ranks of a trained matrix and a randomly initialized matrix. We will start by looking at a randomly initialized matrix and then\n",
    "look at a trained matrix. Specifically, I will look at the rank of a randomly initialized matrix. I will need to consider the sizes \n",
    "that are seen in RoBERTa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1397017-4851-477f-a432-2a934510e545",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Imports and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f2172b3-4630-4fed-8bfd-4cf8b3ea7ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Appropriate Libraries\n",
    "%load_ext autoreload\n",
    "    \n",
    "import torch\n",
    "import matplotlib.pyplot as plt \n",
    "import bisect\n",
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25dba68b-c775-4672-8fbf-f2bb3da6f1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_principle_direction(A, exp_var):\n",
    "    U, S, V = torch.linalg.svd(A)\n",
    "    X = (torch.cumsum(S, 0) / S.sum().item()).tolist()\n",
    "    num = bisect.bisect(X, exp_var)\n",
    "    return num"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35846b95-4743-425e-a2bd-ac1db674217a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Normally Initialized Matrix Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a4005e5-d964-4def-94f1-41d18030aeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(n=100, size=[768, 768], exp_var=0.99):\n",
    "    results = []\n",
    "    for _ in range(n):\n",
    "        A = torch.randn(size)\n",
    "        num = get_principle_direction(A, exp_var)\n",
    "        results.append(num)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86b27c30-3205-4635-b424-9e5e80078d10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([51.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 49.]),\n",
       " array([515. , 515.1, 515.2, 515.3, 515.4, 515.5, 515.6, 515.7, 515.8,\n",
       "        515.9, 516. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0QAAAESCAYAAADdURXtAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAx4klEQVR4nO3df1RVdb7/8RcIHCg8ICigCWaTI6bpFCpSTeMYSd6mLJlmpuVMP8ZVU4OWMv3i3jEnp8KpmXRq0Mxr/rijYzl3LO2HriKzaQJTzNK6oZYtGBXsZoDYeCR5f//oy7mdBOTAgXNgPx9r7bU6n7357M+bD73d7/PZZ58wMzMBAAAAgAOFB3sAAAAAABAsFEQAAAAAHIuCCAAAAIBjURABAAAAcCwKIgAAAACORUEEAAAAwLEoiAAAAAA4VkSwB/BNjY2NOnjwoHr37q2wsLBgDwdwNDPT0aNHNWDAAIWHd5/3T8gjQOjojnmEHAKEjq7IISFXEB08eFCpqanBHgaAr6msrNTAgQODPYw2I48Aoac75RFyCBB6OjOHhFxB1Lt3b0lfBe12u4M8GsDZ6urqlJqa6v3/srsgjwChozvmEXIIEDq6IoeEXEHUtDTtdrtJQkCI6G63jJBHgNDTnfIIOQQIPZ2ZQ7rHzbwAAAAA0AkoiAAAAAA4FgURAAAAAMeiIAIAAADgWBREAAAAAByLgggAAACAY1EQAQAAAHAsCiIAXerkyZOaPXu2Bg8erJiYGH3rW9/Sb3/7W5mZ9xgz0/3336/+/fsrJiZG2dnZ2rt3bxBHDQAAeqqQ+2JWf51934sB7e+TeVcGtD8Avn73u99p0aJFWrFihYYPH67t27fr5ptvVlxcnO644w5J0iOPPKLHH39cK1as0ODBgzV79mzl5OTogw8+UHR0dJAjQFcKdI6XyPMIPK5FgO6NFSIAXeqtt97S5MmTdeWVV+rss8/WD3/4Q02cOFFvv/22pK9WhxYsWKBf//rXmjx5skaOHKmVK1fq4MGDeu6554I7eABB95vf/EZhYWE+W3p6unf/8ePHlZeXp8TERMXGxio3N1fV1dVBHDGAUEdBBKBLXXTRRSouLtaePXskSe+++67efPNNTZo0SZK0f/9+VVVVKTs72/szcXFxyszMVElJSbN9ejwe1dXV+WwAeq7hw4fr0KFD3u3NN9/07ps1a5Y2bNigtWvXasuWLTp48KCmTJkSxNECCHXd/pY5AN3Lfffdp7q6OqWnp6tXr146efKkHnroIU2dOlWSVFVVJUlKTk72+bnk5GTvvm8qLCzUAw880LkDBxAyIiIilJKSckp7bW2tli5dqtWrV2vChAmSpGXLlmnYsGEqLS3VuHHjunqoALoBVogAdKlnn31Wq1at0urVq7Vjxw6tWLFCv//977VixYp291lQUKDa2lrvVllZGcARAwg1e/fu1YABA3TOOedo6tSpqqiokCSVlZWpoaHBZ4U5PT1daWlpLa4wS6wyA07nV0HEfbsAOuruu+/Wfffdp5/85Cc6//zz9bOf/UyzZs1SYWGhJHnf9f1m7qiurm72HWFJcrlccrvdPhuAnikzM1PLly/Xxo0btWjRIu3fv1/f/e53dfToUVVVVSkqKkrx8fE+P9PaCrP01SpzXFycd0tNTe3kKACEEr9XiLhvF0BHfPHFFwoP9009vXr1UmNjoyRp8ODBSklJUXFxsXd/XV2dtm7dqqysrC4dK4DQM2nSJF133XUaOXKkcnJy9NJLL6mmpkbPPvtsu/tklRlwNr8/Q8R9uwA64qqrrtJDDz2ktLQ0DR8+XO+8844ee+wx/fznP5ckhYWFaebMmXrwwQc1ZMgQ72O3BwwYoGuuuSa4gwcQcuLj4/Xtb39b+/bt0+WXX64TJ06opqbGZ5WotRVm6atVZpfL1QWjBRCK/F4h4r5dAB3xxBNP6Ic//KF++ctfatiwYbrrrrv0i1/8Qr/97W+9x9xzzz2aMWOGbr31Vo0ZM0b19fXauHEj30EE4BT19fX66KOP1L9/f2VkZCgyMtJnhbm8vFwVFRWsMANokV8rRE337Q4dOlSHDh3SAw88oO9+97vavXt3h+7b5elQgHP07t1bCxYs0IIFC1o8JiwsTHPnztXcuXO7bmAAuoW77rpLV111lQYNGqSDBw9qzpw56tWrl66//nrFxcVp2rRpys/PV0JCgtxut2bMmKGsrCzuVAE6qCd/UbZfBVHT94RI0siRI5WZmalBgwbp2WefVUxMTLsGUFBQoPz8fO/ruro6PswIAACa9c9//lPXX3+9PvvsM/Xr10+XXHKJSktL1a9fP0nS/PnzFR4ertzcXHk8HuXk5GjhwoVBHjWAUNah7yHivl0AANCV1qxZ0+r+6OhoFRUVqaioqItGBKC769D3EHHfLgAAAIDuzK8VIu7bBQAAANCT+FUQcd8uAAAAgJ7Er4KI+3YBAAAA9CQd+gwRAAAAAHRnFEQAAAAAHIuCCAAAAIBjURABAAAAcCwKIgAAAACO5ddT5gB/nH3fiwHv85N5Vwa8TwAAADgXK0QAAAAAHIuCCAAAAIBjURABAAAAcCwKIgAAAACORUEEAAAAwLEoiAAAAAA4FgURAAAAAMeiIAIAAADgWBREAAAAAByLgggAAACAY1EQAQAAAHAsCiIAAAAAjkVBBAAAAMCxKIgAAAAAOBYFEQAAAADHoiACAAAA4FgURAAAAAAci4IIAAAAgGNREAEAAABwLAoiAAAAAI5FQQQAAADAsSiIAAAAADgWBREAAOi25s2bp7CwMM2cOdPbdvz4ceXl5SkxMVGxsbHKzc1VdXV18AYJIKRREAEAgG5p27ZtWrx4sUaOHOnTPmvWLG3YsEFr167Vli1bdPDgQU2ZMiVIowQQ6iiIAABAt1NfX6+pU6dqyZIl6tOnj7e9trZWS5cu1WOPPaYJEyYoIyNDy5Yt01tvvaXS0tIgjhhAqOpQQcQyNQAACIa8vDxdeeWVys7O9mkvKytTQ0ODT3t6errS0tJUUlLSbF8ej0d1dXU+GwDnaHdBxDI1AAAIhjVr1mjHjh0qLCw8ZV9VVZWioqIUHx/v056cnKyqqqpm+yssLFRcXJx3S01N7YxhAwhR7SqIWKYGAADBUFlZqTvvvFOrVq1SdHR0QPosKChQbW2td6usrAxIvwC6h3YVRCxTAwCAYCgrK9Phw4d14YUXKiIiQhEREdqyZYsef/xxRUREKDk5WSdOnFBNTY3Pz1VXVyslJaXZPl0ul9xut88GwDki/P2BpmXqbdu2nbKvvcvUDzzwgL/DAAAADnTZZZdp165dPm0333yz0tPTde+99yo1NVWRkZEqLi5Wbm6uJKm8vFwVFRXKysoKxpABhDi/CqKmZepXXnkloMvU+fn53td1dXXcuwsAAJrVu3dvjRgxwqftzDPPVGJiord92rRpys/PV0JCgtxut2bMmKGsrCyNGzcuGEMGEOL8Koi+vkzd5OTJk3rjjTf0pz/9SZs2bfIuU399leh0y9Qul6t9owcAAPiG+fPnKzw8XLm5ufJ4PMrJydHChQuDPSwAIcqvzxA1LVPv3LnTu40ePVpTp071/nfTMnUTlqkBfNOBAwf005/+VImJiYqJidH555+v7du3e/ebme6//371799fMTExys7O1t69e4M4YgCh7PXXX9eCBQu8r6Ojo1VUVKQjR47o2LFj+tvf/tbiG7MA4NcKEcvUADrq888/18UXX6zvf//7evnll9WvXz/t3bvX54mVjzzyiB5//HGtWLFCgwcP1uzZs5WTk6MPPvggYLfrAgAASO14qMLpsEwNoDW/+93vlJqaqmXLlnnbBg8e7P1vM9OCBQv061//WpMnT5YkrVy5UsnJyXruuef0k5/85JQ+PR6PPB6P9zVPqwQAAG3V7i9mbcIyNQB/rF+/XqNHj9Z1112npKQkXXDBBVqyZIl3//79+1VVVeXz+P64uDhlZma2+Ph+vlQRAAC0V4cLIgDwx8cff6xFixZpyJAh2rRpk26//XbdcccdWrFihSR5H9GfnJzs83OtPb6fL1UEAADtFfBb5gCgNY2NjRo9erQefvhhSdIFF1yg3bt368knn9SNN97Yrj55WiUAAGgvVogAdKn+/fvrvPPO82kbNmyYKioqJMl7i211dbXPMa09vh8AAKC9KIgAdKmLL75Y5eXlPm179uzRoEGDJH31gIWUlBSfx/fX1dVp69atPL4fAAAEHLfMAehSs2bN0kUXXaSHH35YP/rRj/T222/rqaee0lNPPSVJCgsL08yZM/Xggw9qyJAh3sduDxgwQNdcc01wBw8AAHocCiIAXWrMmDFat26dCgoKNHfuXA0ePFgLFizQ1KlTvcfcc889OnbsmG699VbV1NTokksu0caNG/kOIgAAEHAURAC63A9+8AP94Ac/aHF/WFiY5s6dq7lz53bhqAAAgBPxGSIAAAAAjkVBBAAAAMCxKIgAAAAAOBYFEQAAAADHoiACAAAA4FgURAAAAAAci4IIAAAAgGNREAEAAABwLAoiAAAAAI5FQQQAAADAsSiIAAAAADgWBREAAAAAx6IgAgAAAOBYFEQAAAAAHIuCCAAAAIBjURABAAAAcCwKIgAAAACORUEEAAAAwLEoiAAAAAA4FgURAAAAAMeiIAIAAADgWBREAAAAAByLgggAAHQbixYt0siRI+V2u+V2u5WVlaWXX37Zu//48ePKy8tTYmKiYmNjlZubq+rq6iCOGECooyACAADdxsCBAzVv3jyVlZVp+/btmjBhgiZPnqz3339fkjRr1ixt2LBBa9eu1ZYtW3Tw4EFNmTIlyKMGEMoigj0AAACAtrrqqqt8Xj/00ENatGiRSktLNXDgQC1dulSrV6/WhAkTJEnLli3TsGHDVFpaqnHjxgVjyABCnF8rRCxTAwCAUHHy5EmtWbNGx44dU1ZWlsrKytTQ0KDs7GzvMenp6UpLS1NJSUmL/Xg8HtXV1flsAJzDr4KIZWoAABBsu3btUmxsrFwul2677TatW7dO5513nqqqqhQVFaX4+Hif45OTk1VVVdVif4WFhYqLi/NuqampnRwBgFDi1y1znbFM7fF45PF4vK95VwYAALRm6NCh2rlzp2pra/XXv/5VN954o7Zs2dLu/goKCpSfn+99XVdXR1EEOEi7H6oQqGVq3pUBAAD+iIqK0rnnnquMjAwVFhZq1KhR+uMf/6iUlBSdOHFCNTU1PsdXV1crJSWlxf5cLpf34wBNGwDn8LsgCvQydUFBgWpra71bZWWl30EAAADnamxslMfjUUZGhiIjI1VcXOzdV15eroqKCmVlZQVxhABCmd9PmQv0MrXL5ZLL5Wr3zwMAAOcoKCjQpEmTlJaWpqNHj2r16tV6/fXXtWnTJsXFxWnatGnKz89XQkKC3G63ZsyYoaysLJ4wB6BFfhdETcvUkpSRkaFt27bpj3/8o3784x97l6m/vkp0umVqAACAtjp8+LBuuOEGHTp0SHFxcRo5cqQ2bdqkyy+/XJI0f/58hYeHKzc3Vx6PRzk5OVq4cGGQRw0glHX4e4iaW6bOzc2VxDI1AAAIrKVLl7a6Pzo6WkVFRSoqKuqiEQHo7vwqiFimBgAAANCT+FUQsUwNAAAAoCfxqyBimRoAAABAT9Lu7yECAAAAgO6OgggAAACAY1EQAQAAAHAsCiIAAAAAjkVBBAAAAMCxKIgAAAAAOBYFEQAAAADHoiACEDTz5s1TWFiYZs6c6W07fvy48vLylJiYqNjYWOXm5qq6ujp4gwQAAD0aBRGAoNi2bZsWL16skSNH+rTPmjVLGzZs0Nq1a7VlyxYdPHhQU6ZMCdIoAQBAT0dBBKDL1dfXa+rUqVqyZIn69Onjba+trdXSpUv12GOPacKECcrIyNCyZcv01ltvqbS0NIgjBgAAPRUFEYAul5eXpyuvvFLZ2dk+7WVlZWpoaPBpT09PV1pamkpKSlrsz+PxqK6uzmcDAABoi4hgDwCAs6xZs0Y7duzQtm3bTtlXVVWlqKgoxcfH+7QnJyerqqqqxT4LCwv1wAMPBHqoAADAAVghAtBlKisrdeedd2rVqlWKjo4OWL8FBQWqra31bpWVlQHrGwAA9GwURAC6TFlZmQ4fPqwLL7xQERERioiI0JYtW/T4448rIiJCycnJOnHihGpqanx+rrq6WikpKS3263K55Ha7fTYAAIC24JY5AF3msssu065du3zabr75ZqWnp+vee+9VamqqIiMjVVxcrNzcXElSeXm5KioqlJWVFYwhAwCAHo6CCECX6d27t0aMGOHTduaZZyoxMdHbPm3aNOXn5yshIUFut1szZsxQVlaWxo0bF4whAwCAHo6CCEBImT9/vsLDw5WbmyuPx6OcnBwtXLgw2MMCAAA9FAURgKB6/fXXfV5HR0erqKhIRUVFwRkQAABwFB6qAAAAAMCxKIgAAAAAOBYFEQAAAADHoiACAAAA4FgURAAAAAAci4IIAAAAgGNREAEAAABwLAoiAAAAAI5FQQQAAADAsSiIAAAAADgWBREAAAAAx6IgAgAA3UZhYaHGjBmj3r17KykpSddcc43Ky8t9jjl+/Ljy8vKUmJio2NhY5ebmqrq6OkgjBhDqKIgAAEC3sWXLFuXl5am0tFSvvPKKGhoaNHHiRB07dsx7zKxZs7RhwwatXbtWW7Zs0cGDBzVlypQgjhpAKPOrIOJdGQAAEEwbN27UTTfdpOHDh2vUqFFavny5KioqVFZWJkmqra3V0qVL9dhjj2nChAnKyMjQsmXL9NZbb6m0tDTIowcQivwqiHhXBgAAhJLa2lpJUkJCgiSprKxMDQ0Nys7O9h6Tnp6utLQ0lZSUNNuHx+NRXV2dzwbAOSL8OXjjxo0+r5cvX66kpCSVlZXp0ksv9b4rs3r1ak2YMEGStGzZMg0bNkylpaUaN25c4EYOAAAcrbGxUTNnztTFF1+sESNGSJKqqqoUFRWl+Ph4n2OTk5NVVVXVbD+FhYV64IEHOnu4AEJUhz5DxLsyAAAgWPLy8rR7926tWbOmQ/0UFBSotrbWu1VWVgZohAC6g3YXRIF8VyYuLs67paamtndIAADAIaZPn64XXnhBmzdv1sCBA73tKSkpOnHihGpqanyOr66uVkpKSrN9uVwuud1unw2Ac7S7IOJdGQAA0NXMTNOnT9e6dev02muvafDgwT77MzIyFBkZqeLiYm9beXm5KioqlJWV1dXDBdAN+PUZoiZN78q88cYbLb4r8/VVotO9K+NyudozDAAA4DB5eXlavXq1nn/+efXu3dt7B0pcXJxiYmIUFxenadOmKT8/XwkJCXK73ZoxY4aysrL4LDOAZvm1QsS7MgAAIJgWLVqk2tpajR8/Xv379/duzzzzjPeY+fPn6wc/+IFyc3N16aWXKiUlRX/729+COGoAocyvFSLelQEAAMFkZqc9Jjo6WkVFRSoqKuqCEQHo7vwqiBYtWiRJGj9+vE/7smXLdNNNN0n66l2Z8PBw5ebmyuPxKCcnRwsXLgzIYAEAAAAgkPwqiHhXBgAAAEBP0qHvIQIAAACA7oyCCAAAAIBjURABAAAAcCwKIgAAAACORUEEAAAAwLEoiAAAAAA4FgURAAAAAMeiIAIAAADgWBREAAAAAByLgggAAACAY1EQAQAAAHAsCiIAAAAAjkVBBAAAAMCxKIgAAAAAOBYFEQAAAADHoiACAAAA4FgURAAAAAAci4IIAAAAgGNREAEAAABwLAoiAF2qsLBQY8aMUe/evZWUlKRrrrlG5eXlPsccP35ceXl5SkxMVGxsrHJzc1VdXR2kEQMAgJ6MgghAl9qyZYvy8vJUWlqqV155RQ0NDZo4caKOHTvmPWbWrFnasGGD1q5dqy1btujgwYOaMmVKEEcNAAB6qohgDwCAs2zcuNHn9fLly5WUlKSysjJdeumlqq2t1dKlS7V69WpNmDBBkrRs2TINGzZMpaWlGjduXDCGDQAAeihWiAAEVW1trSQpISFBklRWVqaGhgZlZ2d7j0lPT1daWppKSkqa7cPj8aiurs5nAwAAaAsKIgBB09jYqJkzZ+riiy/WiBEjJElVVVWKiopSfHy8z7HJycmqqqpqtp/CwkLFxcV5t9TU1M4eOgAA6CEoiAAETV5ennbv3q01a9Z0qJ+CggLV1tZ6t8rKygCNEAAA9HR8hghAUEyfPl0vvPCC3njjDQ0cONDbnpKSohMnTqimpsZnlai6ulopKSnN9uVyueRyuTp7yAAAoAdihQhAlzIzTZ8+XevWrdNrr72mwYMH++zPyMhQZGSkiouLvW3l5eWqqKhQVlZWVw8XAAD0cKwQAehSeXl5Wr16tZ5//nn17t3b+7mguLg4xcTEKC4uTtOmTVN+fr4SEhLkdrs1Y8YMZWVl8YQ5AAAQcBREALrUokWLJEnjx4/3aV+2bJluuukmSdL8+fMVHh6u3NxceTwe5eTkaOHChV08UgAA4AQURAC6lJmd9pjo6GgVFRWpqKioC0YEAACcjM8QAQCAbuWNN97QVVddpQEDBigsLEzPPfecz34z0/3336/+/fsrJiZG2dnZ2rt3b3AGCyDk+V0QkYQAAEAwHTt2TKNGjWpxFfmRRx7R448/rieffFJbt27VmWeeqZycHB0/fryLRwqgO/C7ICIJAQCAYJo0aZIefPBBXXvttafsMzMtWLBAv/71rzV58mSNHDlSK1eu1MGDB095ExcApHZ8hmjSpEmaNGlSs/u+mYQkaeXKlUpOTtZzzz2nn/zkJx0bLQAAQCv279+vqqoqZWdne9vi4uKUmZmpkpKSZq9FPB6PPB6P93VdXV2XjBVAaAjoZ4hOl4Sa4/F4VFdX57MBAAC0R9Oj/JOTk33ak5OTvfu+qbCwUHFxcd4tNTW108cJIHQEtCAiCQEAgO6moKBAtbW13q2ysjLYQwLQhYL+lDmSEAAACJSUlBRJUnV1tU97dXW1d983uVwuud1unw2AcwS0ICIJAQCAYBo8eLBSUlJUXFzsbaurq9PWrVuVlZUVxJEBCFUBLYhIQgAAoLPV19dr586d2rlzp6SvPsO8c+dOVVRUKCwsTDNnztSDDz6o9evXa9euXbrhhhs0YMAAXXPNNUEdN4DQ5PdT5urr67Vv3z7v66YklJCQoLS0NG8SGjJkiAYPHqzZs2eThAAAQMBs375d3//+972v8/PzJUk33nijli9frnvuuUfHjh3TrbfeqpqaGl1yySXauHGjoqOjgzVkACHM74KIJAQAAIJp/PjxMrMW94eFhWnu3LmaO3duF44KQHfld0FEEgIAAADQUwT9KXMAAAAAECwURAAAAAAci4IIAAAAgGP5/RkiAADQ/Zx934sB7e+TeVcGtD8ACBZWiAAAAAA4FgURAAAAAMeiIAIAAADgWBREAAAAAByLgggAAACAY1EQAQAAAHAsCiIAAAAAjkVBBAAAAMCxKIgAAAAAOBYFEQAAAADHoiACAAAA4FgURAAAAAAcKyLYAwCAYDr7vhcD3ucn864MeJ8AAKBzsEIEAAAAwLEoiAAAAAA4FgURAAAAAMeiIAIAAADgWBREAAAAAByLgggAAACAY1EQAQAAAHAsCiIAAAAAjkVBBAAAAMCxKIgAAAAAOBYFEQAAAADHoiACAAAA4FgURAAAAAAci4IIAAAAgGNFBHsAAE7v7PteDHifn8y7MuB9AgAAdDedtkJUVFSks88+W9HR0crMzNTbb7/dWacC0AORQwB0FHkEQFt0SkH0zDPPKD8/X3PmzNGOHTs0atQo5eTk6PDhw51xOgA9DDkEQEeRRwC0VafcMvfYY4/plltu0c033yxJevLJJ/Xiiy/q6aef1n333edzrMfjkcfj8b6ura2VJNXV1bXpXI2eLwI0avl1XpxeoOdGcu78BOt32XSMmQX8/K3xJ4dIHcsj/J2GNuYncIL172V3yCNciwCn16OvRSzAPB6P9erVy9atW+fTfsMNN9jVV199yvFz5swxSWxsbCG8VVZWBjpVtMjfHGJGHmFj6w5bKOcRcggbW+hvnZlDAr5C9L//+786efKkkpOTfdqTk5P14YcfnnJ8QUGB8vPzva8bGxt15MgRJSYmKiwsrNVz1dXVKTU1VZWVlXK73YEJIIh6Ujw9KRbJufGYmY4ePaoBAwZ02dj8zSFS+/OIU+e1uyCe0OVPLN0hj3At8n96Ujw9KRbJufF0RQ4J+lPmXC6XXC6XT1t8fLxffbjd7h7xh9GkJ8XTk2KRnBlPXFxcF42m/TqaR5w4r90J8YSutsYS6nmEa5FT9aR4elIskjPj6ewcEvCHKvTt21e9evVSdXW1T3t1dbVSUlICfToAPQw5BEBHkUcA+CPgBVFUVJQyMjJUXFzsbWtsbFRxcbGysrICfToAPQw5BEBHkUcA+KNTbpnLz8/XjTfeqNGjR2vs2LFasGCBjh075n3SS6C4XC7NmTPnlGXu7qonxdOTYpGIp6uRQ9qHeEJbT4qnO8RCHmmfnhRPT4pFIp7OFGbWOc+w+9Of/qRHH31UVVVV+s53vqPHH39cmZmZnXEqAD0QOQRAR5FHALRFpxVEAAAAABDqAv4ZIgAAAADoLiiIAAAAADgWBREAAAAAx6IgAgAAAOBYQSuIDhw4oJ/+9KdKTExUTEyMzj//fG3fvt27v76+XtOnT9fAgQMVExOj8847T08++aR3/yeffKKwsLBmt7Vr17Z4XjPT/fffr/79+ysmJkbZ2dnau3dvt43npptuOuX4K664IqixSFJVVZV+9rOfKSUlRWeeeaYuvPBC/fd///dpz11UVKSzzz5b0dHRyszM1Ntvv92hWIIZz29+85tT5iY9PT0k4vnoo4907bXXql+/fnK73frRj350yhcYNqcz5qc9Tve7bct87dmzR5MnT1bfvn3ldrt1ySWXaPPmza2et7PyR7Di6Yz8Eah4duzYocsvv1zx8fFKTEzUrbfeqvr6+lbPG8rz0554gjU/bckPR44c0dSpU+V2uxUfH69p06adNp7jx48rLy9PiYmJio2NVW5ubpvyTqCcLu6nnnpK48ePl9vtVlhYmGpqak7p4+yzzz6lj3nz5rV63rbEXVFRoSuvvFJnnHGGkpKSdPfdd+vLL78MuXiOHDmiGTNmaOjQoYqJiVFaWpruuOMO1dbW+hzX3LXMmjVrQioWSRo/fvwpP3Pbbbf5HNNd5qat15H+zk2g4pGkF198UZmZmYqJiVGfPn10zTXXtHretuT09uSilk7W5Y4cOWKDBg2ym266ybZu3Woff/yxbdq0yfbt2+c95pZbbrFvfetbtnnzZtu/f78tXrzYevXqZc8//7yZmX355Zd26NAhn+2BBx6w2NhYO3r0aIvnnjdvnsXFxdlzzz1n7777rl199dU2ePBg+9e//tUt47nxxhvtiiuu8Pm5I0eOBDUWM7PLL7/cxowZY1u3brWPPvrIfvvb31p4eLjt2LGjxXOvWbPGoqKi7Omnn7b333/fbrnlFouPj7fq6upuGc+cOXNs+PDhPnPz6aeftjuWQMVTX19v55xzjl177bX23nvv2XvvvWeTJ0+2MWPG2MmTJ1s8d2fMT3ud7nfblvkaMmSI/du//Zu9++67tmfPHvvlL39pZ5xxhh06dKjF83ZG/ghmPIHOH4GK58CBA9anTx+77bbb7MMPP7S3337bLrroIsvNzW31vKE6P+2NJxjz09b8cMUVV9ioUaOstLTU/v73v9u5555r119/favnve222yw1NdWKi4tt+/btNm7cOLvooos6HE9bnW4e58+fb4WFhVZYWGiS7PPPPz+lj0GDBtncuXN9+qivr2/1vKeL+8svv7QRI0ZYdna2vfPOO/bSSy9Z3759raCgIOTi2bVrl02ZMsXWr19v+/bts+LiYhsyZMgpf8uSbNmyZT79tvb/YbDm5nvf+57dcsstPj9TW1vr3d+d5qat15H+zk2g4vnrX/9qffr0sUWLFll5ebm9//779swzz7R63rbk9PbkouYEpSC699577ZJLLmn1mOHDh9vcuXN92i688EL7j//4jxZ/5jvf+Y79/Oc/b3F/Y2OjpaSk2KOPPuptq6mpMZfLZX/5y1/aOPpTBSses6/+wZw8eXKbx3o6gYrlzDPPtJUrV/ock5CQYEuWLGmx37Fjx1peXp739cmTJ23AgAFWWFjoTwg+ghnPnDlzbNSoUf4PuhWBiGfTpk0WHh7uk/RramosLCzMXnnllRb77Yz5aa/T/W5PN1+ffvqpSbI33njDu7+urs4ktfg76Kz8YRaceMwCnz+adDSexYsXW1JSks8F+HvvvWeSbO/evc32Gcrz0554zIIzP23JDx988IFJsm3btnmPefnlly0sLMwOHDjQbL81NTUWGRlpa9eu9bb9z//8j0mykpKSAER1em3NyZs3b271InX+/PltPmdb4n7ppZcsPDzcqqqqvMcsWrTI3G63eTyekIqnOc8++6xFRUVZQ0ODt02SrVu3rs19BCuW733ve3bnnXe2uL+7z01z15H+zo1Zx+NpaGiws846y/7zP/+zzedsS05vTy5qSVBumVu/fr1Gjx6t6667TklJSbrgggu0ZMkSn2MuuugirV+/XgcOHJCZafPmzdqzZ48mTpzYbJ9lZWXauXOnpk2b1uJ59+/fr6qqKmVnZ3vb4uLilJmZqZKSkm4XT5PXX39dSUlJGjp0qG6//XZ99tlnQY/loosu0jPPPKMjR46osbFRa9as0fHjxzV+/Phmz3vixAmVlZX5zE14eLiys7NDYm78jafJ3r17NWDAAJ1zzjmaOnWqKioq2h1LoOLxeDwKCwvz+Wbo6OhohYeH680332z2vJ01Px3R2u/2dPOVmJiooUOHauXKlTp27Ji+/PJLLV68WElJScrIyGj2fJ2VP4IVT5NA5o9AxePxeBQVFaXw8P/7JyomJkaSWvwbDeX5aU88Tbp6ftqSH0pKShQfH6/Ro0d7j8nOzlZ4eLi2bt3a7PnKysrU0NDgMz/p6elKS0vr0hwSiJw8b948JSYm6oILLtCjjz7a6u1TbYm7pKRE559/vpKTk73H5OTkqK6uTu+//35IxdOc2tpaud1uRURE+LTn5eWpb9++Gjt2rJ5++mnZab72MlixrFq1Sn379tWIESNUUFCgL774wruvO89Na9eR/s6N1LF4duzYoQMHDig8PFwXXHCB+vfvr0mTJmn37t0t/kxbcnp7clGL/CqfAsTlcpnL5bKCggLbsWOHLV682KKjo2358uXeY44fP2433HCDSbKIiAiLioqyFStWtNjn7bffbsOGDWv1vP/4xz9Mkh08eNCn/brrrrMf/ehH3S4eM7O//OUv9vzzz9t7771n69ats2HDhtmYMWPsyy+/DGosn3/+uU2cONF7jNvttk2bNrV43gMHDpgke+utt3za7777bhs7dmy7YglmPGZfvbP07LPP2rvvvmsbN260rKwsS0tLs7q6uqDGc/jwYXO73XbnnXfasWPHrL6+3qZPn26S7NZbb232vJ01P+11ut9tW+arsrLSMjIyLCwszHr16mX9+/dv9RbIzsofwYrHLPD5I1Dx7N692yIiIuyRRx4xj8djR44csdzcXJNkDz/8cLPnDOX5aU88ZsGZn7bkh4ceesi+/e1vn9Jvv379bOHChc2ec9WqVRYVFXVK+5gxY+yee+7pUDxt1dac3Nq79n/4wx9s8+bN9u6779qiRYssPj7eZs2a1eI52xL3LbfcYhMnTvTZf+zYMZNkL730UkjF802ffvqppaWl2b//+7/7tM+dO9fefPNN27Fjh82bN89cLpf98Y9/DLlYFi9ebBs3brT33nvP/vznP9tZZ51l1157rXd/d56blq4j/Z2bQMTzl7/8xSRZWlqa/fWvf7Xt27fb9ddfb4mJifbZZ581e8625PT25KKWBKUgioyMtKysLJ+2GTNm2Lhx47yvH330Ufv2t79t69evt3fffdeeeOIJi42Nbfb2jy+++MLi4uLs97//favn7ax/MIMVT3M++ugjk2Svvvqq/4FY4GKZPn26jR071l599VXbuXOn/eY3v7G4uDh77733mj1vZ11wByue5nz++efmdrv9WjLurHg2bdpk55xzjvfi+ac//aldeOGFdttttzV73lAriL7pm7/b081XY2OjXX311TZp0iR78803rayszG6//XY766yzTskPTTrzgjsY8TSno/kjUPGYfXUhmZycbL169bKoqCi76667LDk52ebNm9fsOUJ5ftoTT3O6an5Olx+6a0H0TS3l5NYuUr9p6dKlFhERYcePH292f2cWRN/UFfF8XW1trY0dO9auuOIKO3HiRKvHzp492wYOHHjaPpt0dSxNiouLTZL3c7nddW78uY70d27M/I9n1apVJskWL17sbTt+/Lj17dvXnnzyyWbP4YiCKC0tzaZNm+bTtnDhQhswYICZfTWRkZGR9sILL/gcM23aNMvJyTmlv5UrV1pkZKQdPny41fM2/WPyzjvv+LRfeumldscdd7Qjkq8EK56WtPYHdjqBiGXfvn0myXbv3u1zzGWXXWa/+MUvmj2vx+OxXr16nXJf6w033GBXX311u2IJZjwtGT16tN13333+huEV6L+1Tz/91Ju4kpOT7ZFHHmn2vJ01P4HU9Ltty3y9+uqrp3xOwszs3HPPbfEzUZ2VP1rS2fG0pCP5ozX+xPN1VVVVdvToUauvr7fw8HB79tlnm+0/lOfn69oaT0s6e36+rqX8sHTpUouPj/c5tqGhwXr16mV/+9vfmu2/6ULzmxdKaWlp9thjjwUmiHZoLm5/LlJ3795tkuzDDz9sdn9b4p49e/Ypn8/4+OOPTdJpV3m/qbPjaVJXV2dZWVl22WWXtemhJS+88IJJ8qs46apYvq6+vt4k2caNG82se86NmX/Xke2ZGzP/4nnttddMkv3973/3aR87duwpq4tN2pLT25OLWhKUzxBdfPHFKi8v92nbs2ePBg0aJElqaGhQQ0ODz/3WktSrVy81Njae0t/SpUt19dVXq1+/fq2ed/DgwUpJSVFxcbG3ra6uTlu3blVWVlZ7wwlaPM355z//qc8++0z9+/f3+2elwMTSdP9tW+OVpKioKGVkZPjMTWNjo4qLi4M+N+2Jpzn19fX66KOP2j03UuD/1vr27av4+Hi99tprOnz4sK6++upmz9tZ8xMoX//dtmW+WjomPDy8xTntrPzRnK6IpzkdzR8t8Teer0tOTlZsbKyeeeYZRUdH6/LLL2/2HKE8P1/X1nia0xXz83Ut5YesrCzV1NSorKzMe+xrr72mxsZGZWZmNnuOjIwMRUZG+sxPeXm5KioqgpZDApGTd+7cqfDwcCUlJTW7vy1xZ2VladeuXTp8+LD3mFdeeUVut1vnnXdem8fSFfFIX/1/NXHiREVFRWn9+vWKjo5uU799+vTx+Wxaa7oqluZ+RpL3vN1tbpr4cx3p79xI/seTkZEhl8vlc/3S0NCgTz75xHv98k1tyentyUUt8qt8CpC3337bIiIi7KGHHrK9e/faqlWr7IwzzrA///nP3mO+973v2fDhw23z5s328ccf27Jlyyw6OvqUJbC9e/daWFiYvfzyy82ea+jQoT5V4rx58yw+Pt57X/bkyZM7/FjWYMVz9OhRu+uuu6ykpMT2799vr776ql144YU2ZMgQvyv9QMZy4sQJO/fcc+273/2ubd261fbt22e///3vLSwszF588UVvPxMmTLAnnnjC+3rNmjXmcrls+fLl9sEHH9itt95q8fHxPk936U7x/OpXv7LXX3/d9u/fb//4xz8sOzvb+vbt2+6Vv0DFY2b29NNPW0lJie3bt8/+67/+yxISEiw/P9/nXF0xP+3V2u+2LfP16aefWmJiok2ZMsV27txp5eXldtddd1lkZKTt3LnTe56uyB/Biqcz8keg4jEze+KJJ6ysrMzKy8vtT3/6k8XExJxyn3t3mZ/2xBOs+TFrW3644oor7IILLrCtW7fam2++aUOGDPF51O0///lPGzp0qG3dutXbdtttt1laWpq99tprtn37dsvKyjrlFuDOdLq4Dx06ZO+8844tWbLE+9TGd955x/sZh7feesvmz59vO3futI8++sj+/Oc/W79+/eyGG27oUNxNj3aeOHGi7dy50zZu3Gj9+vU77aOdgxFPbW2tZWZm2vnnn2/79u3zeQxz02fb1q9fb0uWLLFdu3bZ3r17beHChXbGGWfY/fffH1Kx7Nu3z+bOnWvbt2+3/fv32/PPP2/nnHOOXXrppd1ybpq0dh3ZnrkJRDxmZnfeeaedddZZtmnTJvvwww9t2rRplpSU5PNVAu3J6afLRW0VlILIzGzDhg02YsQIc7lclp6ebk899ZTP/kOHDtlNN91kAwYMsOjoaBs6dKj94Q9/sMbGRp/jCgoKLDU1tcXvT9H/f956k8bGRps9e7YlJyeby+Wyyy67zMrLy7tlPF988YVNnDjR+vXrZ5GRkTZo0CC75ZZbOnyBGohY9uzZY1OmTLGkpCQ744wzbOTIkac8lnbQoEE2Z84cn7YnnnjC0tLSLCoqysaOHWulpaUdiiWY8fz4xz+2/v37W1RUlJ111ln24x//2Of7goIZz7333mvJyckWGRlpQ4YMafZvsavmpz1O97tty3xt27bNJk6caAkJCda7d28bN27cKfeEd1X+CEY8nZU/AhXPz372M0tISLCoqKhm938zHrPQnh9/4wnm/LQlP3z22Wd2/fXXW2xsrLndbrv55pt9vutk//79Jsk2b97sbfvXv/5lv/zlL61Pnz52xhln2LXXXtvq92QF2uninjNnjkk6ZWuak7KyMsvMzLS4uDiLjo62YcOG2cMPP+xToLY37k8++cQmTZpkMTEx1rdvX/vVr37l8xjrUImn6Zao5rb9+/eb2VePPf7Od75jsbGxduaZZ9qoUaPsySefbPV77oIRS0VFhV166aWWkJBgLpfLzj33XLv77rtPufW4u8xNk9auI9szN4GIx+yrN5d/9atfWVJSkvXu3duys7NPudW4PTn9dLmorcL+/wAAAAAAwHGC8hkiAAAAAAgFFEQAAAAAHIuCCAAAAIBjURABAAAAcCwKIgAAAACORUEEAAAAwLEoiAAAAAA4FgURAAAAAMeiIAIAAADgWBREAAAAAByLgggAAACAY/0/oK7vWct3YhAAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1000x300 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(10, 3))\n",
    "ax[0].hist(plot_results())\n",
    "ax[1].hist(plot_results(exp_var=0.95))\n",
    "ax[2].hist(plot_results(exp_var=0.9))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601559f6-0a40-40bb-830e-a836a35c5a5b",
   "metadata": {},
   "source": [
    "The above results indicate the rank is very tightly distributed based on the explained variance. The reader is encouraged to play with the explained_variance and support the claim themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41538748-3d56-4a69-a5f6-aa4f2e98f146",
   "metadata": {},
   "source": [
    "## Analysis of RoBERTa pre-trained Weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "373c7b6d-5ccd-447e-b545-7349e19e7e90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/mathadoor/.cache/torch/hub/pytorch_fairseq_main\n",
      "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n",
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.large.tar.gz HTTP/1.1\" 200 0\n",
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.large.tar.gz from cache at /home/mathadoor/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2\n",
      "/home/mathadoor/Documents/peft-study/venv/lib/python3.10/site-packages/hydra/experimental/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydra.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/mathadoor/Documents/peft-study/venv/lib/python3.10/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n",
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n",
      "/home/mathadoor/Documents/peft-study/venv/lib/python3.10/site-packages/hydra/experimental/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.compose()\n",
      "  deprecation_warning(message=message)\n",
      "/home/mathadoor/Documents/peft-study/venv/lib/python3.10/site-packages/hydra/core/default_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @package _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more information\n",
      "  deprecation_warning(\n",
      "/home/mathadoor/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'config' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n",
      "/home/mathadoor/Documents/peft-study/venv/lib/python3.10/site-packages/hydra/compose.py:56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/upgrades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n",
      "/home/mathadoor/Documents/peft-study/venv/lib/python3.10/site-packages/hydra/experimental/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydra.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/mathadoor/Documents/peft-study/venv/lib/python3.10/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n",
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n",
      "/home/mathadoor/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'config' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n",
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n",
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 4, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1024, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19237, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 4400, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0004], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1024}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=4, cpu=False, fp16=True, memory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_invalid_size_inputs_valid_test=True, max_tokens=4400, max_sentences=8, required_batch_size_multiple=1, dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_validation=False, only_validate=False, max_sentences_valid=8, curriculum=0, distributed_world_size=1024, distributed_rank=0, distributed_backend='nccl', distributed_port=19237, device_id=0, distributed_no_spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameters=True, arch='roberta_large', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[0.0004], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_epochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0.98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate=0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_prob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attention_dropout=0.1, encoder_embed_dim=1024, encoder_layers=24, encoder_attention_heads=16, encoder_ffn_embed_dim=4096, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoint_heads=True, data='/home/mathadoor/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', max_source_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classification_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0, mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_large', pooler_dropout=0.0, no_token_positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding=True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'_name': 'masked_lm', 'data': '/home/mathadoor/.cache/torch/pytorch_fairseq/83e3a689e28e5e4696ecb0bbb05a77355444a5c8a3437e0f736d8a564e80035e.c687083d14776c1979f3f71654febb42f2bb3d9a94ff7ebdfe1ac6748dba89d2', 'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 'seed': 4, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi': False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': True, 'lr': [0.0004]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0, 'lr': [0.0004]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n",
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n",
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HTTP/1.1\" 200 0\n",
      "Using cache found in /home/mathadoor/.cache/torch/hub/pytorch_fairseq_main\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): dl.fbaipublicfiles.com:80\n",
      "DEBUG:urllib3.connectionpool:http://dl.fbaipublicfiles.com:80 \"HEAD /fairseq/models/roberta.base.tar.gz HTTP/1.1\" 200 0\n",
      "INFO:fairseq.file_utils:loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.base.tar.gz from cache at /home/mathadoor/.cache/torch/pytorch_fairseq/37d2bc14cf6332d61ed5abeb579948e6054e46cc724c7d23426382d11a31b2d6.ae5852b4abc6bf762e0b6b30f19e741aa05562471e9eb8f4a6ae261f04f9b350\n",
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n",
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n",
      "INFO:fairseq.tasks.masked_lm:dictionary: 50264 types\n",
      "INFO:fairseq.models.roberta.model:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 512, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19812, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 999999, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 999999, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0006], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 512}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=25, log_format='json', tbmf_wrapper=False, seed=1, cpu=False, fp16=True, memory_efficient_fp16=True, fp16_init_scale=4, fp16_scale_window=128, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=1.0, user_dir=None, criterion='masked_lm', tokenizer=None, bpe='gpt2', optimizer='adam', lr_scheduler='polynomial_decay', task='masked_lm', num_workers=2, skip_invalid_size_inputs_valid_test=True, max_tokens=999999, max_sentences=16, required_batch_size_multiple=1, dataset_impl='mmap', train_subset='train', valid_subset='valid', validate_interval=1, disable_validation=False, only_validate=False, max_sentences_valid=16, curriculum=0, distributed_world_size=512, distributed_rank=0, distributed_backend='nccl', distributed_port=19812, device_id=0, distributed_no_spawn=False, ddp_backend='c10d', bucket_cap_mb=200, fix_batches_to_gpus=False, find_unused_parameters=True, arch='roberta_base', max_epoch=0, max_update=500000, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[0.0006], use_bmuf=False, global_sync_iter=10, restore_file='checkpoint_last.pt', reset_dataloader=True, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=2000, keep_interval_updates=-1, keep_last_epochs=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, adam_betas='(0.9, 0.98)', adam_eps=1e-06, weight_decay=0.01, force_anneal=None, warmup_updates=24000, end_learning_rate=0.0, power=1.0, total_num_update=500000, sample_break_mode='complete', tokens_per_sample=512, mask_prob=0.15, leave_unmasked_prob=0.1, random_token_prob=0.1, activation_fn='gelu', dropout=0.1, attention_dropout=0.1, encoder_embed_dim=768, encoder_layers=12, encoder_attention_heads=12, encoder_ffn_embed_dim=3072, pooler_activation_fn='tanh', max_positions=512, activation_dropout=0.0, load_checkpoint_heads=True, data='/home/mathadoor/.cache/torch/pytorch_fairseq/37d2bc14cf6332d61ed5abeb579948e6054e46cc724c7d23426382d11a31b2d6.ae5852b4abc6bf762e0b6b30f19e741aa05562471e9eb8f4a6ae261f04f9b350', max_source_positions=512, max_target_positions=512, stop_min_lr=-1, encoder_layerdrop=0, encoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classification_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0, mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, _name='roberta_base', pooler_dropout=0.0, no_token_positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding=True, encoder_normalize_before=False, untie_weights_roberta=False, adaptive_input=False), 'task': {'_name': 'masked_lm', 'data': '/home/mathadoor/.cache/torch/pytorch_fairseq/37d2bc14cf6332d61ed5abeb579948e6054e46cc724c7d23426382d11a31b2d6.ae5852b4abc6bf762e0b6b30f19e741aa05562471e9eb8f4a6ae261f04f9b350', 'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 'seed': 1, 'include_target_tokens': False, 'include_index': True, 'skip_masking': False, 'd2v2_multi': False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': True, 'lr': [0.0006]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0, 'lr': [0.0006]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n",
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/encoder.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): dl.fbaipublicfiles.com:443\n",
      "DEBUG:urllib3.connectionpool:https://dl.fbaipublicfiles.com:443 \"HEAD /fairseq/gpt2_bpe/vocab.bpe HTTP/1.1\" 200 0\n"
     ]
    }
   ],
   "source": [
    "## Now let's look at how tightly the weights are distributed in RoBERTa \n",
    "roberta_large = torch.hub.load('pytorch/fairseq', 'roberta.large') \n",
    "roberta_base = torch.hub.load('pytorch/fairseq', 'roberta.base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9dd7956d-48ca-4f5f-96e1-72ca43118c2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def print_principle_direction(model, exp_var):\n",
    "    for key, param in model.named_parameters():\n",
    "        with torch.no_grad():\n",
    "            key = \".\".join(key.split(\".\")[3:])\n",
    "            p_size = min(param.size())\n",
    "            if len(param.size()) < 2:\n",
    "                continue\n",
    "            print(key, get_principle_direction(param, exp_var), p_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6379350c-f84f-41e2-bd2e-da56058431bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Roberta Base Principle Directions \n",
      "\n",
      "embed_tokens.weight 745 768\n",
      "embed_positions.weight 439 514\n",
      "layers.0.self_attn.k_proj.weight 652 768\n",
      "layers.0.self_attn.v_proj.weight 667 768\n",
      "layers.0.self_attn.q_proj.weight 653 768\n",
      "layers.0.self_attn.out_proj.weight 669 768\n",
      "layers.0.fc1.weight 748 768\n",
      "layers.0.fc2.weight 748 768\n",
      "layers.1.self_attn.k_proj.weight 649 768\n",
      "layers.1.self_attn.v_proj.weight 665 768\n",
      "layers.1.self_attn.q_proj.weight 644 768\n",
      "layers.1.self_attn.out_proj.weight 662 768\n",
      "layers.1.fc1.weight 745 768\n",
      "layers.1.fc2.weight 746 768\n",
      "layers.2.self_attn.k_proj.weight 639 768\n",
      "layers.2.self_attn.v_proj.weight 661 768\n",
      "layers.2.self_attn.q_proj.weight 638 768\n",
      "layers.2.self_attn.out_proj.weight 662 768\n",
      "layers.2.fc1.weight 744 768\n",
      "layers.2.fc2.weight 744 768\n",
      "layers.3.self_attn.k_proj.weight 649 768\n",
      "layers.3.self_attn.v_proj.weight 660 768\n",
      "layers.3.self_attn.q_proj.weight 649 768\n",
      "layers.3.self_attn.out_proj.weight 656 768\n",
      "layers.3.fc1.weight 745 768\n",
      "layers.3.fc2.weight 744 768\n",
      "layers.4.self_attn.k_proj.weight 645 768\n",
      "layers.4.self_attn.v_proj.weight 655 768\n",
      "layers.4.self_attn.q_proj.weight 646 768\n",
      "layers.4.self_attn.out_proj.weight 649 768\n",
      "layers.4.fc1.weight 747 768\n",
      "layers.4.fc2.weight 744 768\n",
      "layers.5.self_attn.k_proj.weight 653 768\n",
      "layers.5.self_attn.v_proj.weight 660 768\n",
      "layers.5.self_attn.q_proj.weight 654 768\n",
      "layers.5.self_attn.out_proj.weight 652 768\n",
      "layers.5.fc1.weight 747 768\n",
      "layers.5.fc2.weight 743 768\n",
      "layers.6.self_attn.k_proj.weight 644 768\n",
      "layers.6.self_attn.v_proj.weight 660 768\n",
      "layers.6.self_attn.q_proj.weight 647 768\n",
      "layers.6.self_attn.out_proj.weight 659 768\n",
      "layers.6.fc1.weight 747 768\n",
      "layers.6.fc2.weight 744 768\n",
      "layers.7.self_attn.k_proj.weight 643 768\n",
      "layers.7.self_attn.v_proj.weight 662 768\n",
      "layers.7.self_attn.q_proj.weight 645 768\n",
      "layers.7.self_attn.out_proj.weight 662 768\n",
      "layers.7.fc1.weight 746 768\n",
      "layers.7.fc2.weight 744 768\n",
      "layers.8.self_attn.k_proj.weight 637 768\n",
      "layers.8.self_attn.v_proj.weight 677 768\n",
      "layers.8.self_attn.q_proj.weight 640 768\n",
      "layers.8.self_attn.out_proj.weight 680 768\n",
      "layers.8.fc1.weight 746 768\n",
      "layers.8.fc2.weight 743 768\n",
      "layers.9.self_attn.k_proj.weight 638 768\n",
      "layers.9.self_attn.v_proj.weight 680 768\n",
      "layers.9.self_attn.q_proj.weight 641 768\n",
      "layers.9.self_attn.out_proj.weight 684 768\n",
      "layers.9.fc1.weight 746 768\n",
      "layers.9.fc2.weight 746 768\n",
      "layers.10.self_attn.k_proj.weight 646 768\n",
      "layers.10.self_attn.v_proj.weight 675 768\n",
      "layers.10.self_attn.q_proj.weight 646 768\n",
      "layers.10.self_attn.out_proj.weight 688 768\n",
      "layers.10.fc1.weight 745 768\n",
      "layers.10.fc2.weight 745 768\n",
      "layers.11.self_attn.k_proj.weight 652 768\n",
      "layers.11.self_attn.v_proj.weight 666 768\n",
      "layers.11.self_attn.q_proj.weight 650 768\n",
      "layers.11.self_attn.out_proj.weight 684 768\n",
      "layers.11.fc1.weight 735 768\n",
      "layers.11.fc2.weight 716 768\n",
      "dense.weight 717 768\n",
      "\n",
      "# Roberta Large Principle Directions \n",
      "\n",
      "embed_tokens.weight 1000 1024\n",
      "embed_positions.weight 482 514\n",
      "layers.0.self_attn.k_proj.weight 911 1024\n",
      "layers.0.self_attn.v_proj.weight 909 1024\n",
      "layers.0.self_attn.q_proj.weight 909 1024\n",
      "layers.0.self_attn.out_proj.weight 909 1024\n",
      "layers.0.fc1.weight 1001 1024\n",
      "layers.0.fc2.weight 1001 1024\n",
      "layers.1.self_attn.k_proj.weight 896 1024\n",
      "layers.1.self_attn.v_proj.weight 902 1024\n",
      "layers.1.self_attn.q_proj.weight 894 1024\n",
      "layers.1.self_attn.out_proj.weight 905 1024\n",
      "layers.1.fc1.weight 999 1024\n",
      "layers.1.fc2.weight 1002 1024\n",
      "layers.2.self_attn.k_proj.weight 901 1024\n",
      "layers.2.self_attn.v_proj.weight 901 1024\n",
      "layers.2.self_attn.q_proj.weight 899 1024\n",
      "layers.2.self_attn.out_proj.weight 883 1024\n",
      "layers.2.fc1.weight 999 1024\n",
      "layers.2.fc2.weight 1001 1024\n",
      "layers.3.self_attn.k_proj.weight 897 1024\n",
      "layers.3.self_attn.v_proj.weight 899 1024\n",
      "layers.3.self_attn.q_proj.weight 892 1024\n",
      "layers.3.self_attn.out_proj.weight 893 1024\n",
      "layers.3.fc1.weight 997 1024\n",
      "layers.3.fc2.weight 1000 1024\n",
      "layers.4.self_attn.k_proj.weight 896 1024\n",
      "layers.4.self_attn.v_proj.weight 901 1024\n",
      "layers.4.self_attn.q_proj.weight 890 1024\n",
      "layers.4.self_attn.out_proj.weight 900 1024\n",
      "layers.4.fc1.weight 996 1024\n",
      "layers.4.fc2.weight 998 1024\n",
      "layers.5.self_attn.k_proj.weight 891 1024\n",
      "layers.5.self_attn.v_proj.weight 902 1024\n",
      "layers.5.self_attn.q_proj.weight 887 1024\n",
      "layers.5.self_attn.out_proj.weight 904 1024\n",
      "layers.5.fc1.weight 994 1024\n",
      "layers.5.fc2.weight 997 1024\n",
      "layers.6.self_attn.k_proj.weight 888 1024\n",
      "layers.6.self_attn.v_proj.weight 899 1024\n",
      "layers.6.self_attn.q_proj.weight 886 1024\n",
      "layers.6.self_attn.out_proj.weight 900 1024\n",
      "layers.6.fc1.weight 994 1024\n",
      "layers.6.fc2.weight 995 1024\n",
      "layers.7.self_attn.k_proj.weight 892 1024\n",
      "layers.7.self_attn.v_proj.weight 898 1024\n",
      "layers.7.self_attn.q_proj.weight 889 1024\n",
      "layers.7.self_attn.out_proj.weight 901 1024\n",
      "layers.7.fc1.weight 994 1024\n",
      "layers.7.fc2.weight 993 1024\n",
      "layers.8.self_attn.k_proj.weight 888 1024\n",
      "layers.8.self_attn.v_proj.weight 891 1024\n",
      "layers.8.self_attn.q_proj.weight 885 1024\n",
      "layers.8.self_attn.out_proj.weight 890 1024\n",
      "layers.8.fc1.weight 996 1024\n",
      "layers.8.fc2.weight 998 1024\n",
      "layers.9.self_attn.k_proj.weight 882 1024\n",
      "layers.9.self_attn.v_proj.weight 890 1024\n",
      "layers.9.self_attn.q_proj.weight 880 1024\n",
      "layers.9.self_attn.out_proj.weight 894 1024\n",
      "layers.9.fc1.weight 997 1024\n",
      "layers.9.fc2.weight 997 1024\n",
      "layers.10.self_attn.k_proj.weight 880 1024\n",
      "layers.10.self_attn.v_proj.weight 882 1024\n",
      "layers.10.self_attn.q_proj.weight 880 1024\n",
      "layers.10.self_attn.out_proj.weight 879 1024\n",
      "layers.10.fc1.weight 991 1024\n",
      "layers.10.fc2.weight 998 1024\n",
      "layers.11.self_attn.k_proj.weight 876 1024\n",
      "layers.11.self_attn.v_proj.weight 890 1024\n",
      "layers.11.self_attn.q_proj.weight 877 1024\n",
      "layers.11.self_attn.out_proj.weight 896 1024\n",
      "layers.11.fc1.weight 989 1024\n",
      "layers.11.fc2.weight 996 1024\n",
      "layers.12.self_attn.k_proj.weight 873 1024\n",
      "layers.12.self_attn.v_proj.weight 890 1024\n",
      "layers.12.self_attn.q_proj.weight 874 1024\n",
      "layers.12.self_attn.out_proj.weight 899 1024\n",
      "layers.12.fc1.weight 987 1024\n",
      "layers.12.fc2.weight 996 1024\n",
      "layers.13.self_attn.k_proj.weight 884 1024\n",
      "layers.13.self_attn.v_proj.weight 893 1024\n",
      "layers.13.self_attn.q_proj.weight 882 1024\n",
      "layers.13.self_attn.out_proj.weight 894 1024\n",
      "layers.13.fc1.weight 987 1024\n",
      "layers.13.fc2.weight 995 1024\n",
      "layers.14.self_attn.k_proj.weight 884 1024\n",
      "layers.14.self_attn.v_proj.weight 894 1024\n",
      "layers.14.self_attn.q_proj.weight 884 1024\n",
      "layers.14.self_attn.out_proj.weight 896 1024\n",
      "layers.14.fc1.weight 993 1024\n",
      "layers.14.fc2.weight 997 1024\n",
      "layers.15.self_attn.k_proj.weight 891 1024\n",
      "layers.15.self_attn.v_proj.weight 895 1024\n",
      "layers.15.self_attn.q_proj.weight 889 1024\n",
      "layers.15.self_attn.out_proj.weight 897 1024\n",
      "layers.15.fc1.weight 993 1024\n",
      "layers.15.fc2.weight 998 1024\n",
      "layers.16.self_attn.k_proj.weight 886 1024\n",
      "layers.16.self_attn.v_proj.weight 892 1024\n",
      "layers.16.self_attn.q_proj.weight 882 1024\n",
      "layers.16.self_attn.out_proj.weight 884 1024\n",
      "layers.16.fc1.weight 997 1024\n",
      "layers.16.fc2.weight 1000 1024\n",
      "layers.17.self_attn.k_proj.weight 879 1024\n",
      "layers.17.self_attn.v_proj.weight 895 1024\n",
      "layers.17.self_attn.q_proj.weight 880 1024\n",
      "layers.17.self_attn.out_proj.weight 892 1024\n",
      "layers.17.fc1.weight 998 1024\n",
      "layers.17.fc2.weight 1000 1024\n",
      "layers.18.self_attn.k_proj.weight 872 1024\n",
      "layers.18.self_attn.v_proj.weight 897 1024\n",
      "layers.18.self_attn.q_proj.weight 875 1024\n",
      "layers.18.self_attn.out_proj.weight 896 1024\n",
      "layers.18.fc1.weight 997 1024\n",
      "layers.18.fc2.weight 999 1024\n",
      "layers.19.self_attn.k_proj.weight 870 1024\n",
      "layers.19.self_attn.v_proj.weight 899 1024\n",
      "layers.19.self_attn.q_proj.weight 873 1024\n",
      "layers.19.self_attn.out_proj.weight 899 1024\n",
      "layers.19.fc1.weight 997 1024\n",
      "layers.19.fc2.weight 1000 1024\n",
      "layers.20.self_attn.k_proj.weight 863 1024\n",
      "layers.20.self_attn.v_proj.weight 906 1024\n",
      "layers.20.self_attn.q_proj.weight 867 1024\n",
      "layers.20.self_attn.out_proj.weight 912 1024\n",
      "layers.20.fc1.weight 996 1024\n",
      "layers.20.fc2.weight 999 1024\n",
      "layers.21.self_attn.k_proj.weight 864 1024\n",
      "layers.21.self_attn.v_proj.weight 910 1024\n",
      "layers.21.self_attn.q_proj.weight 866 1024\n",
      "layers.21.self_attn.out_proj.weight 916 1024\n",
      "layers.21.fc1.weight 996 1024\n",
      "layers.21.fc2.weight 997 1024\n",
      "layers.22.self_attn.k_proj.weight 870 1024\n",
      "layers.22.self_attn.v_proj.weight 906 1024\n",
      "layers.22.self_attn.q_proj.weight 869 1024\n",
      "layers.22.self_attn.out_proj.weight 920 1024\n",
      "layers.22.fc1.weight 994 1024\n",
      "layers.22.fc2.weight 993 1024\n",
      "layers.23.self_attn.k_proj.weight 878 1024\n",
      "layers.23.self_attn.v_proj.weight 898 1024\n",
      "layers.23.self_attn.q_proj.weight 872 1024\n",
      "layers.23.self_attn.out_proj.weight 912 1024\n",
      "layers.23.fc1.weight 988 1024\n",
      "layers.23.fc2.weight 949 1024\n",
      "dense.weight 954 1024\n"
     ]
    }
   ],
   "source": [
    "explained_variance = 0.99\n",
    "print(\"# Roberta Base Principle Directions \\n\")\n",
    "print_principle_direction(roberta_base, explained_variance)\n",
    "\n",
    "print(\"\\n# Roberta Large Principle Directions \\n\")\n",
    "print_principle_direction(roberta_large, explained_variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a39e3e-2c84-458e-a47f-5e5a86443278",
   "metadata": {},
   "source": [
    "On the surface, the above results suggest you need most of the directions to account for the explained variance in the parameters.\n",
    "As such, we finetune the models for SQuAD and review the difference in the weights of the base and the tuned model. If the difference has \n",
    "fewer principle directions with high explanation, it would mean it is possible the task-based fine-tuning can be done in a smaller subspace. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a1ff52-126c-4665-92fd-c3e5d8c83fb2",
   "metadata": {},
   "source": [
    "## Rank Variation During Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d59c3413-b156-4dfd-a1f5-1df488ff818b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Dataset\n",
    "import hydra\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from train.train import train_epoch\n",
    "from utils.metrics import AverageMeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "719ff899-edf9-4f7e-b665-ea440ef6bc8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:hydra.core.utils:Setting JobRuntime:name=notebook\n"
     ]
    }
   ],
   "source": [
    "# Extract Training Configuration\n",
    "with hydra.initialize(version_base=None, config_path=\"../config\"):\n",
    "    cfg = hydra.compose(config_name=\"app_config\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4b82e45-bb22-4aea-b6ac-1c2ef67ab9d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"GET /api/datasets/squad HTTP/1.1\" 307 64\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"GET /api/datasets/rajpurkar/squad HTTP/1.1\" 200 2859\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
      "DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 \"HEAD /datasets.huggingface.co/datasets/datasets/squad/squad.py HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"GET /api/datasets/squad HTTP/1.1\" 307 64\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"GET /api/datasets/rajpurkar/squad HTTP/1.1\" 200 2859\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /datasets/squad/resolve/7b6d24c440a36b6815f21b70d25016731768db1f/README.md HTTP/1.1\" 307 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /datasets/rajpurkar/squad/resolve/7b6d24c440a36b6815f21b70d25016731768db1f/README.md HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /datasets/squad/resolve/7b6d24c440a36b6815f21b70d25016731768db1f/.huggingface.yaml HTTP/1.1\" 307 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /datasets/rajpurkar/squad/resolve/7b6d24c440a36b6815f21b70d25016731768db1f/.huggingface.yaml HTTP/1.1\" 404 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): datasets-server.huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://datasets-server.huggingface.co:443 \"GET /info?dataset=squad HTTP/1.1\" 500 None\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"POST /api/datasets/squad/paths-info/7b6d24c440a36b6815f21b70d25016731768db1f HTTP/1.1\" 307 116\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"POST /api/datasets/rajpurkar/squad/paths-info/7b6d24c440a36b6815f21b70d25016731768db1f HTTP/1.1\" 200 102\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"POST /api/datasets/squad/paths-info/7b6d24c440a36b6815f21b70d25016731768db1f HTTP/1.1\" 307 116\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"POST /api/datasets/rajpurkar/squad/paths-info/7b6d24c440a36b6815f21b70d25016731768db1f HTTP/1.1\" 200 241\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"POST /api/datasets/squad/paths-info/7b6d24c440a36b6815f21b70d25016731768db1f HTTP/1.1\" 307 116\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"POST /api/datasets/rajpurkar/squad/paths-info/7b6d24c440a36b6815f21b70d25016731768db1f HTTP/1.1\" 200 102\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"POST /api/datasets/squad/paths-info/7b6d24c440a36b6815f21b70d25016731768db1f HTTP/1.1\" 307 116\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"POST /api/datasets/rajpurkar/squad/paths-info/7b6d24c440a36b6815f21b70d25016731768db1f HTTP/1.1\" 200 241\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /datasets/squad/resolve/7b6d24c440a36b6815f21b70d25016731768db1f/dataset_infos.json HTTP/1.1\" 307 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /datasets/rajpurkar/squad/resolve/7b6d24c440a36b6815f21b70d25016731768db1f/dataset_infos.json HTTP/1.1\" 404 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"POST /api/datasets/squad/paths-info/7b6d24c440a36b6815f21b70d25016731768db1f HTTP/1.1\" 307 116\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"POST /api/datasets/rajpurkar/squad/paths-info/7b6d24c440a36b6815f21b70d25016731768db1f HTTP/1.1\" 200 102\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"POST /api/datasets/squad/paths-info/7b6d24c440a36b6815f21b70d25016731768db1f HTTP/1.1\" 307 116\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"POST /api/datasets/rajpurkar/squad/paths-info/7b6d24c440a36b6815f21b70d25016731768db1f HTTP/1.1\" 200 241\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"POST /api/datasets/squad/paths-info/7b6d24c440a36b6815f21b70d25016731768db1f HTTP/1.1\" 307 116\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"POST /api/datasets/rajpurkar/squad/paths-info/7b6d24c440a36b6815f21b70d25016731768db1f HTTP/1.1\" 200 102\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"POST /api/datasets/squad/paths-info/7b6d24c440a36b6815f21b70d25016731768db1f HTTP/1.1\" 307 116\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"POST /api/datasets/rajpurkar/squad/paths-info/7b6d24c440a36b6815f21b70d25016731768db1f HTTP/1.1\" 200 241\n",
      "DEBUG:filelock:Attempting to acquire lock 125587644744656 on /home/mathadoor/.cache/huggingface/datasets/_home_mathadoor_.cache_huggingface_datasets_squad_plain_text_0.0.0_7b6d24c440a36b6815f21b70d25016731768db1f.lock\n",
      "DEBUG:filelock:Lock 125587644744656 acquired on /home/mathadoor/.cache/huggingface/datasets/_home_mathadoor_.cache_huggingface_datasets_squad_plain_text_0.0.0_7b6d24c440a36b6815f21b70d25016731768db1f.lock\n",
      "DEBUG:fsspec.local:open file: /home/mathadoor/.cache/huggingface/datasets/squad/plain_text/0.0.0/7b6d24c440a36b6815f21b70d25016731768db1f/dataset_info.json\n",
      "DEBUG:filelock:Attempting to release lock 125587644744656 on /home/mathadoor/.cache/huggingface/datasets/_home_mathadoor_.cache_huggingface_datasets_squad_plain_text_0.0.0_7b6d24c440a36b6815f21b70d25016731768db1f.lock\n",
      "DEBUG:filelock:Lock 125587644744656 released on /home/mathadoor/.cache/huggingface/datasets/_home_mathadoor_.cache_huggingface_datasets_squad_plain_text_0.0.0_7b6d24c440a36b6815f21b70d25016731768db1f.lock\n",
      "DEBUG:filelock:Attempting to acquire lock 125587644747248 on /home/mathadoor/.cache/huggingface/datasets/squad/plain_text/0.0.0/7b6d24c440a36b6815f21b70d25016731768db1f_builder.lock\n",
      "DEBUG:filelock:Lock 125587644747248 acquired on /home/mathadoor/.cache/huggingface/datasets/squad/plain_text/0.0.0/7b6d24c440a36b6815f21b70d25016731768db1f_builder.lock\n",
      "DEBUG:fsspec.local:open file: /home/mathadoor/.cache/huggingface/datasets/squad/plain_text/0.0.0/7b6d24c440a36b6815f21b70d25016731768db1f/dataset_info.json\n",
      "DEBUG:filelock:Attempting to release lock 125587644747248 on /home/mathadoor/.cache/huggingface/datasets/squad/plain_text/0.0.0/7b6d24c440a36b6815f21b70d25016731768db1f_builder.lock\n",
      "DEBUG:filelock:Lock 125587644747248 released on /home/mathadoor/.cache/huggingface/datasets/squad/plain_text/0.0.0/7b6d24c440a36b6815f21b70d25016731768db1f_builder.lock\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"GET /api/datasets/squad HTTP/1.1\" 307 64\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"GET /api/datasets/rajpurkar/squad HTTP/1.1\" 200 2859\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
      "DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 \"HEAD /datasets.huggingface.co/datasets/datasets/squad/squad.py HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"GET /api/datasets/squad HTTP/1.1\" 307 64\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"GET /api/datasets/rajpurkar/squad HTTP/1.1\" 200 2859\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /datasets/squad/resolve/7b6d24c440a36b6815f21b70d25016731768db1f/README.md HTTP/1.1\" 307 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /datasets/rajpurkar/squad/resolve/7b6d24c440a36b6815f21b70d25016731768db1f/README.md HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /datasets/squad/resolve/7b6d24c440a36b6815f21b70d25016731768db1f/.huggingface.yaml HTTP/1.1\" 307 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /datasets/rajpurkar/squad/resolve/7b6d24c440a36b6815f21b70d25016731768db1f/.huggingface.yaml HTTP/1.1\" 404 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): datasets-server.huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://datasets-server.huggingface.co:443 \"GET /info?dataset=squad HTTP/1.1\" 500 None\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"POST /api/datasets/squad/paths-info/7b6d24c440a36b6815f21b70d25016731768db1f HTTP/1.1\" 307 116\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"POST /api/datasets/rajpurkar/squad/paths-info/7b6d24c440a36b6815f21b70d25016731768db1f HTTP/1.1\" 200 102\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"POST /api/datasets/squad/paths-info/7b6d24c440a36b6815f21b70d25016731768db1f HTTP/1.1\" 307 116\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"POST /api/datasets/rajpurkar/squad/paths-info/7b6d24c440a36b6815f21b70d25016731768db1f HTTP/1.1\" 200 241\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"POST /api/datasets/squad/paths-info/7b6d24c440a36b6815f21b70d25016731768db1f HTTP/1.1\" 307 116\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"POST /api/datasets/rajpurkar/squad/paths-info/7b6d24c440a36b6815f21b70d25016731768db1f HTTP/1.1\" 200 102\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"POST /api/datasets/squad/paths-info/7b6d24c440a36b6815f21b70d25016731768db1f HTTP/1.1\" 307 116\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"POST /api/datasets/rajpurkar/squad/paths-info/7b6d24c440a36b6815f21b70d25016731768db1f HTTP/1.1\" 200 241\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /datasets/squad/resolve/7b6d24c440a36b6815f21b70d25016731768db1f/dataset_infos.json HTTP/1.1\" 307 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /datasets/rajpurkar/squad/resolve/7b6d24c440a36b6815f21b70d25016731768db1f/dataset_infos.json HTTP/1.1\" 404 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"POST /api/datasets/squad/paths-info/7b6d24c440a36b6815f21b70d25016731768db1f HTTP/1.1\" 307 116\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"POST /api/datasets/rajpurkar/squad/paths-info/7b6d24c440a36b6815f21b70d25016731768db1f HTTP/1.1\" 200 102\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"POST /api/datasets/squad/paths-info/7b6d24c440a36b6815f21b70d25016731768db1f HTTP/1.1\" 307 116\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"POST /api/datasets/rajpurkar/squad/paths-info/7b6d24c440a36b6815f21b70d25016731768db1f HTTP/1.1\" 200 241\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"POST /api/datasets/squad/paths-info/7b6d24c440a36b6815f21b70d25016731768db1f HTTP/1.1\" 307 116\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"POST /api/datasets/rajpurkar/squad/paths-info/7b6d24c440a36b6815f21b70d25016731768db1f HTTP/1.1\" 200 102\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"POST /api/datasets/squad/paths-info/7b6d24c440a36b6815f21b70d25016731768db1f HTTP/1.1\" 307 116\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"POST /api/datasets/rajpurkar/squad/paths-info/7b6d24c440a36b6815f21b70d25016731768db1f HTTP/1.1\" 200 241\n",
      "DEBUG:filelock:Attempting to acquire lock 125587670262656 on /home/mathadoor/.cache/huggingface/datasets/_home_mathadoor_.cache_huggingface_datasets_squad_plain_text_0.0.0_7b6d24c440a36b6815f21b70d25016731768db1f.lock\n",
      "DEBUG:filelock:Lock 125587670262656 acquired on /home/mathadoor/.cache/huggingface/datasets/_home_mathadoor_.cache_huggingface_datasets_squad_plain_text_0.0.0_7b6d24c440a36b6815f21b70d25016731768db1f.lock\n",
      "DEBUG:fsspec.local:open file: /home/mathadoor/.cache/huggingface/datasets/squad/plain_text/0.0.0/7b6d24c440a36b6815f21b70d25016731768db1f/dataset_info.json\n",
      "DEBUG:filelock:Attempting to release lock 125587670262656 on /home/mathadoor/.cache/huggingface/datasets/_home_mathadoor_.cache_huggingface_datasets_squad_plain_text_0.0.0_7b6d24c440a36b6815f21b70d25016731768db1f.lock\n",
      "DEBUG:filelock:Lock 125587670262656 released on /home/mathadoor/.cache/huggingface/datasets/_home_mathadoor_.cache_huggingface_datasets_squad_plain_text_0.0.0_7b6d24c440a36b6815f21b70d25016731768db1f.lock\n",
      "DEBUG:filelock:Attempting to acquire lock 125587670262656 on /home/mathadoor/.cache/huggingface/datasets/squad/plain_text/0.0.0/7b6d24c440a36b6815f21b70d25016731768db1f_builder.lock\n",
      "DEBUG:filelock:Lock 125587670262656 acquired on /home/mathadoor/.cache/huggingface/datasets/squad/plain_text/0.0.0/7b6d24c440a36b6815f21b70d25016731768db1f_builder.lock\n",
      "DEBUG:fsspec.local:open file: /home/mathadoor/.cache/huggingface/datasets/squad/plain_text/0.0.0/7b6d24c440a36b6815f21b70d25016731768db1f/dataset_info.json\n",
      "DEBUG:filelock:Attempting to release lock 125587670262656 on /home/mathadoor/.cache/huggingface/datasets/squad/plain_text/0.0.0/7b6d24c440a36b6815f21b70d25016731768db1f_builder.lock\n",
      "DEBUG:filelock:Lock 125587670262656 released on /home/mathadoor/.cache/huggingface/datasets/squad/plain_text/0.0.0/7b6d24c440a36b6815f21b70d25016731768db1f_builder.lock\n"
     ]
    }
   ],
   "source": [
    "train_dataset = load_dataset(\"squad\", split=\"train\")\n",
    "val_dataset = load_dataset(\"squad\", split=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0a7bcbd0-1ab7-48b6-b890-79dddf7abbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Training Artifacts\n",
    "training_cfg = hydra.utils.instantiate(cfg.model.model.train)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=training_cfg.batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=training_cfg.batch_size)\n",
    "\n",
    "model = roberta_base\n",
    "optimizer = AdamW(params=model.parameters(), lr=training_cfg.lr, weight_decay=training_cfg.weight_decay)\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "loss_meter = AverageMeter()\n",
    "acc_meter = AverageMeter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e781f4c5-9bf2-4b92-a0be-184d54fc97aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3c1c168-4382-4963-bab5-e723a863a4f7",
   "metadata": {},
   "source": [
    "# Introducing LoRA Weights\n",
    "This section introduces LoRA weights in our models for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec9a0bca-9500-4e4f-9650-23bfc0a02439",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# On the surface the above results suggest you need most of the directions to account for most of the directions. \n",
    "# It will be useful to see which directions are affected and how later.\n",
    "from models.LoRA import LoRALinearLayer\n",
    "def add_linear_lora(module, rank, init_type=0):\n",
    "    for key, child in module.named_children():\n",
    "        if isinstance(child, torch.nn.Linear):\n",
    "            lora_layer =  LoRALinearLayer(child, rank=rank, init_type=init_type)\n",
    "            setattr(module, key, lora_layer)\n",
    "        else:\n",
    "            add_linear_lora(child, rank, init_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa02e6c0-2f89-40ab-b387-9c77de31b5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_linear_lora(roberta_base, rank=10, init_type=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe6c9164-4b29-4ad1-b98f-a50c4f368e7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaHubInterface(\n",
       "  (model): RobertaModel(\n",
       "    (encoder): RobertaEncoder(\n",
       "      (sentence_encoder): TransformerEncoder(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
       "        (embed_positions): LearnedPositionalEmbedding(514, 768, padding_idx=1)\n",
       "        (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (layers): ModuleList(\n",
       "          (0-11): 12 x TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): LoRALinear(in_features=768, in_features=768, rank=$10, init_type=$1)\n",
       "              (v_proj): LoRALinear(in_features=768, in_features=768, rank=$10, init_type=$1)\n",
       "              (q_proj): LoRALinear(in_features=768, in_features=768, rank=$10, init_type=$1)\n",
       "              (out_proj): LoRALinear(in_features=768, in_features=768, rank=$10, init_type=$1)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): LoRALinear(in_features=768, in_features=3072, rank=$10, init_type=$1)\n",
       "            (fc2): LoRALinear(in_features=3072, in_features=768, rank=$10, init_type=$1)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (lm_head): RobertaLMHead(\n",
       "        (dense): LoRALinear(in_features=768, in_features=768, rank=$10, init_type=$1)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (classification_heads): ModuleDict()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c66542b-d1f8-417c-bae4-172a31970431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaHubInterface(\n",
       "  (model): RobertaModel(\n",
       "    (encoder): RobertaEncoder(\n",
       "      (sentence_encoder): TransformerEncoder(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
       "        (embed_positions): LearnedPositionalEmbedding(514, 1024, padding_idx=1)\n",
       "        (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (layers): ModuleList(\n",
       "          (0-23): 24 x TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (lm_head): RobertaLMHead(\n",
       "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (classification_heads): ModuleDict()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff436bb-df8a-4baa-af98-0be01d85c430",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
