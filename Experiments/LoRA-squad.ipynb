{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e45fd35-cdca-44b8-a00f-607078ee1a67",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Investigating Ranks\n",
    "In the first section, I am going to investigate what an appropriate rank to consider should be. It will make sense to look at the \n",
    "ranks of a trained matrix and a randomly initialized matrix. We will start by looking at a randomly initialized matrix and then\n",
    "look at a trained matrix. Specifically, I will look at the rank of a randomly initialized matrix. I will need to consider the sizes \n",
    "that are seen in RoBERTa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1397017-4851-477f-a432-2a934510e545",
   "metadata": {},
   "source": [
    "## Imports and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f2172b3-4630-4fed-8bfd-4cf8b3ea7ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Appropriate Libraries\n",
    "%load_ext autoreload\n",
    "    \n",
    "import torch\n",
    "import matplotlib.pyplot as plt \n",
    "import bisect\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "# Set Logging Level\n",
    "import logging\n",
    "level = logging.DEBUG\n",
    "logging.getLogger(\"requests\").setLevel(level)\n",
    "logging.getLogger(\"urllib3\").setLevel(level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25dba68b-c775-4672-8fbf-f2bb3da6f1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_principle_direction(A, exp_var):\n",
    "    U, S, V = torch.linalg.svd(A)\n",
    "    X = (torch.cumsum(S, 0) / S.sum().item()).tolist()\n",
    "    num = bisect.bisect(X, exp_var)\n",
    "    return num"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35846b95-4743-425e-a2bd-ac1db674217a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Normally Initialized Matrix Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a4005e5-d964-4def-94f1-41d18030aeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(n=100, size=[768, 768], exp_var=0.99):\n",
    "    results = []\n",
    "    for _ in range(n):\n",
    "        A = torch.randn(size)\n",
    "        num = get_principle_direction(A, exp_var)\n",
    "        results.append(num)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86b27c30-3205-4635-b424-9e5e80078d10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([51.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 49.]),\n",
       " array([515. , 515.1, 515.2, 515.3, 515.4, 515.5, 515.6, 515.7, 515.8,\n",
       "        515.9, 516. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0QAAAESCAYAAADdURXtAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAx4klEQVR4nO3df1RVdb7/8RcIHCg8ICigCWaTI6bpFCpSTeMYSd6mLJlmpuVMP8ZVU4OWMv3i3jEnp8KpmXRq0Mxr/rijYzl3LO2HriKzaQJTzNK6oZYtGBXsZoDYeCR5f//oy7mdBOTAgXNgPx9r7bU6n7357M+bD73d7/PZZ58wMzMBAAAAgAOFB3sAAAAAABAsFEQAAAAAHIuCCAAAAIBjURABAAAAcCwKIgAAAACORUEEAAAAwLEoiAAAAAA4VkSwB/BNjY2NOnjwoHr37q2wsLBgDwdwNDPT0aNHNWDAAIWHd5/3T8gjQOjojnmEHAKEjq7IISFXEB08eFCpqanBHgaAr6msrNTAgQODPYw2I48Aoac75RFyCBB6OjOHhFxB1Lt3b0lfBe12u4M8GsDZ6urqlJqa6v3/srsgjwChozvmEXIIEDq6IoeEXEHUtDTtdrtJQkCI6G63jJBHgNDTnfIIOQQIPZ2ZQ7rHzbwAAAAA0AkoiAAAAAA4FgURAAAAAMeiIAIAAADgWBREAAAAAByLgggAAACAY1EQAQAAAHAsCiIAXerkyZOaPXu2Bg8erJiYGH3rW9/Sb3/7W5mZ9xgz0/3336/+/fsrJiZG2dnZ2rt3bxBHDQAAeqqQ+2JWf51934sB7e+TeVcGtD8Avn73u99p0aJFWrFihYYPH67t27fr5ptvVlxcnO644w5J0iOPPKLHH39cK1as0ODBgzV79mzl5OTogw8+UHR0dJAjQFcKdI6XyPMIPK5FgO6NFSIAXeqtt97S5MmTdeWVV+rss8/WD3/4Q02cOFFvv/22pK9WhxYsWKBf//rXmjx5skaOHKmVK1fq4MGDeu6554I7eABB95vf/EZhYWE+W3p6unf/8ePHlZeXp8TERMXGxio3N1fV1dVBHDGAUEdBBKBLXXTRRSouLtaePXskSe+++67efPNNTZo0SZK0f/9+VVVVKTs72/szcXFxyszMVElJSbN9ejwe1dXV+WwAeq7hw4fr0KFD3u3NN9/07ps1a5Y2bNigtWvXasuWLTp48KCmTJkSxNECCHXd/pY5AN3Lfffdp7q6OqWnp6tXr146efKkHnroIU2dOlWSVFVVJUlKTk72+bnk5GTvvm8qLCzUAw880LkDBxAyIiIilJKSckp7bW2tli5dqtWrV2vChAmSpGXLlmnYsGEqLS3VuHHjunqoALoBVogAdKlnn31Wq1at0urVq7Vjxw6tWLFCv//977VixYp291lQUKDa2lrvVllZGcARAwg1e/fu1YABA3TOOedo6tSpqqiokCSVlZWpoaHBZ4U5PT1daWlpLa4wS6wyA07nV0HEfbsAOuruu+/Wfffdp5/85Cc6//zz9bOf/UyzZs1SYWGhJHnf9f1m7qiurm72HWFJcrlccrvdPhuAnikzM1PLly/Xxo0btWjRIu3fv1/f/e53dfToUVVVVSkqKkrx8fE+P9PaCrP01SpzXFycd0tNTe3kKACEEr9XiLhvF0BHfPHFFwoP9009vXr1UmNjoyRp8ODBSklJUXFxsXd/XV2dtm7dqqysrC4dK4DQM2nSJF133XUaOXKkcnJy9NJLL6mmpkbPPvtsu/tklRlwNr8/Q8R9uwA64qqrrtJDDz2ktLQ0DR8+XO+8844ee+wx/fznP5ckhYWFaebMmXrwwQc1ZMgQ72O3BwwYoGuuuSa4gwcQcuLj4/Xtb39b+/bt0+WXX64TJ06opqbGZ5WotRVm6atVZpfL1QWjBRCK/F4h4r5dAB3xxBNP6Ic//KF++ctfatiwYbrrrrv0i1/8Qr/97W+9x9xzzz2aMWOGbr31Vo0ZM0b19fXauHEj30EE4BT19fX66KOP1L9/f2VkZCgyMtJnhbm8vFwVFRWsMANokV8rRE337Q4dOlSHDh3SAw88oO9+97vavXt3h+7b5elQgHP07t1bCxYs0IIFC1o8JiwsTHPnztXcuXO7bmAAuoW77rpLV111lQYNGqSDBw9qzpw56tWrl66//nrFxcVp2rRpys/PV0JCgtxut2bMmKGsrCzuVAE6qCd/UbZfBVHT94RI0siRI5WZmalBgwbp2WefVUxMTLsGUFBQoPz8fO/ruro6PswIAACa9c9//lPXX3+9PvvsM/Xr10+XXHKJSktL1a9fP0nS/PnzFR4ertzcXHk8HuXk5GjhwoVBHjWAUNah7yHivl0AANCV1qxZ0+r+6OhoFRUVqaioqItGBKC769D3EHHfLgAAAIDuzK8VIu7bBQAAANCT+FUQcd8uAAAAgJ7Er4KI+3YBAAAA9CQd+gwRAAAAAHRnFEQAAAAAHIuCCAAAAIBjURABAAAAcCwKIgAAAACO5ddT5gB/nH3fiwHv85N5Vwa8TwAAADgXK0QAAAAAHIuCCAAAAIBjURABAAAAcCwKIgAAAACORUEEAAAAwLEoiAAAAAA4FgURAAAAAMeiIAIAAADgWBREAAAAAByLgggAAACAY1EQAQAAAHAsCiIAAAAAjkVBBAAAAMCxKIgAAAAAOBYFEQAAAADHoiACAAAA4FgURAAAAAAci4IIAAAAgGNREAEAAABwLAoiAAAAAI5FQQQAAADAsSiIAAAAADgWBREAAOi25s2bp7CwMM2cOdPbdvz4ceXl5SkxMVGxsbHKzc1VdXV18AYJIKRREAEAgG5p27ZtWrx4sUaOHOnTPmvWLG3YsEFr167Vli1bdPDgQU2ZMiVIowQQ6iiIAABAt1NfX6+pU6dqyZIl6tOnj7e9trZWS5cu1WOPPaYJEyYoIyNDy5Yt01tvvaXS0tIgjhhAqOpQQcQyNQAACIa8vDxdeeWVys7O9mkvKytTQ0ODT3t6errS0tJUUlLSbF8ej0d1dXU+GwDnaHdBxDI1AAAIhjVr1mjHjh0qLCw8ZV9VVZWioqIUHx/v056cnKyqqqpm+yssLFRcXJx3S01N7YxhAwhR7SqIWKYGAADBUFlZqTvvvFOrVq1SdHR0QPosKChQbW2td6usrAxIvwC6h3YVRCxTAwCAYCgrK9Phw4d14YUXKiIiQhEREdqyZYsef/xxRUREKDk5WSdOnFBNTY3Pz1VXVyslJaXZPl0ul9xut88GwDki/P2BpmXqbdu2nbKvvcvUDzzwgL/DAAAADnTZZZdp165dPm0333yz0tPTde+99yo1NVWRkZEqLi5Wbm6uJKm8vFwVFRXKysoKxpABhDi/CqKmZepXXnkloMvU+fn53td1dXXcuwsAAJrVu3dvjRgxwqftzDPPVGJiord92rRpys/PV0JCgtxut2bMmKGsrCyNGzcuGEMGEOL8Koi+vkzd5OTJk3rjjTf0pz/9SZs2bfIuU399leh0y9Qul6t9owcAAPiG+fPnKzw8XLm5ufJ4PMrJydHChQuDPSwAIcqvzxA1LVPv3LnTu40ePVpTp071/nfTMnUTlqkBfNOBAwf005/+VImJiYqJidH555+v7du3e/ebme6//371799fMTExys7O1t69e4M4YgCh7PXXX9eCBQu8r6Ojo1VUVKQjR47o2LFj+tvf/tbiG7MA4NcKEcvUADrq888/18UXX6zvf//7evnll9WvXz/t3bvX54mVjzzyiB5//HGtWLFCgwcP1uzZs5WTk6MPPvggYLfrAgAASO14qMLpsEwNoDW/+93vlJqaqmXLlnnbBg8e7P1vM9OCBQv061//WpMnT5YkrVy5UsnJyXruuef0k5/85JQ+PR6PPB6P9zVPqwQAAG3V7i9mbcIyNQB/rF+/XqNHj9Z1112npKQkXXDBBVqyZIl3//79+1VVVeXz+P64uDhlZma2+Ph+vlQRAAC0V4cLIgDwx8cff6xFixZpyJAh2rRpk26//XbdcccdWrFihSR5H9GfnJzs83OtPb6fL1UEAADtFfBb5gCgNY2NjRo9erQefvhhSdIFF1yg3bt368knn9SNN97Yrj55WiUAAGgvVogAdKn+/fvrvPPO82kbNmyYKioqJMl7i211dbXPMa09vh8AAKC9KIgAdKmLL75Y5eXlPm179uzRoEGDJH31gIWUlBSfx/fX1dVp69atPL4fAAAEHLfMAehSs2bN0kUXXaSHH35YP/rRj/T222/rqaee0lNPPSVJCgsL08yZM/Xggw9qyJAh3sduDxgwQNdcc01wBw8AAHocCiIAXWrMmDFat26dCgoKNHfuXA0ePFgLFizQ1KlTvcfcc889OnbsmG699VbV1NTokksu0caNG/kOIgAAEHAURAC63A9+8AP94Ac/aHF/WFiY5s6dq7lz53bhqAAAgBPxGSIAAAAAjkVBBAAAAMCxKIgAAAAAOBYFEQAAAADHoiACAAAA4FgURAAAAAAci4IIAAAAgGNREAEAAABwLAoiAAAAAI5FQQQAAADAsSiIAAAAADgWBREAAAAAx6IgAgAAAOBYFEQAAAAAHIuCCAAAAIBjURABAAAAcCwKIgAAAACORUEEAAAAwLEoiAAAAAA4FgURAAAAAMeiIAIAAADgWBREAAAAAByLgggAAHQbixYt0siRI+V2u+V2u5WVlaWXX37Zu//48ePKy8tTYmKiYmNjlZubq+rq6iCOGECooyACAADdxsCBAzVv3jyVlZVp+/btmjBhgiZPnqz3339fkjRr1ixt2LBBa9eu1ZYtW3Tw4EFNmTIlyKMGEMoigj0AAACAtrrqqqt8Xj/00ENatGiRSktLNXDgQC1dulSrV6/WhAkTJEnLli3TsGHDVFpaqnHjxgVjyABCnF8rRCxTAwCAUHHy5EmtWbNGx44dU1ZWlsrKytTQ0KDs7GzvMenp6UpLS1NJSUmL/Xg8HtXV1flsAJzDr4KIZWoAABBsu3btUmxsrFwul2677TatW7dO5513nqqqqhQVFaX4+Hif45OTk1VVVdVif4WFhYqLi/NuqampnRwBgFDi1y1znbFM7fF45PF4vK95VwYAALRm6NCh2rlzp2pra/XXv/5VN954o7Zs2dLu/goKCpSfn+99XVdXR1EEOEi7H6oQqGVq3pUBAAD+iIqK0rnnnquMjAwVFhZq1KhR+uMf/6iUlBSdOHFCNTU1PsdXV1crJSWlxf5cLpf34wBNGwDn8LsgCvQydUFBgWpra71bZWWl30EAAADnamxslMfjUUZGhiIjI1VcXOzdV15eroqKCmVlZQVxhABCmd9PmQv0MrXL5ZLL5Wr3zwMAAOcoKCjQpEmTlJaWpqNHj2r16tV6/fXXtWnTJsXFxWnatGnKz89XQkKC3G63ZsyYoaysLJ4wB6BFfhdETcvUkpSRkaFt27bpj3/8o3784x97l6m/vkp0umVqAACAtjp8+LBuuOEGHTp0SHFxcRo5cqQ2bdqkyy+/XJI0f/58hYeHKzc3Vx6PRzk5OVq4cGGQRw0glHX4e4iaW6bOzc2VxDI1AAAIrKVLl7a6Pzo6WkVFRSoqKuqiEQHo7vwqiFimBgAAANCT+FUQsUwNAAAAoCfxqyBimRoAAABAT9Lu7yECAAAAgO6OgggAAACAY1EQAQAAAHAsCiIAAAAAjkVBBAAAAMCxKIgAAAAAOBYFEQAAAADHoiACEDTz5s1TWFiYZs6c6W07fvy48vLylJiYqNjYWOXm5qq6ujp4gwQAAD0aBRGAoNi2bZsWL16skSNH+rTPmjVLGzZs0Nq1a7VlyxYdPHhQU6ZMCdIoAQBAT0dBBKDL1dfXa+rUqVqyZIn69Onjba+trdXSpUv12GOPacKECcrIyNCyZcv01ltvqbS0NIgjBgAAPRUFEYAul5eXpyuvvFLZ2dk+7WVlZWpoaPBpT09PV1pamkpKSlrsz+PxqK6uzmcDAABoi4hgDwCAs6xZs0Y7duzQtm3bTtlXVVWlqKgoxcfH+7QnJyerqqqqxT4LCwv1wAMPBHqoAADAAVghAtBlKisrdeedd2rVqlWKjo4OWL8FBQWqra31bpWVlQHrGwAA9GwURAC6TFlZmQ4fPqwLL7xQERERioiI0JYtW/T4448rIiJCycnJOnHihGpqanx+rrq6WikpKS3263K55Ha7fTYAAIC24JY5AF3msssu065du3zabr75ZqWnp+vee+9VamqqIiMjVVxcrNzcXElSeXm5KioqlJWVFYwhAwCAHo6CCECX6d27t0aMGOHTduaZZyoxMdHbPm3aNOXn5yshIUFut1szZsxQVlaWxo0bF4whAwCAHo6CCEBImT9/vsLDw5WbmyuPx6OcnBwtXLgw2MMCAAA9FAURgKB6/fXXfV5HR0erqKhIRUVFwRkQAABwFB6qAAAAAMCxKIgAAAAAOBYFEQAAAADHoiACAAAA4FgURAAAAAAci4IIAAAAgGNREAEAAABwLAoiAAAAAI5FQQQAAADAsSiIAAAAADgWBREAAAAAx6IgAgAA3UZhYaHGjBmj3r17KykpSddcc43Ky8t9jjl+/Ljy8vKUmJio2NhY5ebmqrq6OkgjBhDqKIgAAEC3sWXLFuXl5am0tFSvvPKKGhoaNHHiRB07dsx7zKxZs7RhwwatXbtWW7Zs0cGDBzVlypQgjhpAKPOrIOJdGQAAEEwbN27UTTfdpOHDh2vUqFFavny5KioqVFZWJkmqra3V0qVL9dhjj2nChAnKyMjQsmXL9NZbb6m0tDTIowcQivwqiHhXBgAAhJLa2lpJUkJCgiSprKxMDQ0Nys7O9h6Tnp6utLQ0lZSUNNuHx+NRXV2dzwbAOSL8OXjjxo0+r5cvX66kpCSVlZXp0ksv9b4rs3r1ak2YMEGStGzZMg0bNkylpaUaN25c4EYOAAAcrbGxUTNnztTFF1+sESNGSJKqqqoUFRWl+Ph4n2OTk5NVVVXVbD+FhYV64IEHOnu4AEJUhz5DxLsyAAAgWPLy8rR7926tWbOmQ/0UFBSotrbWu1VWVgZohAC6g3YXRIF8VyYuLs67paamtndIAADAIaZPn64XXnhBmzdv1sCBA73tKSkpOnHihGpqanyOr66uVkpKSrN9uVwuud1unw2Ac7S7IOJdGQAA0NXMTNOnT9e6dev02muvafDgwT77MzIyFBkZqeLiYm9beXm5KioqlJWV1dXDBdAN+PUZoiZN78q88cYbLb4r8/VVotO9K+NyudozDAAA4DB5eXlavXq1nn/+efXu3dt7B0pcXJxiYmIUFxenadOmKT8/XwkJCXK73ZoxY4aysrL4LDOAZvm1QsS7MgAAIJgWLVqk2tpajR8/Xv379/duzzzzjPeY+fPn6wc/+IFyc3N16aWXKiUlRX/729+COGoAocyvFSLelQEAAMFkZqc9Jjo6WkVFRSoqKuqCEQHo7vwqiBYtWiRJGj9+vE/7smXLdNNNN0n66l2Z8PBw5ebmyuPxKCcnRwsXLgzIYAEAAAAgkPwqiHhXBgAAAEBP0qHvIQIAAACA7oyCCAAAAIBjURABAAAAcCwKIgAAAACORUEEAAAAwLEoiAAAAAA4FgURAAAAAMeiIAIAAADgWBREAAAAAByLgggAAACAY1EQAQAAAHAsCiIAAAAAjkVBBAAAAMCxKIgAAAAAOBYFEQAAAADHoiACAAAA4FgURAAAAAAci4IIAAAAgGNREAEAAABwLAoiAF2qsLBQY8aMUe/evZWUlKRrrrlG5eXlPsccP35ceXl5SkxMVGxsrHJzc1VdXR2kEQMAgJ6MgghAl9qyZYvy8vJUWlqqV155RQ0NDZo4caKOHTvmPWbWrFnasGGD1q5dqy1btujgwYOaMmVKEEcNAAB6qohgDwCAs2zcuNHn9fLly5WUlKSysjJdeumlqq2t1dKlS7V69WpNmDBBkrRs2TINGzZMpaWlGjduXDCGDQAAeihWiAAEVW1trSQpISFBklRWVqaGhgZlZ2d7j0lPT1daWppKSkqa7cPj8aiurs5nAwAAaAsKIgBB09jYqJkzZ+riiy/WiBEjJElVVVWKiopSfHy8z7HJycmqqqpqtp/CwkLFxcV5t9TU1M4eOgAA6CEoiAAETV5ennbv3q01a9Z0qJ+CggLV1tZ6t8rKygCNEAAA9HR8hghAUEyfPl0vvPCC3njjDQ0cONDbnpKSohMnTqimpsZnlai6ulopKSnN9uVyueRyuTp7yAAAoAdihQhAlzIzTZ8+XevWrdNrr72mwYMH++zPyMhQZGSkiouLvW3l5eWqqKhQVlZWVw8XAAD0cKwQAehSeXl5Wr16tZ5//nn17t3b+7mguLg4xcTEKC4uTtOmTVN+fr4SEhLkdrs1Y8YMZWVl8YQ5AAAQcBREALrUokWLJEnjx4/3aV+2bJluuukmSdL8+fMVHh6u3NxceTwe5eTkaOHChV08UgAA4AQURAC6lJmd9pjo6GgVFRWpqKioC0YEAACcjM8QAQCAbuWNN97QVVddpQEDBigsLEzPPfecz34z0/3336/+/fsrJiZG2dnZ2rt3b3AGCyDk+V0QkYQAAEAwHTt2TKNGjWpxFfmRRx7R448/rieffFJbt27VmWeeqZycHB0/fryLRwqgO/C7ICIJAQCAYJo0aZIefPBBXXvttafsMzMtWLBAv/71rzV58mSNHDlSK1eu1MGDB095ExcApHZ8hmjSpEmaNGlSs/u+mYQkaeXKlUpOTtZzzz2nn/zkJx0bLQAAQCv279+vqqoqZWdne9vi4uKUmZmpkpKSZq9FPB6PPB6P93VdXV2XjBVAaAjoZ4hOl4Sa4/F4VFdX57MBAAC0R9Oj/JOTk33ak5OTvfu+qbCwUHFxcd4tNTW108cJIHQEtCAiCQEAgO6moKBAtbW13q2ysjLYQwLQhYL+lDmSEAAACJSUlBRJUnV1tU97dXW1d983uVwuud1unw2AcwS0ICIJAQCAYBo8eLBSUlJUXFzsbaurq9PWrVuVlZUVxJEBCFUBLYhIQgAAoLPV19dr586d2rlzp6SvPsO8c+dOVVRUKCwsTDNnztSDDz6o9evXa9euXbrhhhs0YMAAXXPNNUEdN4DQ5PdT5urr67Vv3z7v66YklJCQoLS0NG8SGjJkiAYPHqzZs2eThAAAQMBs375d3//+972v8/PzJUk33nijli9frnvuuUfHjh3TrbfeqpqaGl1yySXauHGjoqOjgzVkACHM74KIJAQAAIJp/PjxMrMW94eFhWnu3LmaO3duF44KQHfld0FEEgIAAADQUwT9KXMAAAAAECwURAAAAAAci4IIAAAAgGP5/RkiAADQ/Zx934sB7e+TeVcGtD8ACBZWiAAAAAA4FgURAAAAAMeiIAIAAADgWBREAAAAAByLgggAAACAY1EQAQAAAHAsCiIAAAAAjkVBBAAAAMCxKIgAAAAAOBYFEQAAAADHoiACAAAA4FgURAAAAAAcKyLYAwCAYDr7vhcD3ucn864MeJ8AAKBzsEIEAAAAwLEoiAAAAAA4FgURAAAAAMeiIAIAAADgWBREAAAAAByLgggAAACAY1EQAQAAAHAsCiIAAAAAjkVBBAAAAMCxKIgAAAAAOBYFEQAAAADHoiACAAAA4FgURAAAAAAci4IIAAAAgGNFBHsAAE7v7PteDHifn8y7MuB9AgAAdDedtkJUVFSks88+W9HR0crMzNTbb7/dWacC0AORQwB0FHkEQFt0SkH0zDPPKD8/X3PmzNGOHTs0atQo5eTk6PDhw51xOgA9DDkEQEeRRwC0VafcMvfYY4/plltu0c033yxJevLJJ/Xiiy/q6aef1n333edzrMfjkcfj8b6ura2VJNXV1bXpXI2eLwI0avl1XpxeoOdGcu78BOt32XSMmQX8/K3xJ4dIHcsj/J2GNuYncIL172V3yCNciwCn16OvRSzAPB6P9erVy9atW+fTfsMNN9jVV199yvFz5swxSWxsbCG8VVZWBjpVtMjfHGJGHmFj6w5bKOcRcggbW+hvnZlDAr5C9L//+786efKkkpOTfdqTk5P14YcfnnJ8QUGB8vPzva8bGxt15MgRJSYmKiwsrNVz1dXVKTU1VZWVlXK73YEJIIh6Ujw9KRbJufGYmY4ePaoBAwZ02dj8zSFS+/OIU+e1uyCe0OVPLN0hj3At8n96Ujw9KRbJufF0RQ4J+lPmXC6XXC6XT1t8fLxffbjd7h7xh9GkJ8XTk2KRnBlPXFxcF42m/TqaR5w4r90J8YSutsYS6nmEa5FT9aR4elIskjPj6ewcEvCHKvTt21e9evVSdXW1T3t1dbVSUlICfToAPQw5BEBHkUcA+CPgBVFUVJQyMjJUXFzsbWtsbFRxcbGysrICfToAPQw5BEBHkUcA+KNTbpnLz8/XjTfeqNGjR2vs2LFasGCBjh075n3SS6C4XC7NmTPnlGXu7qonxdOTYpGIp6uRQ9qHeEJbT4qnO8RCHmmfnhRPT4pFIp7OFGbWOc+w+9Of/qRHH31UVVVV+s53vqPHH39cmZmZnXEqAD0QOQRAR5FHALRFpxVEAAAAABDqAv4ZIgAAAADoLiiIAAAAADgWBREAAAAAx6IgAgAAAOBYQSuIDhw4oJ/+9KdKTExUTEyMzj//fG3fvt27v76+XtOnT9fAgQMVExOj8847T08++aR3/yeffKKwsLBmt7Vr17Z4XjPT/fffr/79+ysmJkbZ2dnau3dvt43npptuOuX4K664IqixSFJVVZV+9rOfKSUlRWeeeaYuvPBC/fd///dpz11UVKSzzz5b0dHRyszM1Ntvv92hWIIZz29+85tT5iY9PT0k4vnoo4907bXXql+/fnK73frRj350yhcYNqcz5qc9Tve7bct87dmzR5MnT1bfvn3ldrt1ySWXaPPmza2et7PyR7Di6Yz8Eah4duzYocsvv1zx8fFKTEzUrbfeqvr6+lbPG8rz0554gjU/bckPR44c0dSpU+V2uxUfH69p06adNp7jx48rLy9PiYmJio2NVW5ubpvyTqCcLu6nnnpK48ePl9vtVlhYmGpqak7p4+yzzz6lj3nz5rV63rbEXVFRoSuvvFJnnHGGkpKSdPfdd+vLL78MuXiOHDmiGTNmaOjQoYqJiVFaWpruuOMO1dbW+hzX3LXMmjVrQioWSRo/fvwpP3Pbbbf5HNNd5qat15H+zk2g4pGkF198UZmZmYqJiVGfPn10zTXXtHretuT09uSilk7W5Y4cOWKDBg2ym266ybZu3Woff/yxbdq0yfbt2+c95pZbbrFvfetbtnnzZtu/f78tXrzYevXqZc8//7yZmX355Zd26NAhn+2BBx6w2NhYO3r0aIvnnjdvnsXFxdlzzz1n7777rl199dU2ePBg+9e//tUt47nxxhvtiiuu8Pm5I0eOBDUWM7PLL7/cxowZY1u3brWPPvrIfvvb31p4eLjt2LGjxXOvWbPGoqKi7Omnn7b333/fbrnlFouPj7fq6upuGc+cOXNs+PDhPnPz6aeftjuWQMVTX19v55xzjl177bX23nvv2XvvvWeTJ0+2MWPG2MmTJ1s8d2fMT3ud7nfblvkaMmSI/du//Zu9++67tmfPHvvlL39pZ5xxhh06dKjF83ZG/ghmPIHOH4GK58CBA9anTx+77bbb7MMPP7S3337bLrroIsvNzW31vKE6P+2NJxjz09b8cMUVV9ioUaOstLTU/v73v9u5555r119/favnve222yw1NdWKi4tt+/btNm7cOLvooos6HE9bnW4e58+fb4WFhVZYWGiS7PPPPz+lj0GDBtncuXN9+qivr2/1vKeL+8svv7QRI0ZYdna2vfPOO/bSSy9Z3759raCgIOTi2bVrl02ZMsXWr19v+/bts+LiYhsyZMgpf8uSbNmyZT79tvb/YbDm5nvf+57dcsstPj9TW1vr3d+d5qat15H+zk2g4vnrX/9qffr0sUWLFll5ebm9//779swzz7R63rbk9PbkouYEpSC699577ZJLLmn1mOHDh9vcuXN92i688EL7j//4jxZ/5jvf+Y79/Oc/b3F/Y2OjpaSk2KOPPuptq6mpMZfLZX/5y1/aOPpTBSses6/+wZw8eXKbx3o6gYrlzDPPtJUrV/ock5CQYEuWLGmx37Fjx1peXp739cmTJ23AgAFWWFjoTwg+ghnPnDlzbNSoUf4PuhWBiGfTpk0WHh7uk/RramosLCzMXnnllRb77Yz5aa/T/W5PN1+ffvqpSbI33njDu7+urs4ktfg76Kz8YRaceMwCnz+adDSexYsXW1JSks8F+HvvvWeSbO/evc32Gcrz0554zIIzP23JDx988IFJsm3btnmPefnlly0sLMwOHDjQbL81NTUWGRlpa9eu9bb9z//8j0mykpKSAER1em3NyZs3b271InX+/PltPmdb4n7ppZcsPDzcqqqqvMcsWrTI3G63eTyekIqnOc8++6xFRUVZQ0ODt02SrVu3rs19BCuW733ve3bnnXe2uL+7z01z15H+zo1Zx+NpaGiws846y/7zP/+zzedsS05vTy5qSVBumVu/fr1Gjx6t6667TklJSbrgggu0ZMkSn2MuuugirV+/XgcOHJCZafPmzdqzZ48mTpzYbJ9lZWXauXOnpk2b1uJ59+/fr6qqKmVnZ3vb4uLilJmZqZKSkm4XT5PXX39dSUlJGjp0qG6//XZ99tlnQY/loosu0jPPPKMjR46osbFRa9as0fHjxzV+/Phmz3vixAmVlZX5zE14eLiys7NDYm78jafJ3r17NWDAAJ1zzjmaOnWqKioq2h1LoOLxeDwKCwvz+Wbo6OhohYeH680332z2vJ01Px3R2u/2dPOVmJiooUOHauXKlTp27Ji+/PJLLV68WElJScrIyGj2fJ2VP4IVT5NA5o9AxePxeBQVFaXw8P/7JyomJkaSWvwbDeX5aU88Tbp6ftqSH0pKShQfH6/Ro0d7j8nOzlZ4eLi2bt3a7PnKysrU0NDgMz/p6elKS0vr0hwSiJw8b948JSYm6oILLtCjjz7a6u1TbYm7pKRE559/vpKTk73H5OTkqK6uTu+//35IxdOc2tpaud1uRURE+LTn5eWpb9++Gjt2rJ5++mnZab72MlixrFq1Sn379tWIESNUUFCgL774wruvO89Na9eR/s6N1LF4duzYoQMHDig8PFwXXHCB+vfvr0mTJmn37t0t/kxbcnp7clGL/CqfAsTlcpnL5bKCggLbsWOHLV682KKjo2358uXeY44fP2433HCDSbKIiAiLioqyFStWtNjn7bffbsOGDWv1vP/4xz9Mkh08eNCn/brrrrMf/ehH3S4eM7O//OUv9vzzz9t7771n69ats2HDhtmYMWPsyy+/DGosn3/+uU2cONF7jNvttk2bNrV43gMHDpgke+utt3za7777bhs7dmy7YglmPGZfvbP07LPP2rvvvmsbN260rKwsS0tLs7q6uqDGc/jwYXO73XbnnXfasWPHrL6+3qZPn26S7NZbb232vJ01P+11ut9tW+arsrLSMjIyLCwszHr16mX9+/dv9RbIzsofwYrHLPD5I1Dx7N692yIiIuyRRx4xj8djR44csdzcXJNkDz/8cLPnDOX5aU88ZsGZn7bkh4ceesi+/e1vn9Jvv379bOHChc2ec9WqVRYVFXVK+5gxY+yee+7pUDxt1dac3Nq79n/4wx9s8+bN9u6779qiRYssPj7eZs2a1eI52xL3LbfcYhMnTvTZf+zYMZNkL730UkjF802ffvqppaWl2b//+7/7tM+dO9fefPNN27Fjh82bN89cLpf98Y9/DLlYFi9ebBs3brT33nvP/vznP9tZZ51l1157rXd/d56blq4j/Z2bQMTzl7/8xSRZWlqa/fWvf7Xt27fb9ddfb4mJifbZZ581e8625PT25KKWBKUgioyMtKysLJ+2GTNm2Lhx47yvH330Ufv2t79t69evt3fffdeeeOIJi42Nbfb2jy+++MLi4uLs97//favn7ax/MIMVT3M++ugjk2Svvvqq/4FY4GKZPn26jR071l599VXbuXOn/eY3v7G4uDh77733mj1vZ11wByue5nz++efmdrv9WjLurHg2bdpk55xzjvfi+ac//aldeOGFdttttzV73lAriL7pm7/b081XY2OjXX311TZp0iR78803rayszG6//XY766yzTskPTTrzgjsY8TSno/kjUPGYfXUhmZycbL169bKoqCi76667LDk52ebNm9fsOUJ5ftoTT3O6an5Olx+6a0H0TS3l5NYuUr9p6dKlFhERYcePH292f2cWRN/UFfF8XW1trY0dO9auuOIKO3HiRKvHzp492wYOHHjaPpt0dSxNiouLTZL3c7nddW78uY70d27M/I9n1apVJskWL17sbTt+/Lj17dvXnnzyyWbP4YiCKC0tzaZNm+bTtnDhQhswYICZfTWRkZGR9sILL/gcM23aNMvJyTmlv5UrV1pkZKQdPny41fM2/WPyzjvv+LRfeumldscdd7Qjkq8EK56WtPYHdjqBiGXfvn0myXbv3u1zzGWXXWa/+MUvmj2vx+OxXr16nXJf6w033GBXX311u2IJZjwtGT16tN13333+huEV6L+1Tz/91Ju4kpOT7ZFHHmn2vJ01P4HU9Ltty3y9+uqrp3xOwszs3HPPbfEzUZ2VP1rS2fG0pCP5ozX+xPN1VVVVdvToUauvr7fw8HB79tlnm+0/lOfn69oaT0s6e36+rqX8sHTpUouPj/c5tqGhwXr16mV/+9vfmu2/6ULzmxdKaWlp9thjjwUmiHZoLm5/LlJ3795tkuzDDz9sdn9b4p49e/Ypn8/4+OOPTdJpV3m/qbPjaVJXV2dZWVl22WWXtemhJS+88IJJ8qs46apYvq6+vt4k2caNG82se86NmX/Xke2ZGzP/4nnttddMkv3973/3aR87duwpq4tN2pLT25OLWhKUzxBdfPHFKi8v92nbs2ePBg0aJElqaGhQQ0ODz/3WktSrVy81Njae0t/SpUt19dVXq1+/fq2ed/DgwUpJSVFxcbG3ra6uTlu3blVWVlZ7wwlaPM355z//qc8++0z9+/f3+2elwMTSdP9tW+OVpKioKGVkZPjMTWNjo4qLi4M+N+2Jpzn19fX66KOP2j03UuD/1vr27av4+Hi99tprOnz4sK6++upmz9tZ8xMoX//dtmW+WjomPDy8xTntrPzRnK6IpzkdzR8t8Teer0tOTlZsbKyeeeYZRUdH6/LLL2/2HKE8P1/X1nia0xXz83Ut5YesrCzV1NSorKzMe+xrr72mxsZGZWZmNnuOjIwMRUZG+sxPeXm5KioqgpZDApGTd+7cqfDwcCUlJTW7vy1xZ2VladeuXTp8+LD3mFdeeUVut1vnnXdem8fSFfFIX/1/NXHiREVFRWn9+vWKjo5uU799+vTx+Wxaa7oqluZ+RpL3vN1tbpr4cx3p79xI/seTkZEhl8vlc/3S0NCgTz75xHv98k1tyentyUUt8qt8CpC3337bIiIi7KGHHrK9e/faqlWr7IwzzrA///nP3mO+973v2fDhw23z5s328ccf27Jlyyw6OvqUJbC9e/daWFiYvfzyy82ea+jQoT5V4rx58yw+Pt57X/bkyZM7/FjWYMVz9OhRu+uuu6ykpMT2799vr776ql144YU2ZMgQvyv9QMZy4sQJO/fcc+273/2ubd261fbt22e///3vLSwszF588UVvPxMmTLAnnnjC+3rNmjXmcrls+fLl9sEHH9itt95q8fHxPk936U7x/OpXv7LXX3/d9u/fb//4xz8sOzvb+vbt2+6Vv0DFY2b29NNPW0lJie3bt8/+67/+yxISEiw/P9/nXF0xP+3V2u+2LfP16aefWmJiok2ZMsV27txp5eXldtddd1lkZKTt3LnTe56uyB/Biqcz8keg4jEze+KJJ6ysrMzKy8vtT3/6k8XExJxyn3t3mZ/2xBOs+TFrW3644oor7IILLrCtW7fam2++aUOGDPF51O0///lPGzp0qG3dutXbdtttt1laWpq99tprtn37dsvKyjrlFuDOdLq4Dx06ZO+8844tWbLE+9TGd955x/sZh7feesvmz59vO3futI8++sj+/Oc/W79+/eyGG27oUNxNj3aeOHGi7dy50zZu3Gj9+vU77aOdgxFPbW2tZWZm2vnnn2/79u3zeQxz02fb1q9fb0uWLLFdu3bZ3r17beHChXbGGWfY/fffH1Kx7Nu3z+bOnWvbt2+3/fv32/PPP2/nnHOOXXrppd1ybpq0dh3ZnrkJRDxmZnfeeaedddZZtmnTJvvwww9t2rRplpSU5PNVAu3J6afLRW0VlILIzGzDhg02YsQIc7lclp6ebk899ZTP/kOHDtlNN91kAwYMsOjoaBs6dKj94Q9/sMbGRp/jCgoKLDU1tcXvT9H/f956k8bGRps9e7YlJyeby+Wyyy67zMrLy7tlPF988YVNnDjR+vXrZ5GRkTZo0CC75ZZbOnyBGohY9uzZY1OmTLGkpCQ744wzbOTIkac8lnbQoEE2Z84cn7YnnnjC0tLSLCoqysaOHWulpaUdiiWY8fz4xz+2/v37W1RUlJ111ln24x//2Of7goIZz7333mvJyckWGRlpQ4YMafZvsavmpz1O97tty3xt27bNJk6caAkJCda7d28bN27cKfeEd1X+CEY8nZU/AhXPz372M0tISLCoqKhm938zHrPQnh9/4wnm/LQlP3z22Wd2/fXXW2xsrLndbrv55pt9vutk//79Jsk2b97sbfvXv/5lv/zlL61Pnz52xhln2LXXXtvq92QF2uninjNnjkk6ZWuak7KyMsvMzLS4uDiLjo62YcOG2cMPP+xToLY37k8++cQmTZpkMTEx1rdvX/vVr37l8xjrUImn6Zao5rb9+/eb2VePPf7Od75jsbGxduaZZ9qoUaPsySefbPV77oIRS0VFhV166aWWkJBgLpfLzj33XLv77rtPufW4u8xNk9auI9szN4GIx+yrN5d/9atfWVJSkvXu3duys7NPudW4PTn9dLmorcL+/wAAAAAAwHGC8hkiAAAAAAgFFEQAAAAAHIuCCAAAAIBjURABAAAAcCwKIgAAAACORUEEAAAAwLEoiAAAAAA4FgURAAAAAMeiIAIAAADgWBREAAAAAByLgggAAACAY/0/oK7vWct3YhAAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1000x300 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(10, 3))\n",
    "ax[0].hist(plot_results())\n",
    "ax[1].hist(plot_results(exp_var=0.95))\n",
    "ax[2].hist(plot_results(exp_var=0.9))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601559f6-0a40-40bb-830e-a836a35c5a5b",
   "metadata": {},
   "source": [
    "The above results indicate the rank is very tightly distributed based on the explained variance. The reader is encouraged to play with the explained_variance and support the claim themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41538748-3d56-4a69-a5f6-aa4f2e98f146",
   "metadata": {},
   "source": [
    "## Analysis of RoBERTa pre-trained Weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "373c7b6d-5ccd-447e-b545-7349e19e7e90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Now let's look at how tightly the weights are distributed in RoBERTa \n",
    "from transformers import AutoModel, AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a18bc9a7-0c31-41be-970e-e37992303f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract RoBERTa Configuration\n",
    "roberta_base_config = AutoConfig.from_pretrained('roberta-base')\n",
    "roberta_large_config = AutoConfig.from_pretrained('roberta-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acf40d4d-e557-4b8c-a6ec-fe8725910263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract RoBERTa Model\n",
    "roberta_base_model = AutoModel.from_config(roberta_base_config)\n",
    "roberta_large_model = AutoModel.from_config(roberta_large_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9dd7956d-48ca-4f5f-96e1-72ca43118c2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def print_principle_direction(model, exp_var):\n",
    "    for key, param in model.named_parameters():\n",
    "        with torch.no_grad():\n",
    "            key = \".\".join(key.split(\".\")[3:])\n",
    "            p_size = min(param.size())\n",
    "            if len(param.size()) < 2:\n",
    "                continue\n",
    "            print(key, get_principle_direction(param, exp_var), p_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6379350c-f84f-41e2-bd2e-da56058431bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Roberta Base Principle Directions \n",
      "\n",
      " 759 768\n",
      " 492 514\n",
      " 0 1\n",
      "attention.self.query.weight 687 768\n",
      "attention.self.key.weight 687 768\n",
      "attention.self.value.weight 687 768\n",
      "attention.output.dense.weight 687 768\n",
      "intermediate.dense.weight 753 768\n",
      "output.dense.weight 753 768\n",
      "attention.self.query.weight 688 768\n",
      "attention.self.key.weight 687 768\n",
      "attention.self.value.weight 687 768\n",
      "attention.output.dense.weight 688 768\n",
      "intermediate.dense.weight 753 768\n",
      "output.dense.weight 753 768\n",
      "attention.self.query.weight 688 768\n",
      "attention.self.key.weight 688 768\n",
      "attention.self.value.weight 687 768\n",
      "attention.output.dense.weight 688 768\n",
      "intermediate.dense.weight 753 768\n",
      "output.dense.weight 753 768\n",
      "attention.self.query.weight 688 768\n",
      "attention.self.key.weight 688 768\n",
      "attention.self.value.weight 687 768\n",
      "attention.output.dense.weight 687 768\n",
      "intermediate.dense.weight 753 768\n",
      "output.dense.weight 753 768\n",
      "attention.self.query.weight 687 768\n",
      "attention.self.key.weight 688 768\n",
      "attention.self.value.weight 687 768\n",
      "attention.output.dense.weight 687 768\n",
      "intermediate.dense.weight 753 768\n",
      "output.dense.weight 753 768\n",
      "attention.self.query.weight 687 768\n",
      "attention.self.key.weight 687 768\n",
      "attention.self.value.weight 687 768\n",
      "attention.output.dense.weight 688 768\n",
      "intermediate.dense.weight 753 768\n",
      "output.dense.weight 753 768\n",
      "attention.self.query.weight 688 768\n",
      "attention.self.key.weight 687 768\n",
      "attention.self.value.weight 688 768\n",
      "attention.output.dense.weight 688 768\n",
      "intermediate.dense.weight 753 768\n",
      "output.dense.weight 753 768\n",
      "attention.self.query.weight 687 768\n",
      "attention.self.key.weight 687 768\n",
      "attention.self.value.weight 688 768\n",
      "attention.output.dense.weight 688 768\n",
      "intermediate.dense.weight 753 768\n",
      "output.dense.weight 753 768\n",
      "attention.self.query.weight 687 768\n",
      "attention.self.key.weight 687 768\n",
      "attention.self.value.weight 688 768\n",
      "attention.output.dense.weight 687 768\n",
      "intermediate.dense.weight 753 768\n",
      "output.dense.weight 753 768\n",
      "attention.self.query.weight 687 768\n",
      "attention.self.key.weight 688 768\n",
      "attention.self.value.weight 688 768\n",
      "attention.output.dense.weight 688 768\n",
      "intermediate.dense.weight 753 768\n",
      "output.dense.weight 753 768\n",
      "attention.self.query.weight 687 768\n",
      "attention.self.key.weight 688 768\n",
      "attention.self.value.weight 688 768\n",
      "attention.output.dense.weight 687 768\n",
      "intermediate.dense.weight 753 768\n",
      "output.dense.weight 753 768\n",
      "attention.self.query.weight 688 768\n",
      "attention.self.key.weight 687 768\n",
      "attention.self.value.weight 688 768\n",
      "attention.output.dense.weight 688 768\n",
      "intermediate.dense.weight 753 768\n",
      "output.dense.weight 753 768\n",
      " 688 768\n",
      "\n",
      "# Roberta Large Principle Directions \n",
      "\n",
      " 1012 1024\n",
      " 498 514\n",
      " 0 1\n",
      "attention.self.query.weight 917 1024\n",
      "attention.self.key.weight 917 1024\n",
      "attention.self.value.weight 917 1024\n",
      "attention.output.dense.weight 917 1024\n",
      "intermediate.dense.weight 1005 1024\n",
      "output.dense.weight 1005 1024\n",
      "attention.self.query.weight 917 1024\n",
      "attention.self.key.weight 916 1024\n",
      "attention.self.value.weight 917 1024\n",
      "attention.output.dense.weight 917 1024\n",
      "intermediate.dense.weight 1005 1024\n",
      "output.dense.weight 1005 1024\n",
      "attention.self.query.weight 916 1024\n",
      "attention.self.key.weight 917 1024\n",
      "attention.self.value.weight 917 1024\n",
      "attention.output.dense.weight 916 1024\n",
      "intermediate.dense.weight 1005 1024\n",
      "output.dense.weight 1005 1024\n",
      "attention.self.query.weight 917 1024\n",
      "attention.self.key.weight 917 1024\n",
      "attention.self.value.weight 916 1024\n",
      "attention.output.dense.weight 916 1024\n",
      "intermediate.dense.weight 1005 1024\n",
      "output.dense.weight 1005 1024\n",
      "attention.self.query.weight 917 1024\n",
      "attention.self.key.weight 916 1024\n",
      "attention.self.value.weight 917 1024\n",
      "attention.output.dense.weight 917 1024\n",
      "intermediate.dense.weight 1005 1024\n",
      "output.dense.weight 1005 1024\n",
      "attention.self.query.weight 917 1024\n",
      "attention.self.key.weight 917 1024\n",
      "attention.self.value.weight 917 1024\n",
      "attention.output.dense.weight 917 1024\n",
      "intermediate.dense.weight 1005 1024\n",
      "output.dense.weight 1005 1024\n",
      "attention.self.query.weight 917 1024\n",
      "attention.self.key.weight 917 1024\n",
      "attention.self.value.weight 918 1024\n",
      "attention.output.dense.weight 916 1024\n",
      "intermediate.dense.weight 1005 1024\n",
      "output.dense.weight 1005 1024\n",
      "attention.self.query.weight 917 1024\n",
      "attention.self.key.weight 917 1024\n",
      "attention.self.value.weight 917 1024\n",
      "attention.output.dense.weight 917 1024\n",
      "intermediate.dense.weight 1005 1024\n",
      "output.dense.weight 1004 1024\n",
      "attention.self.query.weight 917 1024\n",
      "attention.self.key.weight 917 1024\n",
      "attention.self.value.weight 917 1024\n",
      "attention.output.dense.weight 917 1024\n",
      "intermediate.dense.weight 1005 1024\n",
      "output.dense.weight 1005 1024\n",
      "attention.self.query.weight 916 1024\n",
      "attention.self.key.weight 917 1024\n",
      "attention.self.value.weight 917 1024\n",
      "attention.output.dense.weight 917 1024\n",
      "intermediate.dense.weight 1005 1024\n",
      "output.dense.weight 1005 1024\n",
      "attention.self.query.weight 916 1024\n",
      "attention.self.key.weight 917 1024\n",
      "attention.self.value.weight 917 1024\n",
      "attention.output.dense.weight 916 1024\n",
      "intermediate.dense.weight 1005 1024\n",
      "output.dense.weight 1005 1024\n",
      "attention.self.query.weight 917 1024\n",
      "attention.self.key.weight 917 1024\n",
      "attention.self.value.weight 917 1024\n",
      "attention.output.dense.weight 916 1024\n",
      "intermediate.dense.weight 1005 1024\n",
      "output.dense.weight 1005 1024\n",
      "attention.self.query.weight 917 1024\n",
      "attention.self.key.weight 917 1024\n",
      "attention.self.value.weight 917 1024\n",
      "attention.output.dense.weight 917 1024\n",
      "intermediate.dense.weight 1005 1024\n",
      "output.dense.weight 1005 1024\n",
      "attention.self.query.weight 917 1024\n",
      "attention.self.key.weight 916 1024\n",
      "attention.self.value.weight 917 1024\n",
      "attention.output.dense.weight 917 1024\n",
      "intermediate.dense.weight 1005 1024\n",
      "output.dense.weight 1005 1024\n",
      "attention.self.query.weight 917 1024\n",
      "attention.self.key.weight 917 1024\n",
      "attention.self.value.weight 916 1024\n",
      "attention.output.dense.weight 916 1024\n",
      "intermediate.dense.weight 1005 1024\n",
      "output.dense.weight 1005 1024\n",
      "attention.self.query.weight 917 1024\n",
      "attention.self.key.weight 917 1024\n",
      "attention.self.value.weight 917 1024\n",
      "attention.output.dense.weight 917 1024\n",
      "intermediate.dense.weight 1005 1024\n",
      "output.dense.weight 1005 1024\n",
      "attention.self.query.weight 917 1024\n",
      "attention.self.key.weight 917 1024\n",
      "attention.self.value.weight 917 1024\n",
      "attention.output.dense.weight 917 1024\n",
      "intermediate.dense.weight 1005 1024\n",
      "output.dense.weight 1005 1024\n",
      "attention.self.query.weight 917 1024\n",
      "attention.self.key.weight 917 1024\n",
      "attention.self.value.weight 918 1024\n",
      "attention.output.dense.weight 917 1024\n",
      "intermediate.dense.weight 1005 1024\n",
      "output.dense.weight 1005 1024\n",
      "attention.self.query.weight 917 1024\n",
      "attention.self.key.weight 916 1024\n",
      "attention.self.value.weight 917 1024\n",
      "attention.output.dense.weight 917 1024\n",
      "intermediate.dense.weight 1005 1024\n",
      "output.dense.weight 1005 1024\n",
      "attention.self.query.weight 916 1024\n",
      "attention.self.key.weight 916 1024\n",
      "attention.self.value.weight 917 1024\n",
      "attention.output.dense.weight 916 1024\n",
      "intermediate.dense.weight 1005 1024\n",
      "output.dense.weight 1005 1024\n",
      "attention.self.query.weight 917 1024\n",
      "attention.self.key.weight 917 1024\n",
      "attention.self.value.weight 917 1024\n",
      "attention.output.dense.weight 917 1024\n",
      "intermediate.dense.weight 1005 1024\n",
      "output.dense.weight 1005 1024\n",
      "attention.self.query.weight 917 1024\n",
      "attention.self.key.weight 917 1024\n",
      "attention.self.value.weight 917 1024\n",
      "attention.output.dense.weight 917 1024\n",
      "intermediate.dense.weight 1005 1024\n",
      "output.dense.weight 1005 1024\n",
      "attention.self.query.weight 917 1024\n",
      "attention.self.key.weight 916 1024\n",
      "attention.self.value.weight 917 1024\n",
      "attention.output.dense.weight 918 1024\n",
      "intermediate.dense.weight 1005 1024\n",
      "output.dense.weight 1005 1024\n",
      "attention.self.query.weight 917 1024\n",
      "attention.self.key.weight 917 1024\n",
      "attention.self.value.weight 916 1024\n",
      "attention.output.dense.weight 917 1024\n",
      "intermediate.dense.weight 1005 1024\n",
      "output.dense.weight 1005 1024\n",
      " 917 1024\n"
     ]
    }
   ],
   "source": [
    "explained_variance = 0.99\n",
    "print(\"# Roberta Base Principle Directions \\n\")\n",
    "print_principle_direction(roberta_base_model, explained_variance)\n",
    "\n",
    "print(\"\\n# Roberta Large Principle Directions \\n\")\n",
    "print_principle_direction(roberta_large_model, explained_variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a39e3e-2c84-458e-a47f-5e5a86443278",
   "metadata": {},
   "source": [
    "On the surface, the above results suggest you need most of the directions to account for the explained variance in the parameters.\n",
    "As such, we finetune the models for SQuAD and review the difference in the weights of the base and the tuned model. If the difference has \n",
    "fewer principle directions with high explanation, it would mean it is possible the task-based fine-tuning can be done in a smaller subspace. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a1ff52-126c-4665-92fd-c3e5d8c83fb2",
   "metadata": {},
   "source": [
    "## Rank Variation During Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d59c3413-b156-4dfd-a1f5-1df488ff818b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Dataset\n",
    "import hydra\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from train.train import train_epoch\n",
    "from utils.metrics import AverageMeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "719ff899-edf9-4f7e-b665-ea440ef6bc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Training Configuration\n",
    "with hydra.initialize(version_base=None, config_path=\"../config\"):\n",
    "    cfg = hydra.compose(config_name=\"app_config\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9efb6c9-3495-4760-9c2b-094fa47b98bd",
   "metadata": {},
   "source": [
    "### Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f4b82e45-bb22-4aea-b6ac-1c2ef67ab9d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Process Data\n",
    "train_dataset = load_dataset(\"squad\", split=\"train\")\n",
    "val_dataset = load_dataset(\"squad\", split=\"validation\")\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.model.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "76dfbf26-bbad-4d73-933a-652dded44c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Max Length is 512\n",
      "Dataset Features are {'id': Value(dtype='string', id=None), 'title': Value(dtype='string', id=None), 'context': Value(dtype='string', id=None), 'question': Value(dtype='string', id=None), 'answers': Sequence(feature={'text': Value(dtype='string', id=None), 'answer_start': Value(dtype='int32', id=None)}, length=-1, id=None)}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model Max Length is {tokenizer.model_max_length}\")\n",
    "print(f\"Dataset Features are {train_dataset.features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "19b45799-1fa8-4ed4-a7c2-b3004997e9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 143 training instances with length longer than 512\n"
     ]
    }
   ],
   "source": [
    "cut_off_length = 512\n",
    "long_dataset = train_dataset.filter(lambda x: len(tokenizer(x['question'], x['context']).input_ids) > cut_off_length)\n",
    "print(f\"There are {long_dataset.num_rows} training instances with length longer than {cut_off_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932bb38a-5b28-4dbd-9faa-0aa9571a02e7",
   "metadata": {},
   "source": [
    "Since there are only 143 instances with the length longer than 512. I will remove those instances for the purpose of simplicity. These changes are unlikely to affect the outcome. The learned concept will not account for\n",
    "the cases in which the context may not contain the answer. Also, if there is a regularity in the placement of the answer in the question, the model may over fit to it. But given large number of samples, I assume it is not\n",
    "the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "119e8460-86c9-40d3-8fdb-2ac52d692766",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.filter(lambda x: len(tokenizer(x['question'], x['context']).input_ids) <= cut_off_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5dbd6078-c403-46ae-9aee-abd56889548b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188\n",
      "['a copper statue of Christ']\n",
      "a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n"
     ]
    }
   ],
   "source": [
    "## Let's See how the answer looks like\n",
    "idx = 1\n",
    "instance = train_dataset[idx]\n",
    "answer = instance['answers']\n",
    "print(answer['answer_start'][0])\n",
    "print(answer['text'])\n",
    "print(instance['context'][answer['answer_start'][0]:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ae666051-6c8c-4e42-995e-adc84cef27fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer is not character based\n"
     ]
    }
   ],
   "source": [
    "# The above indicates the answer_start is the beginning character. Let's see if this is consistent with the rest of the results\n",
    "char_tokenizer = True\n",
    "for idx, item in enumerate(train_dataset):\n",
    "    start = item['answers']['answer_start'][0]\n",
    "    end = start + len(item['answers']['text'][0])\n",
    "    extracted_answer = item['context'][start:end]\n",
    "    answer = item['answers']['text'][0]\n",
    "    if extracted_answer != answer:\n",
    "        print(idx, extracted_answer, answer)\n",
    "    if char_tokenizer:\n",
    "        tokenized_answer = tokenizer(answer).input_ids\n",
    "        char_tokenizer = len(tokenized_answer) - 2 == len(answer)\n",
    "\n",
    "if not char_tokenizer:\n",
    "    print(\"Tokenizer is not character based\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c37f437-6dbf-4a61-a180-42285188b5c2",
   "metadata": {},
   "source": [
    "The above result confirms that the answers are made available in terms of character offset instead of tokenizer offset. This is likely the case to maintain generality. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60576cad-1774-4952-b66a-4f1c6bbd4a28",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2e2ce7-6346-4edd-810b-ca4b3c81bdf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a7bcbd0-1ab7-48b6-b890-79dddf7abbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Training Artifacts\n",
    "training_cfg = hydra.utils.instantiate(cfg.model.model.train)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=training_cfg.batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=training_cfg.batch_size)\n",
    "\n",
    "model = roberta_base_model\n",
    "optimizer = AdamW(params=model.parameters(), lr=training_cfg.lr, weight_decay=training_cfg.weight_decay)\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "loss_meter = AverageMeter()\n",
    "acc_meter = AverageMeter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0e36ab-0cef-48c2-9871-fa85e359ed33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5935d06e-6829-4170-8382-57d203cd8467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [0, 3972, 2661, 222, 5, 9880, 2708, 2346, 2082, 11, 504, 4432, 11, 226, 2126, 10067, 1470, 116, 2, 2, 37848, 37471, 28108, 6, 5, 334, 34, 10, 4019, 2048, 4, 497, 1517, 5, 4326, 6919, 18, 1637, 31346, 16, 10, 9030, 9577, 9, 5, 9880, 2708, 4, 29261, 11, 760, 9, 5, 4326, 6919, 8, 2114, 24, 6, 16, 10, 7621, 9577, 9, 4845, 19, 3701, 62, 33161, 19, 5, 7875, 22, 39043, 1459, 1614, 1464, 13292, 4977, 845, 4130, 7, 5, 4326, 6919, 16, 5, 26429, 2426, 9, 5, 25095, 6924, 4, 29261, 639, 5, 32394, 2426, 16, 5, 7461, 26187, 6, 10, 19035, 317, 9, 9621, 8, 12456, 4, 85, 16, 10, 24633, 9, 5, 11491, 26187, 23, 226, 2126, 10067, 6, 1470, 147, 5, 9880, 2708, 2851, 13735, 352, 1382, 7, 6130, 6552, 625, 3398, 208, 22895, 853, 1827, 11, 504, 4432, 4, 497, 5, 253, 9, 5, 1049, 1305, 36, 463, 11, 10, 2228, 516, 14, 15230, 149, 155, 19638, 8, 5, 2610, 25336, 238, 16, 10, 2007, 6, 2297, 7326, 9577, 9, 2708, 4, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "for item in train_dataset:\n",
    "    x = f\"[CLS]{item['question']}[SEP]{item['context']}\"\n",
    "    tokenized_x = tokenizer(item['question'], item['context'])\n",
    "    print(tokenized_x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e781f4c5-9bf2-4b92-a0be-184d54fc97aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['id', 'title', 'context', 'question', 'answers'])\n",
      "{'input_ids': [[0, 1121, 99, 97, 1907, 9, 1304, 16, 44018, 67, 5850, 116, 2, 2, 133, 3710, 717, 36, 25674, 1785, 9, 16021, 3061, 43, 9, 666, 6, 552, 19, 484, 97, 194, 1265, 6904, 6, 34, 156, 44018, 41, 3626, 1973, 7, 5, 194, 18, 308, 781, 2777, 25, 10, 200, 50, 371, 2777, 2031, 11, 5, 1304, 24, 37138, 4, 96, 215, 1304, 6, 2239, 44018, 16, 41, 1973, 13, 11165, 195, 7, 290, 36, 21527, 293, 468, 7, 35959, 322, 152, 16, 1528, 9, 144, 1304, 13778, 19, 5, 8242, 3388, 792, 6, 941, 11, 167, 982, 147, 5, 781, 2777, 16, 19840, 4, 44018, 16, 67, 5850, 11, 2065, 821, 710, 1350, 38215, 1328, 666, 4, 2], [0, 1779, 21, 5, 13097, 3853, 7774, 8759, 1419, 116, 2, 2, 133, 5767, 338, 859, 8759, 11, 902, 8, 5, 13097, 3853, 7774, 8759, 11, 587, 3010, 373, 13, 10, 35898, 9, 30519, 6, 5, 741, 4061, 2577, 9, 24064, 4794, 5464, 8, 49, 7465, 19, 274, 27920, 6, 5, 22211, 9, 559, 9545, 6, 7465, 9, 24064, 4794, 88, 168, 6, 41, 23306, 13, 5, 121, 597, 10644, 6, 63, 4972, 25, 10, 559, 537, 6, 8, 5, 7465, 9, 63, 5464, 88, 5, 632, 3835, 4, 3646, 1134, 1143, 7, 1032, 53, 97, 1134, 1419, 15, 7, 5, 1288, 6, 50, 1122, 5983, 19, 5, 168, 36, 242, 4, 571, 4, 121, 5499, 15, 379, 719, 2266, 322, 20, 129, 538, 333, 45, 7, 1203, 41, 1288, 23, 5, 86, 21, 5, 11565, 12887, 6, 61, 1143, 63, 1713, 8, 1419, 10, 1987, 1288, 19, 5, 168, 15, 564, 830, 1125, 4, 2], [0, 2264, 14320, 222, 18125, 139, 12506, 25, 233, 9, 39, 92, 30886, 116, 2, 2, 1121, 18069, 41, 1288, 19, 5, 11484, 6, 10678, 3215, 11, 233, 30, 5, 744, 11, 7571, 9, 1475, 12, 27217, 661, 9599, 32841, 9, 525, 1073, 16552, 726, 15093, 6125, 10577, 179, 1043, 8, 10701, 11, 5, 2352, 18, 1548, 7, 18907, 35754, 34864, 11, 5, 4665, 11484, 1080, 6, 10170, 196, 92, 3519, 7, 5, 43197, 7733, 4019, 2197, 6, 1605, 7, 740, 41072, 2072, 8, 490, 38725, 5119, 4, 20, 1288, 67, 12428, 5734, 6, 61, 56, 9464, 5, 14221, 9, 92, 19929, 11, 37869, 187, 21123, 4, 18125, 139, 18, 92, 30886, 1145, 1756, 31, 2065, 41633, 29681, 11, 6556, 3475, 30, 8445, 2258, 463, 271, 17816, 19277, 4807, 4, 96, 5, 276, 76, 18125, 139, 2998, 14, 42695, 531, 12113, 17030, 5966, 37869, 18, 768, 30, 5, 1370, 9, 49, 7576, 36, 757, 26308, 154, 41, 34465, 9, 37632, 661, 43446, 8, 709, 9, 6176, 44532, 322, 20, 331, 2010, 4237, 36, 13083, 3813, 43, 794, 63, 476, 20819, 124, 8, 63, 813, 2906, 7, 23221, 4, 2], [0, 1779, 222, 234, 16151, 1597, 116, 2, 2, 1620, 5, 3564, 1367, 15, 971, 772, 6200, 6, 722, 71, 11363, 23817, 5, 94, 4681, 884, 7, 989, 6, 234, 16151, 2152, 10, 1144, 908, 4, 91, 21, 1320, 8139, 7, 39, 790, 6, 147, 39, 13018, 21131, 7, 123, 4, 234, 16151, 962, 484, 722, 423, 6, 198, 231, 35, 612, 181, 4, 119, 4, 91, 967, 337, 6, 10738, 415, 6, 8, 234, 16151, 18, 1141, 11318, 493, 58, 23, 39, 744, 5134, 4, 767, 7, 39, 3299, 6, 1076, 12, 104, 9665, 16128, 17310, 6, 234, 16151, 18, 533, 1303, 9, 744, 21, 39929, 4544, 42138, 6, 15747, 636, 3876, 31694, 6, 8, 12385, 31, 251, 12, 8190, 7704, 4, 234, 16151, 21, 10, 2016, 40345, 19, 10, 284, 750, 9, 1144, 2199, 578, 7109, 9, 39, 5396, 962, 11, 49, 856, 9417, 918, 31, 5, 276, 1881, 4, 20, 194, 9, 234, 16151, 18, 474, 21, 45, 684, 7, 5, 285, 2052, 7, 39, 744, 4, 91, 56, 1433, 2152, 1144, 1912, 11, 18069, 8, 772, 15077, 4, 2], [0, 2264, 34555, 222, 5, 39371, 836, 7, 5, 580, 35224, 29, 116, 2, 2, 34623, 5, 2958, 3220, 4516, 6, 10, 16010, 9, 35224, 29, 2035, 11, 10, 4745, 9, 1505, 1005, 4, 767, 7, 7875, 51, 58, 669, 30, 10, 6132, 1440, 4236, 14285, 7529, 6, 31, 2661, 5, 2136, 22, 347, 40516, 113, 38618, 4, 20, 5127, 3220, 1146, 5, 194, 9, 2860, 4266, 21866, 6, 1060, 78, 23586, 36, 500, 1988, 13714, 1469, 9, 4266, 21866, 43, 4036, 45101, 23586, 988, 6395, 7, 2142, 39371, 11, 41, 2120, 7, 1888, 5, 2712, 9, 953, 17932, 493, 15, 3458, 8, 559, 301, 11, 39, 247, 4, 1216, 39371, 6, 37538, 8, 16410, 6125, 6, 1147, 7, 10304, 5, 9096, 29, 31, 2065, 35224, 636, 43571, 1809, 7, 22085, 8, 2885, 10, 2352, 467, 4, 252, 67, 1146, 5, 4573, 1073, 20726, 636, 34555, 7, 5, 580, 35224, 29, 6, 1060, 2777, 21, 1433, 10963, 40251, 4, 152, 2777, 6, 423, 684, 25, 45914, 12, 347, 40516, 6, 21, 1786, 7, 2559, 31, 63, 2598, 580, 35224, 636, 22451, 16283, 45914, 12, 26724, 1417, 677, 6, 45914, 12, 32400, 1173, 8, 45914, 12, 104, 368, 41917, 4, 3687, 97, 1575, 6, 45914, 12, 347, 40516, 21, 4760, 30, 63, 14887, 40557, 337, 304, 9, 5, 12559, 23021, 271, 856, 4063, 3693, 46098, 927, 48229, 35423, 2469, 43521, 8, 4292, 3992, 15, 5, 78, 37201, 868, 4, 2], [0, 41107, 9541, 571, 4715, 687, 6, 61, 97, 24684, 16, 11047, 11, 2297, 9453, 116, 2, 2, 1121, 5, 10130, 11492, 8, 2367, 32246, 3060, 571, 4715, 687, 21, 29341, 9541, 571, 4715, 687, 4, 1596, 24962, 32, 10266, 576, 13, 42, 39752, 4, 509, 22761, 41235, 10, 3950, 364, 48499, 3059, 19, 5, 2136, 26354, 2977, 6697, 119, 41000, 113, 11, 5862, 43, 528, 7, 9541, 24175, 18, 10079, 6, 22, 119, 41000, 34133, 3341, 39706, 4, 28013, 6, 103, 5848, 14, 3060, 571, 4715, 687, 21, 15992, 7, 9541, 571, 4715, 687, 30, 33460, 19, 5, 5862, 26354, 2538, 6697, 605, 463, 8070, 528, 7, 5, 13867, 50, 43059, 4361, 9702, 7, 9541, 24175, 11, 5, 2367, 32246, 36, 9226, 8257, 16, 303, 11, 129, 10, 6095, 9, 42486, 6, 959, 6, 8, 21, 1153, 45, 5859, 322, 96, 11704, 1304, 36, 28481, 154, 5, 1515, 1524, 238, 5, 10870, 21, 7, 5667, 5895, 2072, 5862, 2523, 30, 6614, 49, 5862, 41677, 6, 12113, 9541, 24175, 4, 96, 5, 753, 212, 3220, 6, 103, 1859, 12, 23830, 4187, 1952, 11, 5, 315, 532, 2528, 29685, 7, 3060, 24175, 6, 25, 24, 16, 2789, 7, 39, 1461, 766, 6, 8, 16, 67, 5, 2065, 1859, 24684, 31274, 438, 12257, 956, 742, 13021, 9453, 8621, 258, 6, 600, 5, 9238, 4704, 7, 2496, 10827, 3060, 571, 4715, 687, 7, 1877, 9655, 19, 5, 290, 212, 12, 11046, 25187, 119, 9063, 9541, 571, 4715, 687, 1127, 139, 14171, 29753, 687, 4, 993, 618, 12, 500, 4242, 44096, 6737, 6640, 7, 3327, 5, 18769, 1069, 15259, 22, 133, 20617, 9, 21889, 4324, 49070, 438, 12257, 956, 742, 2], [0, 2264, 5259, 4790, 5, 20268, 26411, 2534, 116, 2, 2, 36926, 4458, 5764, 1146, 5, 343, 8, 63, 24696, 124, 7, 10, 7867, 35762, 8, 1315, 7671, 92, 1196, 4, 287, 5, 343, 18, 13715, 2782, 6, 1196, 1006, 7, 7057, 50, 1045, 435, 3353, 4, 96, 43172, 6, 5, 20268, 26411, 2534, 21, 2885, 30, 5, 470, 7750, 1766, 1544, 25, 5, 78, 481, 5929, 334, 13, 12118, 18, 1704, 470, 1956, 4, 1292, 2897, 255, 4, 13226, 20899, 39, 323, 7, 5, 10012, 9, 5, 315, 532, 4152, 88, 5, 9028, 10005, 3536, 6, 41, 5984, 2122, 13, 320, 3878, 8, 2786, 314, 21297, 196, 50, 15357, 26261, 30, 5, 997, 4, 9028, 10005, 3536, 423, 1770, 19, 272, 5247, 835, 8, 16, 122, 10, 2737, 12, 5234, 5489, 5257, 334, 6, 9028, 12, 534, 5247, 835, 4, 2], [0, 12375, 875, 128, 133, 10927, 282, 12040, 9, 2344, 571, 6782, 17457, 1742, 35661, 2, 2, 4993, 8796, 9293, 15, 19325, 8251, 509, 6, 150, 24, 16, 11, 63, 8787, 6, 3038, 12, 18888, 618, 12, 22862, 6, 37, 40, 822, 39, 251, 12, 23648, 14082, 9, 871, 229, 2399, 6403, 18, 14954, 20, 10927, 282, 12040, 9, 2344, 571, 6782, 17457, 1742, 4, 20, 1040, 3905, 5, 1528, 527, 9, 10, 664, 4586, 2143, 11, 504, 4432, 2627, 54, 21, 16340, 42836, 30, 10, 284, 20667, 8, 172, 15762, 31, 39, 284, 30, 5, 11846, 337, 532, 6, 147, 37, 21, 1179, 8, 5389, 25, 10, 13174, 6, 3735, 758, 10618, 8, 1959, 10, 433, 15583, 4, 1234, 585, 11, 777, 6, 5, 1040, 34, 57, 15517, 30, 3621, 10552, 8, 5, 822, 40, 456, 999, 1190, 248, 4360, 2389, 6, 25, 8509, 221, 6125, 23698, 4, 85, 40, 28, 10571, 11, 419, 193, 13, 800, 23, 5, 253, 9, 14, 76, 6, 137, 19325, 8251, 509, 16, 2121, 8, 703, 11, 199, 4, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "for item in train_loader:\n",
    "    print(item.keys())\n",
    "    \n",
    "    # print(item['question'])\n",
    "    # print(item['context'])\n",
    "    # print(item['answers']['answer_start'])\n",
    "    tokenized_x = tokenizer(item['question'], item['context'])\n",
    "    print(tokenized_x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd23942f-6ea1-4a79-98ae-ddaa0d6a765e",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model.forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c1c168-4382-4963-bab5-e723a863a4f7",
   "metadata": {},
   "source": [
    "# Introducing LoRA Weights\n",
    "This section introduces LoRA weights in our models for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec9a0bca-9500-4e4f-9650-23bfc0a02439",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# On the surface the above results suggest you need most of the directions to account for most of the directions. \n",
    "# It will be useful to see which directions are affected and how later.\n",
    "from models.LoRA import LoRALinearLayer\n",
    "def add_linear_lora(module, rank, init_type=0):\n",
    "    for key, child in module.named_children():\n",
    "        if isinstance(child, torch.nn.Linear):\n",
    "            lora_layer =  LoRALinearLayer(child, rank=rank, init_type=init_type)\n",
    "            setattr(module, key, lora_layer)\n",
    "        else:\n",
    "            add_linear_lora(child, rank, init_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa02e6c0-2f89-40ab-b387-9c77de31b5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_linear_lora(roberta_base, rank=10, init_type=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe6c9164-4b29-4ad1-b98f-a50c4f368e7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaHubInterface(\n",
       "  (model): RobertaModel(\n",
       "    (encoder): RobertaEncoder(\n",
       "      (sentence_encoder): TransformerEncoder(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
       "        (embed_positions): LearnedPositionalEmbedding(514, 768, padding_idx=1)\n",
       "        (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (layers): ModuleList(\n",
       "          (0-11): 12 x TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): LoRALinear(in_features=768, in_features=768, rank=$10, init_type=$1)\n",
       "              (v_proj): LoRALinear(in_features=768, in_features=768, rank=$10, init_type=$1)\n",
       "              (q_proj): LoRALinear(in_features=768, in_features=768, rank=$10, init_type=$1)\n",
       "              (out_proj): LoRALinear(in_features=768, in_features=768, rank=$10, init_type=$1)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): LoRALinear(in_features=768, in_features=3072, rank=$10, init_type=$1)\n",
       "            (fc2): LoRALinear(in_features=3072, in_features=768, rank=$10, init_type=$1)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (lm_head): RobertaLMHead(\n",
       "        (dense): LoRALinear(in_features=768, in_features=768, rank=$10, init_type=$1)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (classification_heads): ModuleDict()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c66542b-d1f8-417c-bae4-172a31970431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaHubInterface(\n",
       "  (model): RobertaModel(\n",
       "    (encoder): RobertaEncoder(\n",
       "      (sentence_encoder): TransformerEncoder(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
       "        (embed_positions): LearnedPositionalEmbedding(514, 1024, padding_idx=1)\n",
       "        (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (layers): ModuleList(\n",
       "          (0-23): 24 x TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (lm_head): RobertaLMHead(\n",
       "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (classification_heads): ModuleDict()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0ff436bb-df8a-4baa-af98-0be01d85c430",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "40d88517-de50-4af3-ac05-2096882f94d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /roberta-base/resolve/main/config.json HTTP/1.1\" 200 0\n"
     ]
    }
   ],
   "source": [
    "model_config = AutoConfig.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "489ec480-9ada-4d51-8c54-a21b050de0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel.from_config(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88bb4c7-0028-4504-840b-081200450898",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
