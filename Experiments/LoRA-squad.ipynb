{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e45fd35-cdca-44b8-a00f-607078ee1a67",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Investigating Ranks\n",
    "In the first section, I am going to investigate what an appropriate rank to consider should be. It will make sense to look at the \n",
    "ranks of a trained matrix and a randomly initialized matrix. We will start by looking at a randomly initialized matrix and then\n",
    "look at a trained matrix. Specifically, I will look at the rank of a randomly initialized matrix. I will need to consider the sizes \n",
    "that are seen in RoBERTa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1397017-4851-477f-a432-2a934510e545",
   "metadata": {},
   "source": [
    "## Imports and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f2172b3-4630-4fed-8bfd-4cf8b3ea7ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Appropriate Libraries\n",
    "%load_ext autoreload\n",
    "    \n",
    "import torch\n",
    "import matplotlib.pyplot as plt \n",
    "from tqdm import tqdm \n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "# Set Logging Level\n",
    "import logging\n",
    "level = logging.DEBUG\n",
    "logging.getLogger(\"requests\").setLevel(level)\n",
    "logging.getLogger(\"urllib3\").setLevel(level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25dba68b-c775-4672-8fbf-f2bb3da6f1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_principle_direction(A, exp_var):\n",
    "    U, S, V = torch.linalg.svd(A)\n",
    "    X = (torch.cumsum(S, 0) / S.sum().item()).tolist()\n",
    "    num = bisect.bisect(X, exp_var)\n",
    "    return num"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35846b95-4743-425e-a2bd-ac1db674217a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Normally Initialized Matrix Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a4005e5-d964-4def-94f1-41d18030aeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(n=100, size=[768, 768], exp_var=0.99):\n",
    "    results = []\n",
    "    for _ in range(n):\n",
    "        A = torch.randn(size)\n",
    "        num = get_principle_direction(A, exp_var)\n",
    "        results.append(num)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86b27c30-3205-4635-b424-9e5e80078d10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([51.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 49.]),\n",
       " array([515. , 515.1, 515.2, 515.3, 515.4, 515.5, 515.6, 515.7, 515.8,\n",
       "        515.9, 516. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(10, 3))\n",
    "ax[0].hist(plot_results())\n",
    "ax[1].hist(plot_results(exp_var=0.95))\n",
    "ax[2].hist(plot_results(exp_var=0.9))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601559f6-0a40-40bb-830e-a836a35c5a5b",
   "metadata": {},
   "source": [
    "The above results indicate the rank is very tightly distributed based on the explained variance. The reader is encouraged to play with the explained_variance and support the claim themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41538748-3d56-4a69-a5f6-aa4f2e98f146",
   "metadata": {},
   "source": [
    "## Analysis of RoBERTa pre-trained Weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "373c7b6d-5ccd-447e-b545-7349e19e7e90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Now let's look at how tightly the weights are distributed in RoBERTa \n",
    "from transformers import AutoModelForQuestionAnswering, AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a18bc9a7-0c31-41be-970e-e37992303f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mathador/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Extract RoBERTa Configuration\n",
    "roberta_base_config = AutoConfig.from_pretrained('roberta-base')\n",
    "roberta_large_config = AutoConfig.from_pretrained('roberta-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acf40d4d-e557-4b8c-a6ec-fe8725910263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract RoBERTa Model\n",
    "roberta_base_model = AutoModelForQuestionAnswering.from_config(roberta_base_config)\n",
    "roberta_large_model = AutoModelForQuestionAnswering.from_config(roberta_large_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9dd7956d-48ca-4f5f-96e1-72ca43118c2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def print_principle_direction(model, exp_var):\n",
    "    for key, param in model.named_parameters():\n",
    "        with torch.no_grad():\n",
    "            key = \".\".join(key.split(\".\")[3:])\n",
    "            p_size = min(param.size())\n",
    "            if len(param.size()) < 2:\n",
    "                continue\n",
    "            print(key, get_principle_direction(param, exp_var), p_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6379350c-f84f-41e2-bd2e-da56058431bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Roberta Base Principle Directions \n",
      "\n",
      " 759 768\n",
      " 492 514\n",
      " 0 1\n",
      "attention.self.query.weight 687 768\n",
      "attention.self.key.weight 687 768\n",
      "attention.self.value.weight 687 768\n",
      "attention.output.dense.weight 687 768\n",
      "intermediate.dense.weight 753 768\n",
      "output.dense.weight 753 768\n",
      "attention.self.query.weight 688 768\n",
      "attention.self.key.weight 687 768\n",
      "attention.self.value.weight 687 768\n",
      "attention.output.dense.weight 688 768\n",
      "intermediate.dense.weight 753 768\n",
      "output.dense.weight 753 768\n",
      "attention.self.query.weight 688 768\n",
      "attention.self.key.weight 688 768\n",
      "attention.self.value.weight 687 768\n",
      "attention.output.dense.weight 688 768\n",
      "intermediate.dense.weight 753 768\n",
      "output.dense.weight 753 768\n",
      "attention.self.query.weight 688 768\n",
      "attention.self.key.weight 688 768\n",
      "attention.self.value.weight 687 768\n",
      "attention.output.dense.weight 687 768\n",
      "intermediate.dense.weight 753 768\n",
      "output.dense.weight 753 768\n",
      "attention.self.query.weight 687 768\n",
      "attention.self.key.weight 688 768\n",
      "attention.self.value.weight 687 768\n",
      "attention.output.dense.weight 687 768\n",
      "intermediate.dense.weight 753 768\n",
      "output.dense.weight 753 768\n",
      "attention.self.query.weight 687 768\n",
      "attention.self.key.weight 687 768\n",
      "attention.self.value.weight 687 768\n",
      "attention.output.dense.weight 688 768\n",
      "intermediate.dense.weight 753 768\n",
      "output.dense.weight 753 768\n",
      "attention.self.query.weight 688 768\n",
      "attention.self.key.weight 687 768\n",
      "attention.self.value.weight 688 768\n",
      "attention.output.dense.weight 688 768\n",
      "intermediate.dense.weight 753 768\n",
      "output.dense.weight 753 768\n",
      "attention.self.query.weight 687 768\n",
      "attention.self.key.weight 687 768\n",
      "attention.self.value.weight 688 768\n",
      "attention.output.dense.weight 688 768\n",
      "intermediate.dense.weight 753 768\n",
      "output.dense.weight 753 768\n",
      "attention.self.query.weight 687 768\n",
      "attention.self.key.weight 687 768\n",
      "attention.self.value.weight 688 768\n",
      "attention.output.dense.weight 687 768\n",
      "intermediate.dense.weight 753 768\n",
      "output.dense.weight 753 768\n",
      "attention.self.query.weight 687 768\n",
      "attention.self.key.weight 688 768\n",
      "attention.self.value.weight 688 768\n",
      "attention.output.dense.weight 688 768\n",
      "intermediate.dense.weight 753 768\n",
      "output.dense.weight 753 768\n",
      "attention.self.query.weight 687 768\n",
      "attention.self.key.weight 688 768\n",
      "attention.self.value.weight 688 768\n",
      "attention.output.dense.weight 687 768\n",
      "intermediate.dense.weight 753 768\n",
      "output.dense.weight 753 768\n",
      "attention.self.query.weight 688 768\n",
      "attention.self.key.weight 687 768\n",
      "attention.self.value.weight 688 768\n",
      "attention.output.dense.weight 688 768\n",
      "intermediate.dense.weight 753 768\n",
      "output.dense.weight 753 768\n",
      " 688 768\n",
      "\n",
      "# Roberta Large Principle Directions \n",
      "\n",
      " 1012 1024\n",
      " 498 514\n",
      " 0 1\n",
      "attention.self.query.weight 917 1024\n",
      "attention.self.key.weight 917 1024\n",
      "attention.self.value.weight 917 1024\n",
      "attention.output.dense.weight 917 1024\n",
      "intermediate.dense.weight 1005 1024\n",
      "output.dense.weight 1005 1024\n",
      "attention.self.query.weight 917 1024\n",
      "attention.self.key.weight 916 1024\n",
      "attention.self.value.weight 917 1024\n",
      "attention.output.dense.weight 917 1024\n",
      "intermediate.dense.weight 1005 1024\n",
      "output.dense.weight 1005 1024\n",
      "attention.self.query.weight 916 1024\n",
      "attention.self.key.weight 917 1024\n",
      "attention.self.value.weight 917 1024\n",
      "attention.output.dense.weight 916 1024\n",
      "intermediate.dense.weight 1005 1024\n",
      "output.dense.weight 1005 1024\n",
      "attention.self.query.weight 917 1024\n",
      "attention.self.key.weight 917 1024\n",
      "attention.self.value.weight 916 1024\n",
      "attention.output.dense.weight 916 1024\n",
      "intermediate.dense.weight 1005 1024\n",
      "output.dense.weight 1005 1024\n",
      "attention.self.query.weight 917 1024\n",
      "attention.self.key.weight 916 1024\n",
      "attention.self.value.weight 917 1024\n",
      "attention.output.dense.weight 917 1024\n",
      "intermediate.dense.weight 1005 1024\n",
      "output.dense.weight 1005 1024\n",
      "attention.self.query.weight 917 1024\n",
      "attention.self.key.weight 917 1024\n",
      "attention.self.value.weight 917 1024\n",
      "attention.output.dense.weight 917 1024\n",
      "intermediate.dense.weight 1005 1024\n",
      "output.dense.weight 1005 1024\n",
      "attention.self.query.weight 917 1024\n",
      "attention.self.key.weight 917 1024\n",
      "attention.self.value.weight 918 1024\n",
      "attention.output.dense.weight 916 1024\n",
      "intermediate.dense.weight 1005 1024\n",
      "output.dense.weight 1005 1024\n",
      "attention.self.query.weight 917 1024\n",
      "attention.self.key.weight 917 1024\n",
      "attention.self.value.weight 917 1024\n",
      "attention.output.dense.weight 917 1024\n",
      "intermediate.dense.weight 1005 1024\n",
      "output.dense.weight 1004 1024\n",
      "attention.self.query.weight 917 1024\n",
      "attention.self.key.weight 917 1024\n",
      "attention.self.value.weight 917 1024\n",
      "attention.output.dense.weight 917 1024\n",
      "intermediate.dense.weight 1005 1024\n",
      "output.dense.weight 1005 1024\n",
      "attention.self.query.weight 916 1024\n",
      "attention.self.key.weight 917 1024\n",
      "attention.self.value.weight 917 1024\n",
      "attention.output.dense.weight 917 1024\n",
      "intermediate.dense.weight 1005 1024\n",
      "output.dense.weight 1005 1024\n",
      "attention.self.query.weight 916 1024\n",
      "attention.self.key.weight 917 1024\n",
      "attention.self.value.weight 917 1024\n",
      "attention.output.dense.weight 916 1024\n",
      "intermediate.dense.weight 1005 1024\n",
      "output.dense.weight 1005 1024\n",
      "attention.self.query.weight 917 1024\n",
      "attention.self.key.weight 917 1024\n",
      "attention.self.value.weight 917 1024\n",
      "attention.output.dense.weight 916 1024\n",
      "intermediate.dense.weight 1005 1024\n",
      "output.dense.weight 1005 1024\n",
      "attention.self.query.weight 917 1024\n",
      "attention.self.key.weight 917 1024\n",
      "attention.self.value.weight 917 1024\n",
      "attention.output.dense.weight 917 1024\n",
      "intermediate.dense.weight 1005 1024\n",
      "output.dense.weight 1005 1024\n",
      "attention.self.query.weight 917 1024\n",
      "attention.self.key.weight 916 1024\n",
      "attention.self.value.weight 917 1024\n",
      "attention.output.dense.weight 917 1024\n",
      "intermediate.dense.weight 1005 1024\n",
      "output.dense.weight 1005 1024\n",
      "attention.self.query.weight 917 1024\n",
      "attention.self.key.weight 917 1024\n",
      "attention.self.value.weight 916 1024\n",
      "attention.output.dense.weight 916 1024\n",
      "intermediate.dense.weight 1005 1024\n",
      "output.dense.weight 1005 1024\n",
      "attention.self.query.weight 917 1024\n",
      "attention.self.key.weight 917 1024\n",
      "attention.self.value.weight 917 1024\n",
      "attention.output.dense.weight 917 1024\n",
      "intermediate.dense.weight 1005 1024\n",
      "output.dense.weight 1005 1024\n",
      "attention.self.query.weight 917 1024\n",
      "attention.self.key.weight 917 1024\n",
      "attention.self.value.weight 917 1024\n",
      "attention.output.dense.weight 917 1024\n",
      "intermediate.dense.weight 1005 1024\n",
      "output.dense.weight 1005 1024\n",
      "attention.self.query.weight 917 1024\n",
      "attention.self.key.weight 917 1024\n",
      "attention.self.value.weight 918 1024\n",
      "attention.output.dense.weight 917 1024\n",
      "intermediate.dense.weight 1005 1024\n",
      "output.dense.weight 1005 1024\n",
      "attention.self.query.weight 917 1024\n",
      "attention.self.key.weight 916 1024\n",
      "attention.self.value.weight 917 1024\n",
      "attention.output.dense.weight 917 1024\n",
      "intermediate.dense.weight 1005 1024\n",
      "output.dense.weight 1005 1024\n",
      "attention.self.query.weight 916 1024\n",
      "attention.self.key.weight 916 1024\n",
      "attention.self.value.weight 917 1024\n",
      "attention.output.dense.weight 916 1024\n",
      "intermediate.dense.weight 1005 1024\n",
      "output.dense.weight 1005 1024\n",
      "attention.self.query.weight 917 1024\n",
      "attention.self.key.weight 917 1024\n",
      "attention.self.value.weight 917 1024\n",
      "attention.output.dense.weight 917 1024\n",
      "intermediate.dense.weight 1005 1024\n",
      "output.dense.weight 1005 1024\n",
      "attention.self.query.weight 917 1024\n",
      "attention.self.key.weight 917 1024\n",
      "attention.self.value.weight 917 1024\n",
      "attention.output.dense.weight 917 1024\n",
      "intermediate.dense.weight 1005 1024\n",
      "output.dense.weight 1005 1024\n",
      "attention.self.query.weight 917 1024\n",
      "attention.self.key.weight 916 1024\n",
      "attention.self.value.weight 917 1024\n",
      "attention.output.dense.weight 918 1024\n",
      "intermediate.dense.weight 1005 1024\n",
      "output.dense.weight 1005 1024\n",
      "attention.self.query.weight 917 1024\n",
      "attention.self.key.weight 917 1024\n",
      "attention.self.value.weight 916 1024\n",
      "attention.output.dense.weight 917 1024\n",
      "intermediate.dense.weight 1005 1024\n",
      "output.dense.weight 1005 1024\n",
      " 917 1024\n"
     ]
    }
   ],
   "source": [
    "explained_variance = 0.99\n",
    "print(\"# Roberta Base Principle Directions \\n\")\n",
    "print_principle_direction(roberta_base_model, explained_variance)\n",
    "\n",
    "print(\"\\n# Roberta Large Principle Directions \\n\")\n",
    "print_principle_direction(roberta_large_model, explained_variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a39e3e-2c84-458e-a47f-5e5a86443278",
   "metadata": {},
   "source": [
    "On the surface, the above results suggest you need most of the directions to account for the explained variance in the parameters.\n",
    "As such, we finetune the models for SQuAD and review the difference in the weights of the base and the tuned model. If the difference has \n",
    "fewer principle directions with high explanation, it would mean it is possible the task-based fine-tuning can be done in a smaller subspace. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a1ff52-126c-4665-92fd-c3e5d8c83fb2",
   "metadata": {},
   "source": [
    "## Rank Variation During Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d59c3413-b156-4dfd-a1f5-1df488ff818b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Dataset\n",
    "import hydra\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from train.train import train_epoch\n",
    "from utils.metrics import AverageMeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "719ff899-edf9-4f7e-b665-ea440ef6bc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Training Configuration\n",
    "with hydra.initialize(version_base=None, config_path=\"../config\"):\n",
    "    cfg = hydra.compose(config_name=\"app_config\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9efb6c9-3495-4760-9c2b-094fa47b98bd",
   "metadata": {},
   "source": [
    "### Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4b82e45-bb22-4aea-b6ac-1c2ef67ab9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Data\n",
    "train_dataset = load_dataset(\"squad\", split=\"train\")\n",
    "val_dataset = load_dataset(\"squad\", split=\"validation\")\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.model.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76dfbf26-bbad-4d73-933a-652dded44c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Max Length is 512\n",
      "Dataset Features are {'id': Value(dtype='string', id=None), 'title': Value(dtype='string', id=None), 'context': Value(dtype='string', id=None), 'question': Value(dtype='string', id=None), 'answers': Sequence(feature={'text': Value(dtype='string', id=None), 'answer_start': Value(dtype='int32', id=None)}, length=-1, id=None)}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model Max Length is {tokenizer.model_max_length}\")\n",
    "print(f\"Dataset Features are {train_dataset.features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19b45799-1fa8-4ed4-a7c2-b3004997e9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 143 training instances with length longer than 512\n"
     ]
    }
   ],
   "source": [
    "cut_off_length = 512\n",
    "long_dataset = train_dataset.filter(lambda x: len(tokenizer(x['question'], x['context']).input_ids) > cut_off_length)\n",
    "print(f\"There are {long_dataset.num_rows} training instances with length longer than {cut_off_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932bb38a-5b28-4dbd-9faa-0aa9571a02e7",
   "metadata": {},
   "source": [
    "Since there are only 143 instances with the length longer than 512. I will remove those instances for the purpose of simplicity. These changes are unlikely to affect the outcome. The learned concept will not account for\n",
    "the cases in which the context may not contain the answer. Also, if there is a regularity in the placement of the answer in the question, the model may over fit to it. But given large number of samples, I assume it is not\n",
    "the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "119e8460-86c9-40d3-8fdb-2ac52d692766",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.filter(lambda x: len(tokenizer(x['question'], x['context']).input_ids) <= cut_off_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5dbd6078-c403-46ae-9aee-abd56889548b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188\n",
      "['a copper statue of Christ']\n",
      "a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n"
     ]
    }
   ],
   "source": [
    "## Let's See how the answer looks like\n",
    "idx = 1\n",
    "instance = train_dataset[idx]\n",
    "answer = instance['answers']\n",
    "print(answer['answer_start'][0])\n",
    "print(answer['text'])\n",
    "print(instance['context'][answer['answer_start'][0]:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae666051-6c8c-4e42-995e-adc84cef27fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer is not character based\n"
     ]
    }
   ],
   "source": [
    "# The above indicates the answer_start is the beginning character. Let's see if this is consistent with the rest of the results\n",
    "char_tokenizer = True\n",
    "for idx, item in enumerate(train_dataset):\n",
    "    start = item['answers']['answer_start'][0]\n",
    "    end = start + len(item['answers']['text'][0])\n",
    "    extracted_answer = item['context'][start:end]\n",
    "    answer = item['answers']['text'][0]\n",
    "    if extracted_answer != answer:\n",
    "        print(idx, extracted_answer, answer)\n",
    "    if char_tokenizer:\n",
    "        tokenized_answer = tokenizer(answer).input_ids\n",
    "        char_tokenizer = len(tokenized_answer) - 2 == len(answer)\n",
    "\n",
    "if not char_tokenizer:\n",
    "    print(\"Tokenizer is not character based\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c37f437-6dbf-4a61-a180-42285188b5c2",
   "metadata": {},
   "source": [
    "The above result confirms that the answers are made available in terms of character offset instead of tokenizer offset. This is likely the case to maintain generality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1e3db4e-f142-4acd-ae85-0ff707c3384e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bisect(l, r, tokenizer, tokenized_context, target, l_off=0, r_off=0, mid_off=0):\n",
    "    # Standard Binary Search with a Few Bells and Whistles\n",
    "    while l < r:\n",
    "        mid = l + (r - l + mid_off) // 2\n",
    "        partial_context = tokenizer.decode(tokenized_context[:mid])\n",
    "        if len(partial_context) <= target:\n",
    "            l = mid + l_off\n",
    "        else:\n",
    "            r = mid + r_off\n",
    "        # print(r, l, mid)\n",
    "    return max(l, r)\n",
    "\n",
    "def get_start_end_index(tokenized_context, answer, answer_start):\n",
    "    # Set Answer End Index\n",
    "    answer_end = answer_start + len(answer)\n",
    "\n",
    "    # Search for the starting index of the Token That contains the answer\n",
    "    ll, rl = 0, len(tokenized_context)\n",
    "    # print(\"Starting Left Search\")\n",
    "    start_index = bisect(ll, rl, tokenizer, tokenized_context, answer_start, r_off=-1, mid_off=1)\n",
    "\n",
    "    # Search for the ending index of the tokens in which the answer terminates\n",
    "    rl, rr = start_index, len(tokenized_context)\n",
    "    # print(\"Starting Right Search\")\n",
    "    end_index = bisect(start_index, len(tokenized_context), tokenizer, tokenized_context, answer_end, l_off=1)\n",
    "    \n",
    "    return start_index, end_index\n",
    "\n",
    "def validate(tokenized_context, start_index, end_index, idx, answer, context, question, ret):\n",
    "    # Test if the correct Solution is found by extracting the partial context and checking if the answer is contained.\n",
    "    # While this is not a 100% foolproof solution. Informal tests before confirmed this to be the case.\n",
    "    # You can set the extracted_answers char offset to answer_start\n",
    "    extracted_context = tokenized_context[:end_index]\n",
    "    extracted_answer = tokenizer.decode(extracted_context).strip()\n",
    "    \n",
    "    if extracted_answer.find(answer) == -1:\n",
    "        failed_instance = {\"idx\" : idx, \n",
    "                           \"start_index\" : start_index,\n",
    "                           \"end_index\" : end_index,\n",
    "                           \"extracted_answer\" : extracted_answer,\n",
    "                           \"answer\" : answer,\n",
    "                           \"context\" : context,\n",
    "                           \"question\" : question} \n",
    "        ret.append(failed_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be31a5ad-b61e-475d-874e-db47e377ea5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's tokenize the start and end character\n",
    "failed_indices = []\n",
    "for idx, item in tqdm(enumerate(train_dataset)):\n",
    "    question = item['question']\n",
    "    context = item['context']\n",
    "    \n",
    "    answer = item['answers']['text'][0]\n",
    "    answer_start = item['answers']['answer_start'][0]\n",
    "\n",
    "    # Extract Start and End Index of the Tokenized Question/Context Duo\n",
    "    tokenized_question = tokenizer(question).input_ids\n",
    "    tokenized_context = tokenizer(question, context).input_ids\n",
    "    start_index, end_index = get_start_end_index(tokenized_context, answer, answer_start + len(question) + 10)\n",
    "\n",
    "    # Test if the correct Solution is found\n",
    "    validate(tokenized_context, start_index, end_index, idx, answer, context, question, failed_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d26eb5e8-1043-40ff-8920-b8693db68547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 56 failed instances by the above method\n"
     ]
    }
   ],
   "source": [
    "print(fr\"There are {len(failed_indices)} failed instances by the above method\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "726329a5-b44d-4615-9c2c-35ac09a49789",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_failed_instances = []\n",
    "\n",
    "def process_batched_training_examples(instances):\n",
    "    questions, contexts = instances['question'], instances['context']\n",
    "    answers = instances['answers']\n",
    "    \n",
    "    inputs = tokenizer(questions, contexts) # Batched Tokenization\n",
    "    start_index, end_index = [], [] # Store the start and end_index in the beginning\n",
    "    \n",
    "    for idx, instance in enumerate(instances['id']):\n",
    "        answer_text, answer_start = answers[idx]['text'][0], answers[idx]['answer_start'][0]\n",
    "\n",
    "        start, end = get_start_end_index(inputs.input_ids[idx], answer_text, answer_start + len(questions[idx]) + 10)\n",
    "        \n",
    "        start_index.append(start)\n",
    "        end_index.append(end)\n",
    "\n",
    "        # Result Validation and storing the ones that failed. - map failed instances must be cleared every time to avoid accumulation\n",
    "        validate(inputs.input_ids[idx], start, end, 0, answer_text, contexts[idx], questions[idx], map_failed_instances)\n",
    "\n",
    "    inputs['start_positions'] = start_index\n",
    "    inputs['end_positions'] = end_index\n",
    "\n",
    "    return inputs\n",
    "\n",
    "def process_training_examples(instance):\n",
    "    \n",
    "    question, context = instance['question'], instance['context']\n",
    "    answer = instance['answers']\n",
    "    \n",
    "    inputs = tokenizer(question, context)\n",
    "    \n",
    "    answer_text, answer_start = answer['text'][0], answer['answer_start'][0]\n",
    "\n",
    "    start_index, end_index = get_start_end_index(inputs.input_ids, answer_text, answer_start + len(question) + 10)\n",
    "    \n",
    "    inputs['start_positions'] = start_index\n",
    "    inputs['end_positions'] = end_index\n",
    "\n",
    "    # Result Validation and storing the ones that failed. - map failed instances must be cleared every time to avoid accumulation\n",
    "    validate(inputs.input_ids, start_index, end_index, 0, answer_text, context, question, map_failed_instances)\n",
    "\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5ca9b2a1-63df-4897-b8bc-a91e46ef61ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4184661ad1a4720841c4fdf087a6ffb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/87456 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(\n",
    "    process_batched_training_examples,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "a15676bf-74fe-4300-a1de-2d51b73735bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(map_failed_instances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a31482-ff23-4244-8f81-6f9656ac8a15",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ab63ac41-7dca-4d80-a70a-cbcec6a5e818",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from collections import defaultdict\n",
    "pad_keys = ['input_ids', 'attention_mask']\n",
    "stack_keys = [ 'start_positions', 'end_positions']\n",
    "pad_id = tokenizer.pad_token_id\n",
    "\n",
    "def collate_fn(batch):\n",
    "    tensor_batch = defaultdict(list)\n",
    "        \n",
    "    # Generate list of samples in batch\n",
    "    for sample in batch:\n",
    "        for key in pad_keys + stack_keys:\n",
    "            key_tensor = torch.tensor(sample[key])\n",
    "            tensor_batch[key].append(key_tensor)\n",
    "\n",
    "    # padding value of attention_mask is 0 since it is multiplied\n",
    "    tensor_batch['input_ids'] = pad_sequence(tensor_batch['input_ids'], padding_value=pad_id, batch_first=True)\n",
    "    tensor_batch['attention_mask'] = pad_sequence(tensor_batch['attention_mask'], padding_value=0, batch_first=True)\n",
    "    \n",
    "    for key in stack_keys:\n",
    "        tensor_batch[key] = torch.stack(tensor_batch[key])\n",
    "\n",
    "    return tensor_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0a7bcbd0-1ab7-48b6-b890-79dddf7abbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Training Artifacts\n",
    "training_cfg = hydra.utils.instantiate(cfg.model.model.train)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=training_cfg.batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=training_cfg.batch_size)\n",
    "\n",
    "model = roberta_base_model\n",
    "optimizer = AdamW(params=model.parameters(), lr=training_cfg.lr, weight_decay=training_cfg.weight_decay)\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "loss_meter = AverageMeter()\n",
    "acc_meter = AverageMeter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7e0e36ab-0cef-48c2-9871-fa85e359ed33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuestionAnsweringModelOutput(loss=tensor(5.5237, grad_fn=<DivBackward0>), start_logits=tensor([[-0.3485,  0.2132, -0.3720,  ..., -0.3881, -0.2330, -0.3561],\n",
      "        [-0.6331,  0.1168,  0.0052,  ..., -0.1770, -0.4284,  0.2183],\n",
      "        [-0.0736,  0.0951, -0.1372,  ..., -0.5970, -0.5458, -0.3755],\n",
      "        ...,\n",
      "        [-0.2512, -0.1587,  0.2909,  ..., -0.6410, -0.5615, -1.2955],\n",
      "        [-0.4225, -0.3017, -0.3239,  ..., -0.6335, -0.2784, -0.7429],\n",
      "        [-0.4239, -0.0021, -0.4860,  ..., -0.5277, -0.0956, -0.2347]],\n",
      "       grad_fn=<CloneBackward0>), end_logits=tensor([[-0.0322,  0.4917, -0.6953,  ...,  0.5170,  0.2709,  0.5642],\n",
      "        [-0.5837,  0.6560,  0.2206,  ...,  0.1962,  0.3853,  0.4577],\n",
      "        [-0.0731,  0.4632, -0.1324,  ..., -0.0496,  0.6684,  0.7222],\n",
      "        ...,\n",
      "        [ 0.2599,  0.0968, -0.0669,  ...,  0.3531,  0.7219,  0.2469],\n",
      "        [-0.1614, -0.2413, -0.9051,  ...,  0.0728,  0.2434,  0.4549],\n",
      "        [ 0.1577, -0.0277, -0.3210,  ...,  0.2739,  0.0668,  0.6673]],\n",
      "       grad_fn=<CloneBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    print(roberta_base_model(**batch))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "50378659-1c48-4fea-9b3a-dff612d13b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618e1ac9-77d9-4289-a7af-3b3ed5a2a073",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5935d06e-6829-4170-8382-57d203cd8467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [0, 3972, 2661, 222, 5, 9880, 2708, 2346, 2082, 11, 504, 4432, 11, 226, 2126, 10067, 1470, 116, 2, 2, 37848, 37471, 28108, 6, 5, 334, 34, 10, 4019, 2048, 4, 497, 1517, 5, 4326, 6919, 18, 1637, 31346, 16, 10, 9030, 9577, 9, 5, 9880, 2708, 4, 29261, 11, 760, 9, 5, 4326, 6919, 8, 2114, 24, 6, 16, 10, 7621, 9577, 9, 4845, 19, 3701, 62, 33161, 19, 5, 7875, 22, 39043, 1459, 1614, 1464, 13292, 4977, 845, 4130, 7, 5, 4326, 6919, 16, 5, 26429, 2426, 9, 5, 25095, 6924, 4, 29261, 639, 5, 32394, 2426, 16, 5, 7461, 26187, 6, 10, 19035, 317, 9, 9621, 8, 12456, 4, 85, 16, 10, 24633, 9, 5, 11491, 26187, 23, 226, 2126, 10067, 6, 1470, 147, 5, 9880, 2708, 2851, 13735, 352, 1382, 7, 6130, 6552, 625, 3398, 208, 22895, 853, 1827, 11, 504, 4432, 4, 497, 5, 253, 9, 5, 1049, 1305, 36, 463, 11, 10, 2228, 516, 14, 15230, 149, 155, 19638, 8, 5, 2610, 25336, 238, 16, 10, 2007, 6, 2297, 7326, 9577, 9, 2708, 4, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "for item in train_dataset:\n",
    "    x = f\"[CLS]{item['question']}[SEP]{item['context']}\"\n",
    "    tokenized_x = tokenizer(item['question'], item['context'])\n",
    "    print(tokenized_x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd23942f-6ea1-4a79-98ae-ddaa0d6a765e",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model.forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1d0c2e-1193-416a-8d95-52dc9e5c3832",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c1c168-4382-4963-bab5-e723a863a4f7",
   "metadata": {},
   "source": [
    "# Introducing LoRA Weights\n",
    "This section introduces LoRA weights in our models for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec9a0bca-9500-4e4f-9650-23bfc0a02439",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# On the surface the above results suggest you need most of the directions to account for most of the directions. \n",
    "# It will be useful to see which directions are affected and how later.\n",
    "from models.LoRA import LoRALinearLayer\n",
    "def add_linear_lora(module, rank, init_type=0):\n",
    "    for key, child in module.named_children():\n",
    "        if isinstance(child, torch.nn.Linear):\n",
    "            lora_layer =  LoRALinearLayer(child, rank=rank, init_type=init_type)\n",
    "            setattr(module, key, lora_layer)\n",
    "        else:\n",
    "            add_linear_lora(child, rank, init_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa02e6c0-2f89-40ab-b387-9c77de31b5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_linear_lora(roberta_base, rank=10, init_type=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe6c9164-4b29-4ad1-b98f-a50c4f368e7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaHubInterface(\n",
       "  (model): RobertaModel(\n",
       "    (encoder): RobertaEncoder(\n",
       "      (sentence_encoder): TransformerEncoder(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
       "        (embed_positions): LearnedPositionalEmbedding(514, 768, padding_idx=1)\n",
       "        (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (layers): ModuleList(\n",
       "          (0-11): 12 x TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): LoRALinear(in_features=768, in_features=768, rank=$10, init_type=$1)\n",
       "              (v_proj): LoRALinear(in_features=768, in_features=768, rank=$10, init_type=$1)\n",
       "              (q_proj): LoRALinear(in_features=768, in_features=768, rank=$10, init_type=$1)\n",
       "              (out_proj): LoRALinear(in_features=768, in_features=768, rank=$10, init_type=$1)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): LoRALinear(in_features=768, in_features=3072, rank=$10, init_type=$1)\n",
       "            (fc2): LoRALinear(in_features=3072, in_features=768, rank=$10, init_type=$1)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (lm_head): RobertaLMHead(\n",
       "        (dense): LoRALinear(in_features=768, in_features=768, rank=$10, init_type=$1)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (classification_heads): ModuleDict()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c66542b-d1f8-417c-bae4-172a31970431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaHubInterface(\n",
       "  (model): RobertaModel(\n",
       "    (encoder): RobertaEncoder(\n",
       "      (sentence_encoder): TransformerEncoder(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
       "        (embed_positions): LearnedPositionalEmbedding(514, 1024, padding_idx=1)\n",
       "        (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (layers): ModuleList(\n",
       "          (0-23): 24 x TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (lm_head): RobertaLMHead(\n",
       "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (classification_heads): ModuleDict()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0ff436bb-df8a-4baa-af98-0be01d85c430",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "40d88517-de50-4af3-ac05-2096882f94d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /roberta-base/resolve/main/config.json HTTP/1.1\" 200 0\n"
     ]
    }
   ],
   "source": [
    "model_config = AutoConfig.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "489ec480-9ada-4d51-8c54-a21b050de0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel.from_config(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88bb4c7-0028-4504-840b-081200450898",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
